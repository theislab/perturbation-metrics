{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "  jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "  jax.tree_util.register_keypaths(data_clz, keypaths)\n"
     ]
    }
   ],
   "source": [
    "import anndata\n",
    "import scanpy as sc\n",
    "import scgen\n",
    "import pandas as pd  \n",
    "import numpy as np\n",
    "\n",
    "font = {'family' : 'Arial',\n",
    "        'size'   : 14}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 3., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = sc.read(\"./data/PapalexiSatija2021_eccite_RNA.h5ad\")\n",
    "train.layers['counts'] = train.X.copy()\n",
    "sc.pp.log1p(train)\n",
    "\n",
    "sc.pp.highly_variable_genes(train, n_top_genes=100, subset=True)\n",
    "train.X = train.layers['counts']\n",
    "train.X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_obs(adata,size_n):\n",
    "    \"\"\"\n",
    "    params \n",
    "    -------------\n",
    "    sample for all control and stim pairs \n",
    "    size_n:the number of rows we will consider in comparison\n",
    "    output x,y in sparse matrix form\n",
    "    which is sample1 and sample2 drawn from two specified codition\n",
    "    \"\"\"\n",
    "\n",
    "    x = adata.X\n",
    "    #print(x.shape)(16893, 6998)\n",
    "    n_rows=x.shape[0]\n",
    "    \n",
    "    df = pd.DataFrame({'x':np.arange(n_rows)})\n",
    "    #shuffle the data index\n",
    "    x_sample1=df['x'].sample(frac=1, replace=False).values[:size_n]\n",
    "    x_sample2=df['x'].sample(frac=1, replace=False).values[:size_n]\n",
    "    \n",
    "    return x[x_sample1,:], x[x_sample2,:]\n",
    "\n",
    "import random\n",
    "\n",
    "def subset(alist, idxs):\n",
    "    '''\n",
    "        use idxs to subset alist\n",
    "        alist: list\n",
    "        idxs: list\n",
    "    '''\n",
    "    sub_list = []\n",
    "    for idx in idxs:\n",
    "        sub_list.append(alist[idx])\n",
    "\n",
    "    return sub_list\n",
    "\n",
    "def split_list(alist, group_num=3, shuffle=True, retain_left=False):\n",
    "    '''\n",
    "        split data into 3 subset and let each subset contains the len(alist)//group number of elements\n",
    "        shuffle: whether shuffle the splitted data, default: True\n",
    "        retain_left: if list alist is splited into the number of group_num subset and there is some element remainï¼Œ\n",
    "        whether take the remaining elements as a subset\n",
    "    '''\n",
    "\n",
    "    index = list(range(len(alist))) \n",
    "\n",
    "    \n",
    "    if shuffle: \n",
    "        random.shuffle(index) \n",
    "    \n",
    "    elem_num = len(alist) // group_num \n",
    "    sub_lists = {}\n",
    "    \n",
    "   \n",
    "    for idx in range(group_num):\n",
    "        start, end = idx*elem_num, (idx+1)*elem_num\n",
    "        sub_lists[str(idx)] = subset(alist, index[start:end])\n",
    "    \n",
    "  \n",
    "    if retain_left and group_num * elem_num != len(index): \n",
    "        sub_lists[str(idx+1)] = subset(alist, index[end:])\n",
    "    \n",
    "    return sub_lists\n",
    "\n",
    "\n",
    "\n",
    "def sample_control_control(adata,size_n):\n",
    "    \"\"\"\n",
    "    the sampling for ctrl and ctrl\n",
    "    split the data into three samples\n",
    "    shuffle the three samples\n",
    "    return two data set have maximum between-sample distance\n",
    "    \n",
    "    \"\"\"\n",
    "    x = adata.X\n",
    "    n_rows=x.shape[0]\n",
    "    index_dict=split_list(range(n_rows))# in form {0: [1,2,3],1:[3,44,2...],3:[]}\n",
    "    sample1=x[index_dict['0'],:size_n].toarray()\n",
    "    sample2=x[index_dict['1'],:size_n].toarray()\n",
    "    sample3=x[index_dict['2'],:size_n].toarray() \n",
    "    \n",
    "    pairs={}\n",
    "    pairs[0]=(sample1,sample2)\n",
    "    pairs[1]=(sample1,sample3)\n",
    "    pairs[2]=(sample2,sample3)\n",
    "    return pairs \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def data_prep(adata,conditions=None,sample_ctrl=False):\n",
    "    \"\"\"\n",
    "    param\n",
    "    -----------\n",
    "    adata\n",
    "    return_mean: bool variable True if we want to compute mean of sampled data per cell to\n",
    "    find statistic between mean of data from sample1 and sample2\n",
    "    output: the sampled data from sample1 and sample2 of type array\n",
    "    \"\"\"\n",
    "    control = adata[adata.obs[\"perturbation\"] == conditions[\"x\"]]\n",
    "    stim = adata[adata.obs[\"perturbation\"] == conditions[\"y\"]]\n",
    "    \n",
    "    #fix the number of rows of sampled data as the minimum number of rows between sample1,sample2\n",
    "    n=np.minimum(control.shape[0],stim.shape[0])\n",
    "    \n",
    "    x,_ = sample_obs(control,n)\n",
    "    y,_ = sample_obs(stim,n)\n",
    "        \n",
    "    x=x.toarray()\n",
    "    y=y.toarray()\n",
    "        \n",
    "    \n",
    "    return x,y\n",
    " \n",
    "def compute_from_mean(x,y,fn,if_return=False):\n",
    "    \"\"\"\n",
    "    param\n",
    "    -------\n",
    "    x,y:data from sample1 and sample2\n",
    "    fn: the statistic function with input x,y\n",
    "    print statistic computed with mean of sample 1 and 2\n",
    "    \"\"\" \n",
    "    \n",
    "    x_mean = np.mean(x, axis=0).ravel()\n",
    "    y_mean = np.mean(y, axis=0).ravel()\n",
    "    mean = fn(x_mean,y_mean)\n",
    "#     print(\"statistic computed with mean of sample 1 and 2:\",mean)\n",
    "    if if_return:\n",
    "        return mean\n",
    "\n",
    "def compute_from_sample(x,y,fn):\n",
    "    \"\"\"\n",
    "    param\n",
    "    -----------\n",
    "    x,y:sampled data from sample1 and sample2 in sparse matric form\n",
    "    output: the average of statistic computed between each data from sample1 and sample2\n",
    "    \"\"\"\n",
    "#     x=x.toarray()\n",
    "#     y=y.toarray()\n",
    "    a=dict.fromkeys(range(x.shape[0]))\n",
    "    \n",
    "    for i in range(x.shape[0]):\n",
    "        a[i]=fn(x[i],y[i])\n",
    "        \n",
    "    return np.mean(list(a.values()))\n",
    "\n",
    "def dist_based(x,y):\n",
    "    \"\"\"\n",
    "    transpose the data so that we can compute static between genes (columns)\n",
    "    \"\"\"\n",
    "    m=np.minimum(x.shape[0],y.shape[0])\n",
    "    x=x.T[:,:m]#delete .toarray()\n",
    "    y=y.T[:,:m]\n",
    "    return x,y\n",
    "\n",
    "\n",
    "def test(fn,train,metric_str):\n",
    "    \"\"\"\n",
    "    fn: the function for computing specific statistic to apply \n",
    "    print out the test result for statistic between (stim,ctrl) and (ctrl,ctrl)\n",
    "    \n",
    "    \"\"\"\n",
    "    list_stim=list(train.obs['perturbation'].unique())\n",
    "    list_stim.remove('control')\n",
    "    conditions={\"x\":\"control\",\"y\":\"stim\"}\n",
    "    \n",
    "    difference=[]\n",
    "    if_stim=['stim']*len(list_stim)\n",
    "    \n",
    "    for stim in list_stim:\n",
    "        conditions[\"y\"]=str(stim)\n",
    "#         print(conditions)\n",
    "#         print(\"mean of computed statistics:\",fn(train,conditions))\n",
    "        difference.append(fn(train,conditions))\n",
    "    \n",
    "    #lastly,for (ctrl,ctrl) where we apply a different sampling method, get three points \n",
    "    conditions[\"y\"]=\"control\"\n",
    "    control = train[train.obs[\"perturbation\"] == conditions[\"x\"]]\n",
    "    print(conditions)\n",
    "    pairs=sample_control_control(control,control.shape[0])\n",
    "    for i in range(3):\n",
    "        (x,y)=pairs[i]\n",
    "        print(\"mean of computed statistics for (contrl, control):\",fn(train,conditions,True,x,y))\n",
    "        #set the sample_ctrl True\n",
    "        difference.append(fn(train,conditions,True,x,y))\n",
    "    zero=['control']*3\n",
    "    if_stim=if_stim + zero\n",
    "    metric=[metric_str]*len(if_stim)\n",
    "        \n",
    "    return difference,if_stim,metric    \n",
    "              \n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scvi import settings\n",
    "import torch \n",
    "def log_zinb_positive(\n",
    "    x: torch.Tensor, mu: torch.Tensor, theta: torch.Tensor, pi: torch.Tensor, eps=1e-8\n",
    "):\n",
    "    \"\"\"Log likelihood (scalar) of a minibatch according to a zinb model.\n",
    "    Parameters\n",
    "    ----------\n",
    "    x\n",
    "        Data\n",
    "    mu\n",
    "        mean of the negative binomial (has to be positive support) (shape: minibatch x vars)\n",
    "    theta\n",
    "        inverse dispersion parameter (has to be positive support) (shape: minibatch x vars)\n",
    "    pi\n",
    "        logit of the dropout parameter (real support) (shape: minibatch x vars)\n",
    "    eps\n",
    "        numerical stability constant\n",
    "    Notes\n",
    "    -----\n",
    "    We parametrize the bernoulli using the logits, hence the softplus functions appearing.\n",
    "    \"\"\"\n",
    "    # theta is the dispersion rate. If .ndimension() == 1, it is shared for all cells (regardless of batch or labels)\n",
    "    if theta.ndimension() == 1:\n",
    "        theta = theta.view(\n",
    "            1, theta.size(0)\n",
    "        )  # In this case, we reshape theta for broadcasting\n",
    "\n",
    "    # Uses log(sigmoid(x)) = -softplus(-x)\n",
    "    softplus_pi = F.softplus(-pi)\n",
    "   \n",
    "    log_theta_eps = torch.log(theta + eps)\n",
    "    log_theta_mu_eps = torch.log(theta + mu + eps)\n",
    "    pi_theta_log = -pi + theta * (log_theta_eps - log_theta_mu_eps)\n",
    "\n",
    "    case_zero = F.softplus(pi_theta_log) - softplus_pi\n",
    "    mul_case_zero = torch.mul((x < eps).type(torch.float32), case_zero)\n",
    "#     print(mul_case_zero)\n",
    "\n",
    "    case_non_zero = (\n",
    "        -softplus_pi\n",
    "        + pi_theta_log\n",
    "        + x * (torch.log(mu + eps) - log_theta_mu_eps)\n",
    "        + torch.lgamma(x + theta)\n",
    "        - torch.lgamma(theta)\n",
    "        - torch.lgamma(x + 1)\n",
    "    )\n",
    "    mul_case_non_zero = torch.mul((x > eps).type(torch.float32), case_non_zero)\n",
    "\n",
    "    res = mul_case_zero + mul_case_non_zero\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_get_data(zinb,train,metric_str):\n",
    "    \"\"\"\n",
    "    fn: the function to get datasets after sampling before we fit the gene in sample1 and sample2\n",
    "    \n",
    "    \"\"\"\n",
    "    list_stim=list(train.obs['perturbation'].unique())\n",
    "    list_stim.remove('control')\n",
    "    conditions={\"x\":\"control\",\"y\":\"stim\"}\n",
    "    \n",
    "    # sample1 is always sampled from control and sample2 is sampled from 99 different stim \n",
    "    #use prep_data to sample respectively \n",
    "    first = True \n",
    "    for stim in list_stim:\n",
    "        conditions[\"y\"]=str(stim)\n",
    "        print(conditions)\n",
    "        if first:\n",
    "            print(\"data:\",zinb(train,conditions,str(stim)))\n",
    "            first=False \n",
    "\n",
    "    \n",
    "    #lastly,for (ctrl,ctrl) where we apply a different sampling method, get three points \n",
    "    conditions[\"y\"]=\"control\"\n",
    "    control = train[train.obs[\"perturbation\"] == conditions[\"x\"]]\n",
    "    print(conditions)\n",
    "    pairs=sample_control_control(control,control.shape[0])\n",
    "    for i in range(3):\n",
    "        (x,y)=pairs[i]\n",
    "        print(\" data for (contrl, control):\",zinb(train,conditions,'control'+str(i),True,x,y))\n",
    "        #set the sample_ctrl True\n",
    "\n",
    "    #     difference.append(fn(train,conditions,True,x,y))\n",
    "    # zero=['control']*3\n",
    "    # if_stim=if_stim + zero\n",
    "    # metric=[metric_str]*len(if_stim)\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': 'control', 'y': 'STAT2g2'}\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.024750\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.003759398496240588\n",
      "theta 20.0\n",
      "p 0.00017901897601145655\n",
      "logit 8.627839711503304\n",
      "output: tensor([-1.4215e+01, -6.7168e-07])\n",
      "counts: tensor([  1, 265])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.326571\n",
      "         Iterations: 3\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.1917293233082118\n",
      "theta 0.028322598929980715\n",
      "p 0.18644861399303625\n",
      "logit 1.473253418305425\n",
      "output: tensor([-9.3551, -9.1329, -8.4106, -7.5417, -6.2425, -5.4395, -0.0106])\n",
      "counts: tensor([  1,   1,   1,   1,   4,   7, 251])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.735402\n",
      "         Iterations: 5\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 1.0300751879698709\n",
      "theta 0.04203289726006362\n",
      "p 0.9885246336064489\n",
      "logit -4.456010877330695\n",
      "output: tensor([-8.1004, -7.8243, -7.2186, -7.1361, -6.9649, -6.7841, -6.3837, -6.0331,\n",
      "        -5.6087, -4.7874, -4.4736, -4.0489, -3.3570, -0.1345])\n",
      "counts: tensor([  1,   1,   2,   2,   1,   1,   1,   2,   1,   2,   1,   6,  13, 232])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.699379\n",
      "         Iterations: 4\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.9924812030072483\n",
      "theta 0.03882488621348882\n",
      "p 0.9553883586913643\n",
      "logit -3.0641230752772213\n",
      "output: tensor([-8.9637, -8.5436, -7.6784, -7.5305, -7.2166, -6.2509, -6.1291, -5.9993,\n",
      "        -5.5422, -5.3561, -5.1431, -4.5781, -4.1535, -3.4600, -0.1213])\n",
      "counts: tensor([  1,   1,   1,   1,   1,   2,   1,   1,   1,   2,   2,   2,   3,  13,\n",
      "        234])\n",
      "(266,)\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: nan\n",
      "         Iterations: 2\n",
      "         Function evaluations: 120\n",
      "         Gradient evaluations: 120\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.111024\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.09398496240601494\n",
      "theta 0.005092546442947388\n",
      "p 0.09350876467906416\n",
      "logit 2.2715261896610177\n",
      "output: tensor([-1.1898e+01, -7.7176e+00, -1.4038e-03])\n",
      "counts: tensor([  1,   3, 262])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.489100\n",
      "         Iterations: 5\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.2368421052631427\n",
      "theta 0.07550752928590512\n",
      "p 0.2202142698344442\n",
      "logit 1.2644181508693473\n",
      "output: tensor([-7.9060, -7.0436, -6.5624, -6.0228, -5.3777, -4.4806, -0.0226])\n",
      "counts: tensor([  2,   2,   2,   3,   4,  14, 239])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.242924\n",
      "         Iterations: 2\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.11278195488732028\n",
      "theta 0.024512337110388006\n",
      "p 0.11008354980421128\n",
      "logit 2.0898879609752874\n",
      "output: tensor([-1.0158e+01, -8.4995e+00, -7.0196e+00, -6.1540e+00, -4.5628e-03])\n",
      "counts: tensor([  1,   2,   2,   6, 255])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.369150\n",
      "         Iterations: 4\n",
      "         Function evaluations: 8\n",
      "         Gradient evaluations: 8\n",
      "mu 0.11654135338345321\n",
      "theta 0.310687665378274\n",
      "p 0.08891618992219517\n",
      "logit 2.3269406511188224\n",
      "output: tensor([-8.2689, -6.7087, -4.9871, -0.0084])\n",
      "counts: tensor([  1,   4,  20, 241])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.472075\n",
      "         Iterations: 6\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "mu 0.24436090225568258\n",
      "theta 0.057892182786276246\n",
      "p 0.23098847522635518\n",
      "logit 1.202738137612886\n",
      "output: tensor([-7.3479, -6.9645, -6.5431, -6.0619, -5.4723, -4.6229, -0.0213])\n",
      "counts: tensor([  2,   2,   4,   2,   7,   7, 242])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.488083\n",
      "         Iterations: 5\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.26691729323314695\n",
      "theta 0.057519498352930895\n",
      "p 0.252399406014609\n",
      "logit 1.085856092789208\n",
      "output: tensor([-8.1316, -7.8258, -7.1656, -6.7995, -6.3955, -5.9317, -5.3594, -4.5270,\n",
      "        -0.0242])\n",
      "counts: tensor([  1,   1,   1,   1,   4,   2,   6,   9, 241])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.739800\n",
      "         Iterations: 5\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.5789473684210135\n",
      "theta 0.07496384323677599\n",
      "p 0.5385738060526488\n",
      "logit -0.15460243108632946\n",
      "output: tensor([-9.1966, -8.2005, -7.6669, -7.2914, -5.9905, -5.7271, -5.4379, -5.1115,\n",
      "        -4.7268, -4.2363, -3.4937, -0.0842])\n",
      "counts: tensor([  1,   1,   1,   1,   1,   2,   1,   2,   7,   8,  15, 226])\n",
      "(266,)\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.236080\n",
      "         Iterations: 4\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "mu 0.09398496240608847\n",
      "theta 0.029630696545377536\n",
      "p 0.09128026458557162\n",
      "logit 2.2981021205956425\n",
      "output: tensor([-9.7473e+00, -8.3838e+00, -7.8319e+00, -7.1671e+00, -6.2291e+00,\n",
      "        -3.7899e-03])\n",
      "counts: tensor([  1,   1,   2,   1,   6, 255])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.520062\n",
      "         Iterations: 4\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 9\n",
      "mu 0.24436090225566917\n",
      "theta 0.0931780539737315\n",
      "p 0.22353257218017755\n",
      "logit 1.2451975540188112\n",
      "output: tensor([-7.9764, -7.0277, -6.5046, -5.9245, -5.2415, -4.3144, -0.0256])\n",
      "counts: tensor([  1,   3,   3,   1,   6,  16, 236])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.031245\n",
      "         Iterations: 1\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 12\n",
      "mu 0.007518796992567985\n",
      "theta 0.00677586483657674\n",
      "p 0.007468193522684874\n",
      "logit 4.889605919943364\n",
      "output: tensor([-1.1868e+01, -3.7682e-05])\n",
      "counts: tensor([  1, 265])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.306457\n",
      "         Iterations: 4\n",
      "         Function evaluations: 13\n",
      "         Gradient evaluations: 13\n",
      "mu 0.289473684210368\n",
      "theta 0.01703186793017451\n",
      "p 0.2846259722416507\n",
      "logit 0.9216295814157603\n",
      "output: tensor([-11.6043,  -8.6340,  -6.9624,  -6.6232,  -6.1691,  -5.4356,  -0.0138])\n",
      "counts: tensor([  1,   1,   1,   2,   2,   6, 253])\n",
      "(266,)\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.044289\n",
      "         Iterations: 1\n",
      "         Function evaluations: 124\n",
      "         Gradient evaluations: 112\n",
      "mu 0.007518796992481457\n",
      "theta 47.47727051581377\n",
      "p 0.00015509942933005584\n",
      "logit 8.77128905569785\n",
      "output: tensor([-1.3669e+01, -1.1617e-06])\n",
      "counts: tensor([  2, 264])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.474963\n",
      "         Iterations: 4\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 12\n",
      "mu 0.23684210526313376\n",
      "theta 0.07291480250354276\n",
      "p 0.2207464233977252\n",
      "logit 1.2613218711337104\n",
      "output: tensor([-11.0637,  -7.8845,  -7.4741,  -5.3942,  -4.5031,  -0.0223])\n",
      "counts: tensor([  1,   1,   2,   5,  18, 239])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.183657\n",
      "         Iterations: 4\n",
      "         Function evaluations: 13\n",
      "         Gradient evaluations: 13\n",
      "mu 0.5375939849404653\n",
      "theta 0.005173259624811268\n",
      "p 0.5348271850577545\n",
      "logit -0.1395346945490977\n",
      "output: tensor([-10.8081, -10.7358,  -7.0337,  -5.9237,  -0.0128])\n",
      "counts: tensor([  1,   1,   1,   3, 260])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.065098\n",
      "         Iterations: 3\n",
      "         Function evaluations: 13\n",
      "         Gradient evaluations: 13\n",
      "mu 0.03759398496302691\n",
      "theta 0.004099645917720698\n",
      "p 0.037440492202013474\n",
      "logit 3.246843092590118\n",
      "output: tensor([-1.1688e+01, -9.6874e+00, -3.5439e-04])\n",
      "counts: tensor([  1,   1, 264])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.698451\n",
      "         Iterations: 4\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 9\n",
      "mu 0.5075187969924617\n",
      "theta 0.07478250478516461\n",
      "p 0.4722060460910724\n",
      "logit 0.11129054007546994\n",
      "output: tensor([-9.6990, -7.6210, -6.9667, -6.4862, -6.2259, -5.9467, -5.6417, -5.2997,\n",
      "        -4.8991, -4.3929, -3.6344, -0.0696])\n",
      "counts: tensor([  1,   1,   1,   2,   1,   1,   2,   2,   3,   8,  16, 228])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.435220\n",
      "         Iterations: 3\n",
      "         Function evaluations: 8\n",
      "         Gradient evaluations: 8\n",
      "mu 0.16917293233084635\n",
      "theta 0.11207601007984491\n",
      "p 0.15212353364110429\n",
      "logit 1.7180420366263043\n",
      "output: tensor([-8.7692, -8.1007, -6.6376, -5.7783, -4.6831, -0.0150])\n",
      "counts: tensor([  1,   1,   1,   8,  15, 240])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.033724\n",
      "         Iterations: 2\n",
      "         Function evaluations: 13\n",
      "         Gradient evaluations: 13\n",
      "mu 0.011278195488824111\n",
      "theta 0.004188467633585382\n",
      "p 0.011231154163124056\n",
      "logit 4.477769040706925\n",
      "output: tensor([-1.2010e+01, -6.1287e-05])\n",
      "counts: tensor([  1, 265])\n",
      "(266,)\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.044289\n",
      "         Iterations: 1\n",
      "         Function evaluations: 124\n",
      "         Gradient evaluations: 112\n",
      "mu 0.007518796992481444\n",
      "theta 47.47727051581377\n",
      "p 0.00015509942933005557\n",
      "logit 8.771289055697851\n",
      "output: tensor([-1.3669e+01, -1.1617e-06])\n",
      "counts: tensor([  2, 264])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.252308\n",
      "         Iterations: 4\n",
      "         Function evaluations: 13\n",
      "         Gradient evaluations: 13\n",
      "mu 0.09022556390953392\n",
      "theta 0.04553166900146143\n",
      "p 0.08629634719310239\n",
      "logit 2.359719017468672\n",
      "output: tensor([-1.0820e+01, -8.5275e+00, -7.0548e+00, -5.9976e+00, -4.1963e-03])\n",
      "counts: tensor([  1,   1,   1,  10, 253])\n",
      "(266,)\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: nan\n",
      "         Iterations: 2\n",
      "         Function evaluations: 120\n",
      "         Gradient evaluations: 120\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.307367\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.0902255639097744\n",
      "theta 0.27535248870737233\n",
      "p 0.07074558971631607\n",
      "logit 2.5752923568188697\n",
      "output: tensor([-8.9403e+00, -7.2647e+00, -5.4156e+00, -5.3254e-03])\n",
      "counts: tensor([  1,   2,  17, 246])\n",
      "(266,)\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.024750\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.003759398496240585\n",
      "theta 20.0\n",
      "p 0.00017901897601145641\n",
      "logit 8.627839711503306\n",
      "output: tensor([-1.4215e+01, -6.7168e-07])\n",
      "counts: tensor([  1, 265])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.024750\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.003759398496240588\n",
      "theta 20.0\n",
      "p 0.00017901897601145655\n",
      "logit 8.627839711503304\n",
      "output: tensor([-1.4215e+01, -6.7168e-07])\n",
      "counts: tensor([  1, 265])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.209264\n",
      "         Iterations: 3\n",
      "         Function evaluations: 13\n",
      "         Gradient evaluations: 13\n",
      "mu 0.07894736842108481\n",
      "theta 0.029403983247934192\n",
      "p 0.07669230905051798\n",
      "logit 2.4881611083826165\n",
      "output: tensor([-1.1519e+01, -7.4303e+00, -6.4495e+00, -2.8896e-03])\n",
      "counts: tensor([  1,   2,   7, 256])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.269725\n",
      "         Iterations: 4\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 9\n",
      "mu 0.07518796992481008\n",
      "theta 0.2691890124914782\n",
      "p 0.059240955590383296\n",
      "logit 2.765073923369177\n",
      "output: tensor([-7.7031e+00, -5.7265e+00, -3.8080e-03])\n",
      "counts: tensor([  3,  14, 249])\n",
      "(266,)\n",
      "(266,)\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.044289\n",
      "         Iterations: 1\n",
      "         Function evaluations: 124\n",
      "         Gradient evaluations: 112\n",
      "mu 0.00751879699248133\n",
      "theta 47.47727051581377\n",
      "p 0.0001550994293300532\n",
      "logit 8.771289055697865\n",
      "output: tensor([-1.3669e+01, -1.1617e-06])\n",
      "counts: tensor([  2, 264])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.422648\n",
      "         Iterations: 5\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 12\n",
      "mu 0.1992481203007622\n",
      "theta 0.0633269081290757\n",
      "p 0.18738180965563955\n",
      "logit 1.4671130705100273\n",
      "output: tensor([-11.5054,  -7.3843,  -6.9009,  -5.7078,  -4.8001,  -0.0163])\n",
      "counts: tensor([  1,   1,   1,   8,  12, 243])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.532097\n",
      "         Iterations: 2\n",
      "         Function evaluations: 8\n",
      "         Gradient evaluations: 8\n",
      "mu 0.3045112781955971\n",
      "theta 0.06684445055241364\n",
      "p 0.285431749715641\n",
      "logit 0.9176755693185175\n",
      "output: tensor([-8.4622, -7.2483, -6.9068, -6.5393, -6.1342, -5.6702, -5.0991, -4.2722,\n",
      "        -0.0314])\n",
      "counts: tensor([  2,   2,   1,   1,   2,   1,   3,  17, 237])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.214429\n",
      "         Iterations: 2\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.07894736842100088\n",
      "theta 0.02952693049427996\n",
      "p 0.0766831503699451\n",
      "logit 2.4882904559974652\n",
      "output: tensor([-1.0227e+01, -8.1370e+00, -7.4285e+00, -6.4467e+00, -2.8945e-03])\n",
      "counts: tensor([  1,   1,   3,   5, 256])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.519357\n",
      "         Iterations: 5\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.3270676691729639\n",
      "theta 0.0521202773292661\n",
      "p 0.31086528434106636\n",
      "logit 0.7960771263412011\n",
      "output: tensor([-8.3300, -7.6144, -7.0813, -6.7879, -6.4681, -6.1100, -5.6917, -5.1641,\n",
      "        -4.3739, -0.0310])\n",
      "counts: tensor([  1,   1,   1,   3,   1,   3,   3,   2,  11, 240])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.102206\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.04135338345864638\n",
      "theta 0.008158721677289558\n",
      "p 0.04101872311320791\n",
      "logit 3.151842927348839\n",
      "output: tensor([-1.1516e+01, -8.1972e+00, -5.9920e-04])\n",
      "counts: tensor([  1,   3, 262])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.325126\n",
      "         Iterations: 5\n",
      "         Function evaluations: 14\n",
      "         Gradient evaluations: 14\n",
      "mu 0.11278195488726098\n",
      "theta 0.09015225772936981\n",
      "p 0.10345523213626098\n",
      "logit 2.1594092486789767\n",
      "output: tensor([-1.1299e+01, -8.3240e+00, -6.5297e+00, -5.3354e+00, -7.3241e-03])\n",
      "counts: tensor([  1,   1,   1,  16, 247])\n",
      "(266,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: invalid value encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3123: RuntimeWarning: invalid value encountered in log\n",
      "  (y + a1) * np.log(a2))\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: divide by zero encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in multiply\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3156: RuntimeWarning: invalid value encountered in divide\n",
      "  a4 = p * a1 / mu\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3164: RuntimeWarning: divide by zero encountered in divide\n",
      "  y / mu)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3164: RuntimeWarning: invalid value encountered in divide\n",
      "  y / mu)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: invalid value encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: invalid value encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3225: RuntimeWarning: invalid value encountered in log\n",
      "  lprob = np.log(prob)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3123: RuntimeWarning: invalid value encountered in log\n",
      "  (y + a1) * np.log(a2))\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: invalid value encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3439: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(linpred)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in multiply\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: divide by zero encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3162: RuntimeWarning: invalid value encountered in multiply\n",
      "  dparams = (a4 * dgterm -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3123: RuntimeWarning: invalid value encountered in log\n",
      "  (y + a1) * np.log(a2))\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: invalid value encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3439: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(linpred)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in multiply\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: divide by zero encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3162: RuntimeWarning: invalid value encountered in multiply\n",
      "  dparams = (a4 * dgterm -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: invalid value encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3123: RuntimeWarning: invalid value encountered in log\n",
      "  (y + a1) * np.log(a2))\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: divide by zero encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in multiply\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3156: RuntimeWarning: invalid value encountered in divide\n",
      "  a4 = p * a1 / mu\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3164: RuntimeWarning: divide by zero encountered in divide\n",
      "  y / mu)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3164: RuntimeWarning: invalid value encountered in divide\n",
      "  y / mu)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: invalid value encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: invalid value encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3225: RuntimeWarning: invalid value encountered in log\n",
      "  lprob = np.log(prob)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3123: RuntimeWarning: invalid value encountered in log\n",
      "  (y + a1) * np.log(a2))\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: invalid value encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3439: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(linpred)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in multiply\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: divide by zero encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3162: RuntimeWarning: invalid value encountered in multiply\n",
      "  dparams = (a4 * dgterm -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3123: RuntimeWarning: invalid value encountered in log\n",
      "  (y + a1) * np.log(a2))\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: invalid value encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3439: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(linpred)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in multiply\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: divide by zero encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3162: RuntimeWarning: invalid value encountered in multiply\n",
      "  dparams = (a4 * dgterm -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/var/folders/12/d54y_pk95zdbftjg_qkcj_c80000gq/T/ipykernel_42688/534514316.py:123: RuntimeWarning: invalid value encountered in log\n",
      "  logit=np.log((1-p)/p)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: invalid value encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3439: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(linpred)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in multiply\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: divide by zero encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3162: RuntimeWarning: invalid value encountered in multiply\n",
      "  dparams = (a4 * dgterm -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/var/folders/12/d54y_pk95zdbftjg_qkcj_c80000gq/T/ipykernel_42688/534514316.py:123: RuntimeWarning: invalid value encountered in log\n",
      "  logit=np.log((1-p)/p)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.044289\n",
      "         Iterations: 1\n",
      "         Function evaluations: 124\n",
      "         Gradient evaluations: 112\n",
      "mu 0.0075187969924814775\n",
      "theta 47.47727051581377\n",
      "p 0.00015509942933005625\n",
      "logit 8.771289055697846\n",
      "output: tensor([-1.3669e+01, -1.1617e-06])\n",
      "counts: tensor([  2, 264])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.086846\n",
      "         Iterations: 3\n",
      "         Function evaluations: 13\n",
      "         Gradient evaluations: 13\n",
      "mu 0.045112781954346134\n",
      "theta 0.005354809806664969\n",
      "p 0.04487249826061066\n",
      "logit 3.058019745056626\n",
      "output: tensor([-1.1538e+01, -9.2578e+00, -8.4579e+00, -5.3596e-04])\n",
      "counts: tensor([  1,   1,   1, 263])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.284713\n",
      "         Iterations: 4\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "mu 0.13909774436102887\n",
      "theta 0.028301571743892116\n",
      "p 0.13526940751936575\n",
      "logit 1.8551496035344217\n",
      "output: tensor([-9.1039e+00, -8.7892e+00, -8.0927e+00, -7.6914e+00, -7.2279e+00,\n",
      "        -6.6513e+00, -5.8008e+00, -6.6585e-03])\n",
      "counts: tensor([  1,   1,   1,   1,   1,   2,   6, 253])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.024750\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.003759398496240588\n",
      "theta 20.0\n",
      "p 0.00017901897601145655\n",
      "logit 8.627839711503304\n",
      "output: tensor([-1.4215e+01, -6.7168e-07])\n",
      "counts: tensor([  1, 265])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.077215\n",
      "         Iterations: 1\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "mu 0.02255639097736338\n",
      "theta 0.010839746447863749\n",
      "p 0.022314507375306075\n",
      "logit 3.779951014239443\n",
      "output: tensor([-1.1275e+01, -8.7317e+00, -2.7056e-04])\n",
      "counts: tensor([  1,   2, 263])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.958806\n",
      "         Iterations: 5\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 12\n",
      "mu 1.691729323309194\n",
      "theta 0.056674571932840256\n",
      "p 1.6009936911936176\n",
      "logit nan\n",
      "output: tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan])\n",
      "counts: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.031245\n",
      "         Iterations: 1\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 12\n",
      "mu 0.007518796992531803\n",
      "theta 0.006775864836576762\n",
      "p 0.007468193522648935\n",
      "logit 4.889605919948212\n",
      "output: tensor([-1.1868e+01, -3.7682e-05])\n",
      "counts: tensor([  1, 265])\n",
      "(266,)\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.203026\n",
      "         Iterations: 4\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 12\n",
      "mu 0.12781954887205513\n",
      "theta 0.012996688277439217\n",
      "p 0.126179631534045\n",
      "logit 1.9351682876836749\n",
      "output: tensor([-1.0052e+01, -9.4776e+00, -8.7873e+00, -7.3180e+00, -6.5409e+00,\n",
      "        -3.8551e-03])\n",
      "counts: tensor([  1,   1,   1,   2,   3, 258])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.033724\n",
      "         Iterations: 2\n",
      "         Function evaluations: 13\n",
      "         Gradient evaluations: 13\n",
      "mu 0.011278195488887659\n",
      "theta 0.004188467633585431\n",
      "p 0.01123115416318734\n",
      "logit 4.477769040701227\n",
      "output: tensor([-1.2010e+01, -6.1287e-05])\n",
      "counts: tensor([  1, 265])\n",
      "(266,)\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.831331\n",
      "         Iterations: 1\n",
      "         Function evaluations: 124\n",
      "         Gradient evaluations: 112\n",
      "mu 2.150375939849804\n",
      "theta 0.01426057922354675\n",
      "p 2.1201414941079486\n",
      "logit nan\n",
      "output: tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan])\n",
      "counts: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.742426\n",
      "         Iterations: 5\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 9\n",
      "mu 0.3496240601503704\n",
      "theta 0.297754612915804\n",
      "p 0.26940690995875766\n",
      "logit 0.9976337433072449\n",
      "output: tensor([-10.4347,  -6.8782,  -6.1108,  -5.3017,  -4.4189,  -3.3703,  -0.0572])\n",
      "counts: tensor([  1,   3,   1,   2,   9,  40, 210])\n",
      "(266,)\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.210214\n",
      "         Iterations: 2\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "mu 0.11278195488787\n",
      "theta 0.017055189704866153\n",
      "p 0.11089069308087166\n",
      "logit 2.081675213758957\n",
      "output: tensor([-1.0862e+01, -9.1952e+00, -7.2630e+00, -6.4460e+00, -3.7804e-03])\n",
      "counts: tensor([  1,   1,   2,   5, 257])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.069016\n",
      "         Iterations: 4\n",
      "         Function evaluations: 14\n",
      "         Gradient evaluations: 14\n",
      "mu 0.06766917293480962\n",
      "theta 0.002908153340180146\n",
      "p 0.06747295124627095\n",
      "logit 2.6261713625133067\n",
      "output: tensor([-1.1982e+01, -9.3199e+00, -6.2309e-04])\n",
      "counts: tensor([  1,   1, 264])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.031245\n",
      "         Iterations: 1\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 12\n",
      "mu 0.007518796992523602\n",
      "theta 0.006775864836576749\n",
      "p 0.00746819352264079\n",
      "logit 4.889605919949311\n",
      "output: tensor([-1.1868e+01, -3.7682e-05])\n",
      "counts: tensor([  1, 265])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.664390\n",
      "         Iterations: 5\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.38345864661655327\n",
      "theta 0.10053599288358747\n",
      "p 0.3484289919603881\n",
      "logit 0.6259519261462272\n",
      "output: tensor([-10.2529,  -6.4758,  -6.0806,  -5.6494,  -5.1619,  -4.5726,  -3.7424,\n",
      "         -0.0523])\n",
      "counts: tensor([  1,   3,   1,   3,   7,   4,  20, 227])\n",
      "(266,)\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.431199\n",
      "         Iterations: 6\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "mu 0.23308270676690862\n",
      "theta 0.046114672853466786\n",
      "p 0.22280798923423323\n",
      "logit 1.2493770722090125\n",
      "output: tensor([-8.3593, -7.7593, -7.4322, -7.0786, -6.6864, -6.2334, -5.6702, -4.8416,\n",
      "        -0.0179])\n",
      "counts: tensor([  1,   1,   1,   1,   1,   5,   5,   6, 245])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.479878\n",
      "         Iterations: 4\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 12\n",
      "mu 0.21804511278197145\n",
      "theta 0.09461860807135171\n",
      "p 0.19919733793503935\n",
      "logit 1.3913185709711506\n",
      "output: tensor([-11.4753,  -9.2749,  -6.7448,  -6.1277,  -5.4080,  -4.4449,  -0.0215])\n",
      "counts: tensor([  1,   1,   1,   1,   3,  22, 237])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.057324\n",
      "         Iterations: 3\n",
      "         Function evaluations: 13\n",
      "         Gradient evaluations: 13\n",
      "mu 0.018796992481220104\n",
      "theta 0.005777935401872592\n",
      "p 0.018689008596822623\n",
      "logit 3.960953848527185\n",
      "output: tensor([-1.1590e+01, -9.4099e+00, -1.5569e-04])\n",
      "counts: tensor([  1,   1, 264])\n",
      "(266,)\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.621326\n",
      "         Iterations: 3\n",
      "         Function evaluations: 7\n",
      "         Gradient evaluations: 7\n",
      "mu 0.25939849624060124\n",
      "theta 0.2885284469003336\n",
      "p 0.20131375202822005\n",
      "logit 1.3781035413617169\n",
      "output: tensor([-7.8295, -6.9282, -5.9846, -4.9661, -3.7787, -0.0346])\n",
      "counts: tensor([  1,   2,   3,   8,  31, 221])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.118781\n",
      "         Iterations: 3\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.026315789473690242\n",
      "theta 0.08416309164453253\n",
      "p 0.0242729066101786\n",
      "logit 3.69382215587548\n",
      "output: tensor([-9.6979e+00, -7.6509e+00, -5.4964e-04])\n",
      "counts: tensor([  1,   5, 260])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.074555\n",
      "         Iterations: 3\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 12\n",
      "mu 0.018796992481260773\n",
      "theta 0.013416086390185579\n",
      "p 0.018548148913065065\n",
      "logit 3.96866296103459\n",
      "output: tensor([-1.1005e+01, -8.8491e+00, -2.1671e-04])\n",
      "counts: tensor([  1,   2, 263])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.024750\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.003759398496240588\n",
      "theta 20.0\n",
      "p 0.00017901897601145655\n",
      "logit 8.627839711503304\n",
      "output: tensor([-1.4215e+01, -6.7168e-07])\n",
      "counts: tensor([  1, 265])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.077215\n",
      "         Iterations: 1\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "mu 0.022556390977319235\n",
      "theta 0.010839746447863762\n",
      "p 0.022314507375262405\n",
      "logit 3.7799510142414445\n",
      "output: tensor([-1.1275e+01, -8.7317e+00, -2.7056e-04])\n",
      "counts: tensor([  1,   2, 263])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.980932\n",
      "         Iterations: 4\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 9\n",
      "mu 0.9473684210525655\n",
      "theta 0.09865873721049524\n",
      "p 0.8622954416745847\n",
      "logit -1.83448744320953\n",
      "output: tensor([-9.9366, -7.0388, -6.5778, -6.2511, -6.0802, -5.5249, -4.8648, -4.6030,\n",
      "        -4.3052, -3.9508, -3.4944, -2.7963, -0.1975])\n",
      "counts: tensor([  1,   2,   1,   2,   2,   2,   2,   2,   2,   6,  11,  23, 210])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.547655\n",
      "         Iterations: 3\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 9\n",
      "mu 0.37969924812028794\n",
      "theta 0.052497198197024614\n",
      "p 0.3607603409973252\n",
      "logit 0.5720655747585932\n",
      "output: tensor([-8.9366, -8.5592, -7.2993, -7.0586, -6.5281, -6.2267, -5.4872, -4.9782,\n",
      "        -4.2067, -0.0385])\n",
      "counts: tensor([  1,   1,   1,   1,   2,   3,   1,   4,  14, 238])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 1.802545\n",
      "         Iterations: 4\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 9\n",
      "mu 2.187969924812043\n",
      "theta 0.30732825559993915\n",
      "p 1.6736193954654286\n",
      "logit nan\n",
      "output: tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan])\n",
      "counts: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "(266,)\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.335639\n",
      "         Iterations: 4\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "mu 0.19924812030075145\n",
      "theta 0.02770773451847866\n",
      "p 0.19387624867307923\n",
      "logit 1.4250172062623754\n",
      "output: tensor([-9.4571, -8.3347, -8.0749, -7.4881, -6.7330, -6.2111, -5.4151, -0.0110])\n",
      "counts: tensor([  1,   1,   1,   1,   3,   3,   5, 251])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.627428\n",
      "         Iterations: 6\n",
      "         Function evaluations: 15\n",
      "         Gradient evaluations: 15\n",
      "mu 0.2593984962405971\n",
      "theta 0.3679726132446787\n",
      "p 0.18962257996184054\n",
      "logit 1.4524644144538783\n",
      "output: tensor([-13.0404,  -7.1799,  -6.1247,  -5.0050,  -3.7420,  -0.0344])\n",
      "counts: tensor([  1,   1,   3,   2,  42, 217])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.619693\n",
      "         Iterations: 3\n",
      "         Function evaluations: 7\n",
      "         Gradient evaluations: 7\n",
      "mu 0.28195488721806555\n",
      "theta 0.17012428181869096\n",
      "p 0.24096148725315833\n",
      "logit 1.147415400748423\n",
      "output: tensor([-8.2143, -6.9950, -6.3414, -5.6368, -4.8409, -3.8327, -0.0376])\n",
      "counts: tensor([  1,   3,   1,   4,   5,  27, 225])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.105489\n",
      "         Iterations: 1\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "mu 0.045112781954497236\n",
      "theta 0.009012582132602148\n",
      "p 0.04470983093109598\n",
      "logit 3.061821727504479\n",
      "output: tensor([-1.1346e+01, -8.8813e+00, -8.0150e+00, -7.1682e-04])\n",
      "counts: tensor([  1,   1,   2, 262])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.374187\n",
      "         Iterations: 5\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 12\n",
      "mu 0.19172932330812612\n",
      "theta 0.04368666227739331\n",
      "p 0.18370391252271065\n",
      "logit 1.4914518510193644\n",
      "output: tensor([-10.8121,  -8.1759,  -7.4448,  -6.5488,  -5.9597,  -5.1040,  -0.0131])\n",
      "counts: tensor([  1,   1,   1,   2,   3,  11, 247])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.965657\n",
      "         Iterations: 5\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 2.996240601504976\n",
      "theta 0.038948202437756055\n",
      "p 2.8839172101888133\n",
      "logit nan\n",
      "output: tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan])\n",
      "counts: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.329085\n",
      "         Iterations: 5\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "mu 0.15037593984973446\n",
      "theta 0.03976141837528267\n",
      "p 0.14462542771082035\n",
      "logit 1.7773923263576186\n",
      "output: tensor([-9.0751e+00, -7.9214e+00, -7.4736e+00, -6.9644e+00, -6.3441e+00,\n",
      "        -5.4553e+00, -8.7627e-03])\n",
      "counts: tensor([  1,   2,   1,   1,   4,   7, 250])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.112705\n",
      "         Iterations: 4\n",
      "         Function evaluations: 13\n",
      "         Gradient evaluations: 13\n",
      "mu 0.06766917292993643\n",
      "theta 0.006522488046599538\n",
      "p 0.06723066174235694\n",
      "logit 2.6300285250111104\n",
      "output: tensor([-1.1139e+01, -9.8040e+00, -7.8400e+00, -1.0583e-03])\n",
      "counts: tensor([  1,   1,   2, 262])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 1.171739\n",
      "         Iterations: 4\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "mu 1.0751879699248046\n",
      "theta 0.16083371758646933\n",
      "p 0.9262204858765355\n",
      "logit -2.530031205332449\n",
      "output: tensor([-14.4192,  -7.6445,  -6.5270,  -6.1288,  -5.9227,  -5.0277,  -4.7775,\n",
      "         -4.5104,  -4.2203,  -3.8972,  -3.5223,  -3.0548,  -2.3714,  -0.2997])\n",
      "counts: tensor([  1,   1,   1,   1,   1,   2,   2,   2,   3,   6,   6,  12,  39, 189])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.531396\n",
      "         Iterations: 3\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.7556390977449715\n",
      "theta 0.02656861653942428\n",
      "p 0.7360824065440851\n",
      "logit -1.0257051694560855\n",
      "output: tensor([-10.0687,  -8.8640,  -7.5330,  -7.3557,  -6.7250,  -6.1475,  -5.9632,\n",
      "         -5.7517,  -5.5006,  -5.1872,  -4.7603,  -4.0589,  -0.0654])\n",
      "counts: tensor([  1,   1,   1,   2,   1,   1,   1,   1,   1,   1,   2,  10, 243])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.031245\n",
      "         Iterations: 1\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 12\n",
      "mu 0.007518796992611859\n",
      "theta 0.00677586483657674\n",
      "p 0.007468193522728453\n",
      "logit 4.889605919937485\n",
      "output: tensor([-1.1868e+01, -3.7682e-05])\n",
      "counts: tensor([  1, 265])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.795165\n",
      "         Iterations: 3\n",
      "         Function evaluations: 8\n",
      "         Gradient evaluations: 8\n",
      "mu 0.6052631578947584\n",
      "theta 0.08993521291363168\n",
      "p 0.5553203077793583\n",
      "logit -0.2221908481758386\n",
      "output: tensor([-7.8776, -7.4794, -7.0626, -6.3863, -6.1412, -5.8819, -5.3010, -4.9616,\n",
      "        -4.5649, -4.0649, -3.3193, -0.0979])\n",
      "counts: tensor([  1,   1,   1,   3,   1,   4,   1,   2,   4,   4,  23, 221])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.188195\n",
      "         Iterations: 2\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.06015037593962924\n",
      "theta 0.03349443351043158\n",
      "p 0.058200967503345635\n",
      "logit 2.7838899321491586\n",
      "output: tensor([-1.0028e+01, -8.6517e+00, -7.8202e+00, -6.7173e+00, -1.9721e-03])\n",
      "counts: tensor([  1,   1,   1,   6, 257])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.201223\n",
      "         Iterations: 2\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.07894736842093791\n",
      "theta 0.023755303806204074\n",
      "p 0.07711546707247396\n",
      "logit 2.4822002559361276\n",
      "output: tensor([-1.0460e+01, -8.1897e+00, -7.5330e+00, -6.6002e+00, -2.6393e-03])\n",
      "counts: tensor([  1,   1,   3,   4, 257])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.496897\n",
      "         Iterations: 4\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.29323308270673115\n",
      "theta 0.05583943586000762\n",
      "p 0.2777250713958089\n",
      "logit 0.955774181410572\n",
      "output: tensor([-9.6474, -8.4229, -7.6002, -6.9811, -6.2520, -5.8084, -5.2562, -4.4431,\n",
      "        -0.0274])\n",
      "counts: tensor([  1,   1,   1,   1,   1,   4,   4,  13, 240])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.501512\n",
      "         Iterations: 6\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "mu 0.25939849624060013\n",
      "theta 0.06474838575337788\n",
      "p 0.24362422118823784\n",
      "logit 1.1329113535474276\n",
      "output: tensor([-7.9509, -6.8452, -6.4153, -5.9262, -5.3297, -4.4765, -0.0244])\n",
      "counts: tensor([  1,   2,   3,   6,   7,   7, 240])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.571656\n",
      "         Iterations: 4\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.48120300751870826\n",
      "theta 0.04496798595851368\n",
      "p 0.4604954543916645\n",
      "logit 0.1583482256386151\n",
      "output: tensor([-8.6124, -7.8913, -7.7362, -7.5762, -7.4106, -6.8684, -6.4504, -5.9517,\n",
      "        -5.6504, -5.2883, -4.8157, -4.0772, -0.0494])\n",
      "counts: tensor([  1,   1,   1,   1,   1,   1,   1,   1,   3,   1,   2,  14, 238])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.394072\n",
      "         Iterations: 4\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.14285714285712242\n",
      "theta 0.11956611659405147\n",
      "p 0.12760045229997044\n",
      "logit 1.9223436002975143\n",
      "output: tensor([-10.1965,  -7.8854,  -6.0732,  -4.8848,  -0.0115])\n",
      "counts: tensor([  1,   1,   5,  17, 242])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.036902\n",
      "         Iterations: 2\n",
      "         Function evaluations: 13\n",
      "         Gradient evaluations: 13\n",
      "mu 0.018796992482597332\n",
      "theta 0.0034069278413649253\n",
      "p 0.01873316992442479\n",
      "logit 3.958548675320889\n",
      "output: tensor([-1.2101e+01, -1.1926e-04])\n",
      "counts: tensor([  1, 265])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.561026\n",
      "         Iterations: 4\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "mu 0.27067669172931735\n",
      "theta 0.10813013271423412\n",
      "p 0.24426435464427515\n",
      "logit 1.1294405800835796\n",
      "output: tensor([-10.3769,  -8.2657,  -6.8420,  -5.7211,  -5.0322,  -4.1056,  -0.0315])\n",
      "counts: tensor([  1,   1,   1,   4,   7,  20, 232])\n",
      "(266,)\n",
      "(266,)\n",
      "(266,)\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.044289\n",
      "         Iterations: 1\n",
      "         Function evaluations: 124\n",
      "         Gradient evaluations: 112\n",
      "mu 0.007518796992481464\n",
      "theta 47.47727051581377\n",
      "p 0.00015509942933005598\n",
      "logit 8.771289055697848\n",
      "output: tensor([-1.3669e+01, -1.1617e-06])\n",
      "counts: tensor([  2, 264])\n",
      "(266,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.705846\n",
      "         Iterations: 5\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 9\n",
      "mu 0.33082706766917563\n",
      "theta 0.24271556152280557\n",
      "p 0.2662130240517669\n",
      "logit 1.0139219324092712\n",
      "output: tensor([-8.8397, -7.5253, -6.8401, -6.1256, -5.3655, -4.5244, -3.4983, -0.0514])\n",
      "counts: tensor([  1,   2,   1,   1,   4,   5,  37, 215])\n",
      "(266,)\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: nan\n",
      "         Iterations: 1\n",
      "         Function evaluations: 112\n",
      "         Gradient evaluations: 112\n",
      "(266,)\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: nan\n",
      "         Iterations: 1\n",
      "         Function evaluations: 112\n",
      "         Gradient evaluations: 112\n",
      "(266,)\n",
      "(266,)\n",
      "data: None\n",
      "{'x': 'control', 'y': 'CAV1g4'}\n",
      "{'x': 'control', 'y': 'STAT1g2'}\n",
      "{'x': 'control', 'y': 'CD86g1'}\n",
      "{'x': 'control', 'y': 'IRF7g2'}\n",
      "{'x': 'control', 'y': 'ATF2g4'}\n",
      "{'x': 'control', 'y': 'STAT3g4'}\n",
      "{'x': 'control', 'y': 'JAK2g2'}\n",
      "{'x': 'control', 'y': 'NFKBIAg1'}\n",
      "{'x': 'control', 'y': 'CAV1g1'}\n",
      "{'x': 'control', 'y': 'SMAD4g1'}\n",
      "{'x': 'control', 'y': 'STAT5Ag2'}\n",
      "{'x': 'control', 'y': 'CMTM6g2'}\n",
      "{'x': 'control', 'y': 'JAK2g3'}\n",
      "{'x': 'control', 'y': 'ATF2g2'}\n",
      "{'x': 'control', 'y': 'CMTM6g3'}\n",
      "{'x': 'control', 'y': 'STAT5Ag3'}\n",
      "{'x': 'control', 'y': 'IFNGR1g4'}\n",
      "{'x': 'control', 'y': 'UBE2L6g2'}\n",
      "{'x': 'control', 'y': 'CD86g4'}\n",
      "{'x': 'control', 'y': 'PDCD1LG2g1'}\n",
      "{'x': 'control', 'y': 'CUL3g1'}\n",
      "{'x': 'control', 'y': 'BRD4g4'}\n",
      "{'x': 'control', 'y': 'MARCH8g2'}\n",
      "{'x': 'control', 'y': 'IRF1g2'}\n",
      "{'x': 'control', 'y': 'POU2F2g1'}\n",
      "{'x': 'control', 'y': 'BRD4g3'}\n",
      "{'x': 'control', 'y': 'MARCH8g3'}\n",
      "{'x': 'control', 'y': 'IFNGR1g2'}\n",
      "{'x': 'control', 'y': 'ETV7g3'}\n",
      "{'x': 'control', 'y': 'UBE2L6g1'}\n",
      "{'x': 'control', 'y': 'IFNGR1g3'}\n",
      "{'x': 'control', 'y': 'ETV7g1'}\n",
      "{'x': 'control', 'y': 'IRF1g1'}\n",
      "{'x': 'control', 'y': 'CD86g2'}\n",
      "{'x': 'control', 'y': 'IRF1g4'}\n",
      "{'x': 'control', 'y': 'STAT3g3'}\n",
      "{'x': 'control', 'y': 'IFNGR2g2'}\n",
      "{'x': 'control', 'y': 'PDCD1LG2g3'}\n",
      "{'x': 'control', 'y': 'ETV7g2'}\n",
      "{'x': 'control', 'y': 'CMTM6g1'}\n",
      "{'x': 'control', 'y': 'PDCD1LG2g4'}\n",
      "{'x': 'control', 'y': 'STAT1g3'}\n",
      "{'x': 'control', 'y': 'CAV1g3'}\n",
      "{'x': 'control', 'y': 'MARCH8g1'}\n",
      "{'x': 'control', 'y': 'IFNGR2g4'}\n",
      "{'x': 'control', 'y': 'NFKBIAg2'}\n",
      "{'x': 'control', 'y': 'MARCH8g4'}\n",
      "{'x': 'control', 'y': 'IFNGR2g1'}\n",
      "{'x': 'control', 'y': 'STAT5Ag1'}\n",
      "{'x': 'control', 'y': 'NFKBIAg3'}\n",
      "{'x': 'control', 'y': 'JAK2g1'}\n",
      "{'x': 'control', 'y': 'IRF7g3'}\n",
      "{'x': 'control', 'y': 'TNFRSF14g2'}\n",
      "{'x': 'control', 'y': 'STAT3g1'}\n",
      "{'x': 'control', 'y': 'IFNGR2g3'}\n",
      "{'x': 'control', 'y': 'STAT5Ag4'}\n",
      "{'x': 'control', 'y': 'IRF7g1'}\n",
      "{'x': 'control', 'y': 'BRD4g2'}\n",
      "{'x': 'control', 'y': 'ATF2g1'}\n",
      "{'x': 'control', 'y': 'CD86g3'}\n",
      "{'x': 'control', 'y': 'IRF1g3'}\n",
      "{'x': 'control', 'y': 'TNFRSF14g3'}\n",
      "{'x': 'control', 'y': 'POU2F2g3'}\n",
      "{'x': 'control', 'y': 'STAT2g3'}\n",
      "{'x': 'control', 'y': 'SMAD4g4'}\n",
      "{'x': 'control', 'y': 'ETV7g4'}\n",
      "{'x': 'control', 'y': 'SMAD4g2'}\n",
      "{'x': 'control', 'y': 'CUL3g2'}\n",
      "{'x': 'control', 'y': 'CAV1g2'}\n",
      "{'x': 'control', 'y': 'TNFRSF14g1'}\n",
      "{'x': 'control', 'y': 'SPI1g4'}\n",
      "{'x': 'control', 'y': 'STAT1g1'}\n",
      "{'x': 'control', 'y': 'JAK2g4'}\n",
      "{'x': 'control', 'y': 'MYCg3'}\n",
      "{'x': 'control', 'y': 'MYCg1'}\n",
      "{'x': 'control', 'y': 'STAT3g2'}\n",
      "{'x': 'control', 'y': 'SMAD4g3'}\n",
      "{'x': 'control', 'y': 'UBE2L6g3'}\n",
      "{'x': 'control', 'y': 'IFNGR1g1'}\n",
      "{'x': 'control', 'y': 'STAT2g4'}\n",
      "{'x': 'control', 'y': 'IRF7g4'}\n",
      "{'x': 'control', 'y': 'STAT2g1'}\n",
      "{'x': 'control', 'y': 'ATF2g3'}\n",
      "{'x': 'control', 'y': 'POU2F2g2'}\n",
      "{'x': 'control', 'y': 'NFKBIAg4'}\n",
      "{'x': 'control', 'y': 'STAT1g4'}\n",
      "{'x': 'control', 'y': 'SPI1g1'}\n",
      "{'x': 'control', 'y': 'UBE2L6g4'}\n",
      "{'x': 'control', 'y': 'POU2F2g4'}\n",
      "{'x': 'control', 'y': 'MYCg2'}\n",
      "{'x': 'control', 'y': 'BRD4g1'}\n",
      "{'x': 'control', 'y': 'TNFRSF14g4'}\n",
      "{'x': 'control', 'y': 'SPI1g3'}\n",
      "{'x': 'control', 'y': 'PDCD1LG2g2'}\n",
      "{'x': 'control', 'y': 'CUL3g3'}\n",
      "{'x': 'control', 'y': 'SPI1g2'}\n",
      "{'x': 'control', 'y': 'MYCg4'}\n",
      "{'x': 'control', 'y': 'control'}\n",
      "(795,)\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.038170\n",
      "         Iterations: 1\n",
      "         Function evaluations: 124\n",
      "         Gradient evaluations: 112\n",
      "mu 0.006289308176100685\n",
      "theta 33.61131987639512\n",
      "p 0.00018171246281740284\n",
      "logit 8.612903265860547\n",
      "output: tensor([-1.3688e+01, -1.1392e-06])\n",
      "counts: tensor([  5, 790])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.215580\n",
      "         Iterations: 3\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.07044025157241413\n",
      "theta 0.04111245130913337\n",
      "p 0.06765863906808524\n",
      "logit 2.6232239658462433\n",
      "output: tensor([-1.0990e+01, -9.7494e+00, -8.3429e+00, -7.4981e+00, -6.3855e+00,\n",
      "        -2.7241e-03])\n",
      "counts: tensor([  1,   2,   2,   6,  21, 763])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.608128\n",
      "         Iterations: 4\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.6729559748427939\n",
      "theta 0.03908113820384626\n",
      "p 0.647645260894702\n",
      "logit -0.608704659886767\n",
      "output: tensor([-10.3506,  -9.9672,  -8.6420,  -8.2737,  -7.9816,  -7.7772,  -7.4519,\n",
      "         -7.0962,  -6.8354,  -6.6955,  -6.5476,  -6.3902,  -5.8323,  -5.6013,\n",
      "         -5.3314,  -5.0002,  -4.5577,  -3.8464,  -0.0720])\n",
      "counts: tensor([  1,   1,   1,   1,   4,   2,   3,   1,   1,   1,   3,   2,   4,   1,\n",
      "          2,   6,  15,  37, 709])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.657534\n",
      "         Iterations: 3\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 9\n",
      "mu 0.6515723270439794\n",
      "theta 0.047004231107443305\n",
      "p 0.6223206245831449\n",
      "logit -0.4994098110343273\n",
      "output: tensor([-9.7475, -9.0660, -8.8631, -8.5502, -8.3347, -7.8825, -7.6429, -7.3915,\n",
      "        -7.2604, -6.8392, -6.5265, -6.3567, -6.1751, -5.9786, -5.7626, -5.5200,\n",
      "        -5.2389, -4.8971, -4.4452, -3.7283, -0.0770])\n",
      "counts: tensor([  1,   1,   1,   1,   1,   1,   2,   1,   1,   4,   2,   2,   5,   1,\n",
      "          2,   5,   6,   9,  13,  36, 700])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.462693\n",
      "         Iterations: 4\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 12\n",
      "mu 0.24402515723288185\n",
      "theta 0.05971120553204695\n",
      "p 0.23027515039851318\n",
      "logit 1.2067582128075873\n",
      "output: tensor([-13.3623,  -8.3914,  -8.0622,  -7.3551,  -6.9658,  -6.5386,  -6.0517,\n",
      "         -5.4568,  -4.6027,  -0.0215])\n",
      "counts: tensor([  1,   3,   1,   3,   3,   4,   5,  14,  40, 721])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.027824\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.005031446540880504\n",
      "theta 0.010152397848688998\n",
      "p 0.004980878678896296\n",
      "logit 5.297155637364061\n",
      "output: tensor([-1.2788e+01, -1.1001e+01, -2.0313e-05])\n",
      "counts: tensor([  1,   2, 792])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.436862\n",
      "         Iterations: 3\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 9\n",
      "mu 0.19496855345916406\n",
      "theta 0.07479803945879589\n",
      "p 0.18140017594127597\n",
      "logit 1.5068898415404235\n",
      "output: tensor([-9.7383, -8.8833, -8.4357, -7.9692, -7.4770, -6.9477, -6.3599, -5.6664,\n",
      "        -4.7207, -0.0167])\n",
      "counts: tensor([  1,   1,   2,   3,   3,   1,   6,  12,  44, 722])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.244188\n",
      "         Iterations: 4\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 12\n",
      "mu 0.10817610062923917\n",
      "theta 0.026841755146616866\n",
      "p 0.10534836559484603\n",
      "logit 2.1391617811216466\n",
      "output: tensor([-1.2142e+01, -1.0355e+01, -9.6952e+00, -9.3438e+00, -8.1358e+00,\n",
      "        -7.6354e+00, -7.0216e+00, -6.1333e+00, -4.4805e-03])\n",
      "counts: tensor([  1,   1,   1,   1,   1,   2,   8,  19, 761])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.379791\n",
      "         Iterations: 4\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 9\n",
      "mu 0.12327044025155538\n",
      "theta 0.24500012790248343\n",
      "p 0.09901239163664627\n",
      "logit 2.208246493961281\n",
      "output: tensor([-1.0428e+01, -9.1697e+00, -7.8661e+00, -6.4817e+00, -4.9133e+00,\n",
      "        -9.4536e-03])\n",
      "counts: tensor([  1,   2,   2,   8,  63, 719])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.520598\n",
      "         Iterations: 5\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.28930817610065424\n",
      "theta 0.06398796320740435\n",
      "p 0.2719092565939668\n",
      "logit 0.984957292793966\n",
      "output: tensor([-8.2834, -7.6757, -7.3515, -7.0081, -6.6387, -6.2316, -5.7652, -5.1914,\n",
      "        -4.3605, -0.0286])\n",
      "counts: tensor([  2,   3,   2,   4,   6,   7,  10,  12,  36, 713])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.544110\n",
      "         Iterations: 3\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 9\n",
      "mu 0.38238993710700897\n",
      "theta 0.04974155357636486\n",
      "p 0.36427055383703216\n",
      "logit 0.5568762024042198\n",
      "output: tensor([-10.6830,  -8.3131,  -7.9224,  -7.7176,  -7.5049,  -7.2828,  -7.0489,\n",
      "         -6.5320,  -6.2373,  -5.9042,  -5.5107,  -5.0075,  -4.2406,  -0.0378])\n",
      "counts: tensor([  1,   1,   2,   2,   3,   4,   1,   2,   3,   6,  11,  13,  32, 714])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.775695\n",
      "         Iterations: 4\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 9\n",
      "mu 0.7308176100628745\n",
      "theta 0.06365654021073215\n",
      "p 0.6870804460226274\n",
      "logit -0.7865052414662392\n",
      "output: tensor([-10.3704,  -8.7163,  -7.9897,  -7.8627,  -7.6021,  -6.8992,  -6.7464,\n",
      "         -6.5882,  -6.4234,  -6.2509,  -6.0691,  -5.8757,  -5.6677,  -5.4406,\n",
      "         -5.1874,  -4.8966,  -4.5464,  -4.0887,  -3.3738,  -0.1076])\n",
      "counts: tensor([  1,   1,   1,   1,   1,   2,   3,   3,   2,   2,   2,   4,   7,   6,\n",
      "          7,  10,   8,  12,  45, 677])\n",
      "(795,)\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.316258\n",
      "         Iterations: 4\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "mu 0.17735849056646122\n",
      "theta 0.027000926609737574\n",
      "p 0.1726955506768085\n",
      "logit 1.5666425427938004\n",
      "output: tensor([-9.3009e+00, -9.0666e+00, -8.8225e+00, -8.5664e+00, -8.2950e+00,\n",
      "        -8.0036e+00, -7.6850e+00, -7.3269e+00, -6.9064e+00, -6.3727e+00,\n",
      "        -5.5645e+00, -9.2270e-03])\n",
      "counts: tensor([  1,   1,   3,   1,   1,   1,   1,   3,   9,   7,  14, 753])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.560389\n",
      "         Iterations: 5\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.3132075471698196\n",
      "theta 0.07104462515843359\n",
      "p 0.29243183693068675\n",
      "logit 0.8836023650571488\n",
      "output: tensor([-7.8416, -7.5283, -7.2004, -6.8536, -6.4809, -6.0710, -5.6023, -5.0273,\n",
      "        -4.1983, -0.0336])\n",
      "counts: tensor([  1,   1,   2,   7,   4,  13,  16,  11,  34, 706])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.017573\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.0025157232704402514\n",
      "theta 20.0\n",
      "p 0.00011979634621144054\n",
      "logit 9.029597568295443\n",
      "output: tensor([-1.5018e+01, -3.0098e-07])\n",
      "counts: tensor([  2, 793])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.327295\n",
      "         Iterations: 7\n",
      "         Function evaluations: 16\n",
      "         Gradient evaluations: 16\n",
      "mu 0.3144654088066522\n",
      "theta 0.0192583588167498\n",
      "p 0.3085237477686354\n",
      "logit 0.8070299895533781\n",
      "output: tensor([-12.5233, -11.9787, -10.5609, -10.3807,  -7.6860,  -7.0475,  -6.7697,\n",
      "         -6.4290,  -5.9736,  -5.2401,  -0.0166])\n",
      "counts: tensor([  1,   1,   1,   1,   2,   2,   1,   5,   5,  24, 752])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.034919\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.006289308176100629\n",
      "theta 0.015954347258223322\n",
      "p 0.006190542117442298\n",
      "logit 5.0785228335940635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/var/folders/12/d54y_pk95zdbftjg_qkcj_c80000gq/T/ipykernel_42688/534514316.py:123: RuntimeWarning: invalid value encountered in log\n",
      "  logit=np.log((1-p)/p)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3123: RuntimeWarning: invalid value encountered in log\n",
      "  (y + a1) * np.log(a2))\n",
      "/var/folders/12/d54y_pk95zdbftjg_qkcj_c80000gq/T/ipykernel_42688/534514316.py:123: RuntimeWarning: invalid value encountered in log\n",
      "  logit=np.log((1-p)/p)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3123: RuntimeWarning: invalid value encountered in log\n",
      "  (y + a1) * np.log(a2))\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: invalid value encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3439: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(linpred)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in multiply\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: divide by zero encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3162: RuntimeWarning: invalid value encountered in multiply\n",
      "  dparams = (a4 * dgterm -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3123: RuntimeWarning: invalid value encountered in log\n",
      "  (y + a1) * np.log(a2))\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: invalid value encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: divide by zero encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in multiply\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3156: RuntimeWarning: invalid value encountered in divide\n",
      "  a4 = p * a1 / mu\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3164: RuntimeWarning: divide by zero encountered in divide\n",
      "  y / mu)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3164: RuntimeWarning: invalid value encountered in divide\n",
      "  y / mu)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3123: RuntimeWarning: invalid value encountered in log\n",
      "  (y + a1) * np.log(a2))\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3123: RuntimeWarning: invalid value encountered in log\n",
      "  (y + a1) * np.log(a2))\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: invalid value encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: divide by zero encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in multiply\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3156: RuntimeWarning: invalid value encountered in divide\n",
      "  a4 = p * a1 / mu\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3164: RuntimeWarning: divide by zero encountered in divide\n",
      "  y / mu)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3164: RuntimeWarning: invalid value encountered in divide\n",
      "  y / mu)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3123: RuntimeWarning: invalid value encountered in log\n",
      "  (y + a1) * np.log(a2))\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3123: RuntimeWarning: invalid value encountered in log\n",
      "  (y + a1) * np.log(a2))\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: invalid value encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3439: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(linpred)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in multiply\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: divide by zero encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3162: RuntimeWarning: invalid value encountered in multiply\n",
      "  dparams = (a4 * dgterm -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3123: RuntimeWarning: invalid value encountered in log\n",
      "  (y + a1) * np.log(a2))\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: invalid value encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3439: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(linpred)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in multiply\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: divide by zero encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3162: RuntimeWarning: invalid value encountered in multiply\n",
      "  dparams = (a4 * dgterm -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: invalid value encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: tensor([-1.2432e+01, -1.0491e+01, -3.2736e-05])\n",
      "counts: tensor([  1,   3, 791])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.506703\n",
      "         Iterations: 3\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "mu 0.24654088050306855\n",
      "theta 0.08575782319711077\n",
      "p 0.2270680212803873\n",
      "logit 1.2249414224218522\n",
      "output: tensor([-13.7452, -10.2398,  -8.3053,  -7.8855,  -7.4470,  -6.9832,  -6.4828,\n",
      "         -5.9248,  -5.2628,  -4.3534,  -0.0252])\n",
      "counts: tensor([  1,   1,   1,   1,   3,   3,   3,   4,  17,  54, 707])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.097332\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.13836477987421394\n",
      "theta 0.0029397849731835762\n",
      "p 0.13795920946332138\n",
      "logit 1.8323445324876688\n",
      "output: tensor([-1.3528e+01, -1.0921e+01, -1.0326e+01, -9.2866e+00, -8.9789e+00,\n",
      "        -8.5539e+00, -7.8426e+00, -1.5629e-03])\n",
      "counts: tensor([  1,   1,   1,   1,   2,   1,   2, 786])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.029737\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.006289308176100584\n",
      "theta 0.007913963061527389\n",
      "p 0.006239925635117585\n",
      "logit 5.070527538781605\n",
      "output: tensor([-1.2235e+01, -1.0735e+01, -2.8815e-05])\n",
      "counts: tensor([  2,   1, 792])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.747682\n",
      "         Iterations: 5\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "mu 0.6452830188677721\n",
      "theta 0.0701396099213638\n",
      "p 0.6029895659269998\n",
      "logit -0.4179373303322622\n",
      "output: tensor([-11.0995, -10.0777,  -9.1413,  -8.7224,  -8.2889,  -8.1404,  -7.9896,\n",
      "         -7.5206,  -7.3575,  -6.8410,  -6.6572,  -6.4657,  -6.2649,  -6.0527,\n",
      "         -5.8259,  -5.5802,  -5.3086,  -4.9997,  -4.6319,  -4.1577,  -3.4292,\n",
      "         -0.0950])\n",
      "counts: tensor([  1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   2,   2,   3,\n",
      "          3,   3,   5,   6,   6,  24,  56, 674])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.487078\n",
      "         Iterations: 3\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 12\n",
      "mu 0.25534591194914863\n",
      "theta 0.06954360663293727\n",
      "p 0.23874287160016863\n",
      "logit 1.1595840613092554\n",
      "output: tensor([-12.6279, -11.4869, -10.3040,  -9.3743,  -9.0527,  -7.2869,  -6.8776,\n",
      "         -6.4308,  -5.9252,  -5.3130,  -4.4462,  -0.0246])\n",
      "counts: tensor([  1,   1,   1,   1,   1,   1,   2,   2,   8,  11,  53, 713])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.009658\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.0012578616352201257\n",
      "theta 20.0\n",
      "p 5.989817310572027e-05\n",
      "logit 9.722804652410682\n",
      "output: tensor([-1.6403e+01, -7.5294e-08])\n",
      "counts: tensor([  1, 794])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.042223\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.016352201257861642\n",
      "theta 0.004264354649545183\n",
      "p 0.01628276576994313\n",
      "logit 4.101231258434309\n",
      "output: tensor([-1.2754e+01, -1.2341e+01, -9.8136e+00, -1.0906e-04])\n",
      "counts: tensor([  1,   1,   2, 791])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.202400\n",
      "         Iterations: 3\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 12\n",
      "mu 0.060377358490659644\n",
      "theta 0.05096202494818898\n",
      "p 0.05744961003099628\n",
      "logit 2.797681165751591\n",
      "output: tensor([-1.2718e+01, -8.7330e+00, -7.7407e+00, -6.4853e+00, -2.2456e-03])\n",
      "counts: tensor([  1,   3,   4,  23, 764])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.493150\n",
      "         Iterations: 4\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.21886792452824036\n",
      "theta 0.10287952042745513\n",
      "p 0.19845134529600417\n",
      "logit 1.3960017165426046\n",
      "output: tensor([-12.6127,  -9.8810,  -8.9114,  -8.4071,  -7.3374,  -6.7544,  -6.1151,\n",
      "         -5.3745,  -4.3940,  -0.0222])\n",
      "counts: tensor([  1,   1,   1,   1,   4,   4,   4,  12,  61, 706])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.283806\n",
      "         Iterations: 4\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 9\n",
      "mu 0.08050314465408737\n",
      "theta 0.3018892777313197\n",
      "p 0.0618356307491622\n",
      "logit 2.7194454197353584\n",
      "output: tensor([-1.1171e+01, -9.4210e+00, -7.5980e+00, -5.6105e+00, -4.2681e-03])\n",
      "counts: tensor([  1,   1,   4,  49, 740])\n",
      "(795,)\n",
      "(795,)\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.031659\n",
      "         Iterations: 1\n",
      "         Function evaluations: 124\n",
      "         Gradient evaluations: 112\n",
      "mu 0.005031446540880552\n",
      "theta 26.997358511388374\n",
      "p 0.0001797114730960755\n",
      "logit 8.623978192757319\n",
      "output: tensor([-1.3921e+01, -9.0185e-07])\n",
      "counts: tensor([  4, 791])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.009658\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.0012578616352201257\n",
      "theta 20.0\n",
      "p 5.989817310572027e-05\n",
      "logit 9.722804652410682\n",
      "output: tensor([-1.6403e+01, -7.5294e-08])\n",
      "counts: tensor([  1, 794])\n",
      "(795,)\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.336383\n",
      "         Iterations: 1\n",
      "         Function evaluations: 124\n",
      "         Gradient evaluations: 112\n",
      "mu 0.24402515723270635\n",
      "theta 0.006028197681645216\n",
      "p 0.2425629398808635\n",
      "logit 1.1386792223780156\n",
      "output: tensor([-1.3037e+01, -9.7780e+00, -9.6933e+00, -9.3098e+00, -8.9508e+00,\n",
      "        -8.0231e+00, -7.7131e+00, -7.2862e+00, -6.5747e+00, -5.4009e-03])\n",
      "counts: tensor([  1,   1,   1,   1,   1,   1,   2,  10,  25, 752])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.348764\n",
      "         Iterations: 4\n",
      "         Function evaluations: 13\n",
      "         Gradient evaluations: 13\n",
      "mu 0.19119496855393464\n",
      "theta 0.03656648917698182\n",
      "p 0.18445026976102666\n",
      "logit 1.4864825149307337\n",
      "output: tensor([-11.4330, -10.7516, -10.5183,  -8.4515,  -7.8251,  -7.4751,  -6.6355,\n",
      "         -6.0731,  -5.2409,  -0.0120])\n",
      "counts: tensor([  1,   1,   2,   2,   1,   1,   2,   8,  34, 743])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.056675\n",
      "         Iterations: 2\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "mu 0.011320754717071892\n",
      "theta 0.023725956799973705\n",
      "p 0.011058383976566357\n",
      "logit 4.493446425352875\n",
      "output: tensor([-1.1185e+01, -9.3851e+00, -1.0189e-04])\n",
      "counts: tensor([  2,   5, 788])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.041648\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.0075471698113207565\n",
      "theta 0.02313688351424205\n",
      "p 0.00737650057673412\n",
      "logit 4.902052087927705\n",
      "output: tensor([-1.2158e+01, -1.0085e+01, -4.8027e-05])\n",
      "counts: tensor([  1,   4, 790])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.438052\n",
      "         Iterations: 3\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "mu 0.18867924528301444\n",
      "theta 0.08541267690044227\n",
      "p 0.17383180544916443\n",
      "logit 1.5587101817846933\n",
      "output: tensor([-12.7789, -10.5702,  -8.1518,  -7.6130,  -7.0376,  -6.4046,  -5.6675,\n",
      "         -4.6829,  -0.0166])\n",
      "counts: tensor([  1,   1,   3,   1,   4,   3,  12,  51, 719])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.495714\n",
      "         Iterations: 4\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 9\n",
      "mu 0.23522012578615076\n",
      "theta 0.08293644575873228\n",
      "p 0.21720584500353543\n",
      "logit 1.2820242703082605\n",
      "output: tensor([-8.4137, -7.9899, -7.5474, -7.0795, -6.5749, -6.0125, -5.3456, -4.4301,\n",
      "        -0.0232])\n",
      "counts: tensor([  3,   2,   5,   1,   4,   6,  17,  46, 711])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.347610\n",
      "         Iterations: 3\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.15597484276718135\n",
      "theta 0.047081888880060124\n",
      "p 0.14896145604620212\n",
      "logit 1.7427698318631117\n",
      "output: tensor([-1.1542e+01, -9.0981e+00, -8.7075e+00, -8.2973e+00, -7.8606e+00,\n",
      "        -7.3854e+00, -6.8495e+00, -6.2035e+00, -5.2925e+00, -9.9554e-03])\n",
      "counts: tensor([  1,   1,   1,   1,   2,   4,   6,   7,  30, 742])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.570493\n",
      "         Iterations: 5\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.40125786163516874\n",
      "theta 0.051946604780208794\n",
      "p 0.3814431833438985\n",
      "logit 0.48342713963072415\n",
      "output: tensor([-9.2822, -8.2141, -7.8248, -7.6208, -7.4089, -7.1876, -6.9545, -6.7066,\n",
      "        -6.4394, -6.1456, -5.8137, -5.4214, -4.9198, -4.1556, -0.0414])\n",
      "counts: tensor([  1,   2,   1,   1,   1,   1,   3,   5,   5,   7,   2,  12,  18,  25,\n",
      "        711])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.107944\n",
      "         Iterations: 3\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "mu 0.025157232704437153\n",
      "theta 0.036515745526698846\n",
      "p 0.024270960487583974\n",
      "logit 3.6939043303665713\n",
      "output: tensor([-1.0782e+01, -9.4983e+00, -7.9443e+00, -4.6019e-04])\n",
      "counts: tensor([  1,   3,  11, 780])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.274690\n",
      "         Iterations: 3\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "mu 0.0842767295594339\n",
      "theta 0.10342538117128094\n",
      "p 0.07637737086487402\n",
      "logit 2.4926171187621122\n",
      "output: tensor([-1.4033e+01, -1.1270e+01, -8.2546e+00, -7.0988e+00, -5.7034e+00,\n",
      "        -4.5763e-03])\n",
      "counts: tensor([  1,   1,   1,   4,  41, 747])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.009658\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.0012578616352201257\n",
      "theta 20.0\n",
      "p 5.989817310572027e-05\n",
      "logit 9.722804652410682\n",
      "output: tensor([-1.6403e+01, -7.5294e-08])\n",
      "counts: tensor([  1, 794])\n",
      "(795,)\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: nan\n",
      "         Iterations: 1\n",
      "         Function evaluations: 112\n",
      "         Gradient evaluations: 112\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.414844\n",
      "         Iterations: 4\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 12\n",
      "mu 0.343396226414437\n",
      "theta 0.028102017701123347\n",
      "p 0.33400987499497814\n",
      "logit 0.6901042847178093\n",
      "output: tensor([-13.5193,  -9.5826,  -9.0824,  -8.0856,  -7.9225,  -7.5704,  -7.3775,\n",
      "         -7.1693,  -6.9412,  -6.6858,  -6.3910,  -6.0340,  -5.5638,  -4.8197,\n",
      "         -0.0237])\n",
      "counts: tensor([  1,   1,   1,   2,   1,   2,   2,   1,   2,   1,   6,   3,  11,  22,\n",
      "        739])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.024830\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.0037735849056603783\n",
      "theta 20.0\n",
      "p 0.00017969451931716085\n",
      "logit 8.624072553043334\n",
      "output: tensor([-1.4208e+01, -6.7675e-07])\n",
      "counts: tensor([  3, 792])\n",
      "(795,)\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.038170\n",
      "         Iterations: 1\n",
      "         Function evaluations: 124\n",
      "         Gradient evaluations: 112\n",
      "mu 0.006289308176100808\n",
      "theta 33.61131987639512\n",
      "p 0.0001817124628174064\n",
      "logit 8.612903265860526\n",
      "output: tensor([-1.3688e+01, -1.1392e-06])\n",
      "counts: tensor([  5, 790])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.677615\n",
      "         Iterations: 2\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "mu 0.6477987421388878\n",
      "theta 0.05451464242346712\n",
      "p 0.6143098597949556\n",
      "logit -0.46546515698753016\n",
      "output: tensor([-14.6674, -13.1800, -10.6405,  -7.1350,  -6.8280,  -6.6651,  -6.4945,\n",
      "         -6.3143,  -6.1225,  -5.9160,  -5.6901,  -5.4378,  -5.1474,  -4.7969,\n",
      "         -4.3375,  -3.6167,  -0.0833])\n",
      "counts: tensor([  1,   1,   1,   1,   3,   3,   2,   2,   1,   1,   4,   5,   6,   6,\n",
      "         20,  48, 690])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.141634\n",
      "         Iterations: 1\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.04402515723284357\n",
      "theta 0.021099075617702846\n",
      "p 0.04311546086378644\n",
      "logit 3.0998010818915143\n",
      "output: tensor([-1.1119e+01, -1.0550e+01, -9.9402e+00, -8.4815e+00, -7.4177e+00,\n",
      "        -1.0137e-03])\n",
      "counts: tensor([  1,   1,   2,   1,  14, 776])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.027824\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.005031446540880504\n",
      "theta 0.010152397848689\n",
      "p 0.004980878678896296\n",
      "logit 5.297155637364061\n",
      "output: tensor([-1.2788e+01, -1.1001e+01, -2.0313e-05])\n",
      "counts: tensor([  1,   2, 792])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.263434\n",
      "         Iterations: 4\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "mu 0.1358490566039966\n",
      "theta 0.023642772176182335\n",
      "p 0.1327113914116664\n",
      "logit 1.877195022268574\n",
      "output: tensor([-1.0387e+01, -1.0149e+01, -9.1116e+00, -8.8209e+00, -8.5103e+00,\n",
      "        -7.7945e+00, -7.3542e+00, -6.8001e+00, -5.9699e+00, -5.8737e-03])\n",
      "counts: tensor([  1,   1,   1,   1,   3,   5,   1,   5,  17, 760])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.038280\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.06289308176100658\n",
      "theta 0.0015791488077665315\n",
      "p 0.0627939208158153\n",
      "logit 2.7030449266588525\n",
      "output: tensor([-1.4140e+01, -1.0707e+01, -9.2494e+00, -3.6681e-04])\n",
      "counts: tensor([  1,   1,   1, 792])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.713721\n",
      "         Iterations: 4\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "mu 1.329559748425537\n",
      "theta 0.03413182558923479\n",
      "p 1.2856772372013319\n",
      "logit nan\n",
      "output: tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan])\n",
      "counts: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.655332\n",
      "         Iterations: 5\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 9\n",
      "mu 0.27421383647799047\n",
      "theta 0.43203373428674846\n",
      "p 0.19148559835747714\n",
      "logit 1.440385890257641\n",
      "output: tensor([-15.3577,  -8.2525,  -7.1858,  -6.0866,  -4.9307,  -3.6506,  -0.0373])\n",
      "counts: tensor([  1,   1,   3,   7,  26, 116, 641])\n",
      "(795,)\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.355106\n",
      "         Iterations: 3\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 12\n",
      "mu 0.2088050314469857\n",
      "theta 0.034277253776794796\n",
      "p 0.20188496912651574\n",
      "logit 1.374554660597927\n",
      "output: tensor([-10.9555,  -9.6994,  -9.4760,  -9.2468,  -8.7670,  -7.6668,  -7.3393,\n",
      "         -6.9727,  -6.0039,  -5.1925,  -0.0132])\n",
      "counts: tensor([  1,   2,   1,   2,   1,   2,   1,   3,   3,  36, 743])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.094690\n",
      "         Iterations: 2\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 12\n",
      "mu 0.04654088050107775\n",
      "theta 0.006687482100578605\n",
      "p 0.04623170678944414\n",
      "logit 3.026754905874758\n",
      "output: tensor([-1.1722e+01, -1.1483e+01, -1.0362e+01, -1.0007e+01, -9.5869e+00,\n",
      "        -9.0505e+00, -8.2297e+00, -6.3711e-04])\n",
      "counts: tensor([  1,   1,   1,   1,   1,   1,   4, 785])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.009658\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.0012578616352201257\n",
      "theta 20.0\n",
      "p 5.989817310572027e-05\n",
      "logit 9.722804652410682\n",
      "output: tensor([-1.6403e+01, -7.5294e-08])\n",
      "counts: tensor([  1, 794])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.575498\n",
      "         Iterations: 3\n",
      "         Function evaluations: 8\n",
      "         Gradient evaluations: 8\n",
      "mu 0.2679245283019247\n",
      "theta 0.1243857070381726\n",
      "p 0.23828524911410023\n",
      "logit 1.162103661231094\n",
      "output: tensor([-10.3583,  -7.9830,  -7.4680,  -6.9289,  -6.3550,  -5.7266,  -5.0001,\n",
      "         -4.0429,  -0.0322])\n",
      "counts: tensor([  1,   1,   4,   3,   5,  12,  19,  61, 689])\n",
      "(795,)\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.557020\n",
      "         Iterations: 5\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.3547169811320106\n",
      "theta 0.05783783149899493\n",
      "p 0.335322646411089\n",
      "logit 0.6842085484093449\n",
      "output: tensor([-10.4552,  -8.4081,  -8.1818,  -7.4584,  -6.9204,  -6.6248,  -6.3029,\n",
      "         -5.9431,  -5.5234,  -4.9954,  -4.2075,  -0.0367])\n",
      "counts: tensor([  1,   1,   1,   3,   3,   6,   3,   9,  14,  13,  31, 710])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.540306\n",
      "         Iterations: 4\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 9\n",
      "mu 0.2301886792452281\n",
      "theta 0.14839989076975182\n",
      "p 0.20044296511639054\n",
      "logit 1.3835281249010636\n",
      "output: tensor([-11.1062,  -8.7345,  -8.1072,  -7.4566,  -6.7723,  -6.0354,  -5.2039,\n",
      "         -4.1516,  -0.0264])\n",
      "counts: tensor([  1,   1,   2,   4,   3,   5,  18,  70, 691])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.060836\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.02767295597484261\n",
      "theta 0.004699231961664179\n",
      "p 0.027543522573229673\n",
      "logit 3.564057928556757\n",
      "output: tensor([-1.3071e+01, -1.0966e+01, -1.0524e+01, -9.9636e+00, -9.1183e+00,\n",
      "        -2.4869e-04])\n",
      "counts: tensor([  1,   1,   1,   1,   2, 789])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.034919\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.006289308176100629\n",
      "theta 0.015954347258223322\n",
      "p 0.006190542117442298\n",
      "logit 5.0785228335940635\n",
      "output: tensor([-1.2432e+01, -1.0491e+01, -3.2736e-05])\n",
      "counts: tensor([  1,   3, 791])\n",
      "(795,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3439: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(linpred)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in multiply\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: divide by zero encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3162: RuntimeWarning: invalid value encountered in multiply\n",
      "  dparams = (a4 * dgterm -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3123: RuntimeWarning: invalid value encountered in log\n",
      "  (y + a1) * np.log(a2))\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: invalid value encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: divide by zero encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in multiply\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3156: RuntimeWarning: invalid value encountered in divide\n",
      "  a4 = p * a1 / mu\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3164: RuntimeWarning: divide by zero encountered in divide\n",
      "  y / mu)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3164: RuntimeWarning: invalid value encountered in divide\n",
      "  y / mu)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3123: RuntimeWarning: invalid value encountered in log\n",
      "  (y + a1) * np.log(a2))\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3123: RuntimeWarning: invalid value encountered in log\n",
      "  (y + a1) * np.log(a2))\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: invalid value encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3439: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(linpred)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in multiply\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: divide by zero encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3162: RuntimeWarning: invalid value encountered in multiply\n",
      "  dparams = (a4 * dgterm -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/var/folders/12/d54y_pk95zdbftjg_qkcj_c80000gq/T/ipykernel_42688/534514316.py:123: RuntimeWarning: invalid value encountered in log\n",
      "  logit=np.log((1-p)/p)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3123: RuntimeWarning: invalid value encountered in log\n",
      "  (y + a1) * np.log(a2))\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: invalid value encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3439: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(linpred)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in multiply\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: divide by zero encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3162: RuntimeWarning: invalid value encountered in multiply\n",
      "  dparams = (a4 * dgterm -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3123: RuntimeWarning: invalid value encountered in log\n",
      "  (y + a1) * np.log(a2))\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: invalid value encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3439: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(linpred)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in multiply\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: divide by zero encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3162: RuntimeWarning: invalid value encountered in multiply\n",
      "  dparams = (a4 * dgterm -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/var/folders/12/d54y_pk95zdbftjg_qkcj_c80000gq/T/ipykernel_42688/534514316.py:123: RuntimeWarning: invalid value encountered in log\n",
      "  logit=np.log((1-p)/p)\n",
      "/var/folders/12/d54y_pk95zdbftjg_qkcj_c80000gq/T/ipykernel_42688/534514316.py:123: RuntimeWarning: invalid value encountered in log\n",
      "  logit=np.log((1-p)/p)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.711981\n",
      "         Iterations: 3\n",
      "         Function evaluations: 115\n",
      "         Gradient evaluations: 115\n",
      "mu 0.39496855345918325\n",
      "theta 0.13767716793484833\n",
      "p 0.34717102934934\n",
      "logit 0.6314976432349193\n",
      "output: tensor([-17.6645,  -9.6289,  -8.5250,  -7.3553,  -6.5117,  -6.0575,  -5.5691,\n",
      "         -5.0273,  -4.3893,  -3.5261,  -0.0608])\n",
      "counts: tensor([  1,   2,   1,   3,   3,   1,   5,  13,  22,  87, 657])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.142872\n",
      "         Iterations: 3\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.033962264151028154\n",
      "theta 0.07624499743746713\n",
      "p 0.03155625738739051\n",
      "logit 3.4239184923005226\n",
      "output: tensor([-1.0577e+01, -9.0318e+00, -7.2350e+00, -8.7445e-04])\n",
      "counts: tensor([  1,   3,  18, 773])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.081093\n",
      "         Iterations: 1\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "mu 0.021383647798648656\n",
      "theta 0.010612932749558414\n",
      "p 0.021159087822545976\n",
      "logit 3.8342996315108713\n",
      "output: tensor([-1.2599e+01, -1.0705e+01, -8.8161e+00, -2.4640e-04])\n",
      "counts: tensor([  1,   1,   8, 785])\n",
      "(795,)\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.031659\n",
      "         Iterations: 1\n",
      "         Function evaluations: 124\n",
      "         Gradient evaluations: 112\n",
      "mu 0.005031446540880557\n",
      "theta 26.997358511388374\n",
      "p 0.00017971147309607568\n",
      "logit 8.623978192757319\n",
      "output: tensor([-1.3921e+01, -9.0185e-07])\n",
      "counts: tensor([  4, 791])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.113881\n",
      "         Iterations: 2\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "mu 0.036477987421118005\n",
      "theta 0.014648915820432512\n",
      "p 0.03595133927839696\n",
      "logit 3.2889754338711588\n",
      "output: tensor([-1.1841e+01, -1.0277e+01, -9.6569e+00, -8.9211e+00, -7.9049e+00,\n",
      "        -6.5250e-04])\n",
      "counts: tensor([  1,   1,   1,   4,   7, 781])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.899108\n",
      "         Iterations: 3\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "mu 1.135849056603594\n",
      "theta 0.0642772907142482\n",
      "p 1.0672491713520573\n",
      "logit nan\n",
      "output: tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan])\n",
      "counts: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.677750\n",
      "         Iterations: 5\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.5610062893081347\n",
      "theta 0.05892430368085701\n",
      "p 0.5297888502115379\n",
      "logit -0.11929668264976077\n",
      "output: tensor([-8.9632, -8.0936, -7.9400, -7.2886, -7.1135, -6.7427, -6.5440, -6.3337,\n",
      "        -6.1086, -5.8644, -5.5939, -5.2855, -4.9174, -4.4411, -3.7053, -0.0711])\n",
      "counts: tensor([  1,   2,   3,   3,   2,   4,   2,   4,   2,   3,   5,   5,  10,  18,\n",
      "         39, 692])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 1.883337\n",
      "         Iterations: 5\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 2.578616352201276\n",
      "theta 0.2787259919896823\n",
      "p 2.0165511363298245\n",
      "logit nan\n",
      "output: tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan])\n",
      "counts: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.020233\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.0037735849056603783\n",
      "theta 0.005685439117826078\n",
      "p 0.003752251707025327\n",
      "logit 5.581639854931183\n",
      "output: tensor([-1.3284e+01, -1.1677e+01, -1.0844e-05])\n",
      "counts: tensor([  1,   1, 793])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.474628\n",
      "         Iterations: 5\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "mu 0.349685534591246\n",
      "theta 0.03702952505490207\n",
      "p 0.3371992080676132\n",
      "logit 0.6758006028092722\n",
      "output: tensor([-9.3485, -9.0458, -8.5685, -8.2296, -8.0520, -7.6754, -7.4735, -7.2597,\n",
      "        -7.0308, -6.7821, -6.5065, -6.1919, -5.8159, -5.3281, -4.5706, -0.0285])\n",
      "counts: tensor([  1,   1,   1,   1,   1,   2,   2,   2,   3,   3,   5,   7,   6,   5,\n",
      "         26, 729])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.677008\n",
      "         Iterations: 4\n",
      "         Function evaluations: 8\n",
      "         Gradient evaluations: 8\n",
      "mu 0.29056603773584366\n",
      "theta 0.3976431385088636\n",
      "p 0.20789715895993785\n",
      "logit 1.3376477038284986\n",
      "output: tensor([-11.6888,  -9.8167,  -8.8645,  -7.8964,  -6.9058,  -5.8803,  -4.7939,\n",
      "         -3.5733,  -0.0416])\n",
      "counts: tensor([  1,   1,   1,   1,   2,  11,  22, 119, 637])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.621070\n",
      "         Iterations: 3\n",
      "         Function evaluations: 8\n",
      "         Gradient evaluations: 8\n",
      "mu 0.284276729559759\n",
      "theta 0.16643528699632296\n",
      "p 0.2437141028987536\n",
      "logit 1.1324236465861526\n",
      "output: tensor([-9.8309, -8.1540, -7.5664, -6.9559, -6.3126, -5.6181, -4.8317, -3.8316,\n",
      "        -0.0379])\n",
      "counts: tensor([  1,   3,   3,   2,   3,   9,  27,  74, 673])\n",
      "(795,)\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.044428\n",
      "         Iterations: 1\n",
      "         Function evaluations: 124\n",
      "         Gradient evaluations: 112\n",
      "mu 0.007547169811320677\n",
      "theta 47.97559473333018\n",
      "p 0.0001541006260855976\n",
      "logit 8.777750640292973\n",
      "output: tensor([-1.3672e+01, -1.1586e-06])\n",
      "counts: tensor([  6, 789])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.522881\n",
      "         Iterations: 4\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 12\n",
      "mu 0.4113207547175752\n",
      "theta 0.042899606464252775\n",
      "p 0.3944011026258585\n",
      "logit 0.4288494629760889\n",
      "output: tensor([-13.6821,  -9.4409,  -8.3995,  -8.0735,  -7.9035,  -7.7278,  -7.5455,\n",
      "         -7.1554,  -6.7171,  -6.4709,  -6.1979,  -5.8862,  -5.5135,  -5.0301,\n",
      "         -4.2797,  -0.0387])\n",
      "counts: tensor([  1,   1,   1,   1,   2,   1,   2,   2,   3,   1,   2,   6,   5,  14,\n",
      "         35, 718])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 1.173266\n",
      "         Iterations: 5\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 12\n",
      "mu 2.932075471697383\n",
      "theta 0.061614151624503066\n",
      "p 2.7619031521110218\n",
      "logit nan\n",
      "output: tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan])\n",
      "counts: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.390720\n",
      "         Iterations: 4\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "mu 0.21509433962289404\n",
      "theta 0.041100389301163595\n",
      "p 0.20660288079161668\n",
      "logit 1.3455253767282198\n",
      "output: tensor([-11.8508, -10.5121,  -9.5558,  -8.2169,  -7.5922,  -7.2432,  -6.8554,\n",
      "         -6.4065,  -5.8465,  -5.0188,  -0.0151])\n",
      "counts: tensor([  1,   1,   1,   1,   1,   3,   6,   8,   5,  31, 737])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.069507\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.03144654088050314\n",
      "theta 0.005640061770752087\n",
      "p 0.031270175161012784\n",
      "logit 3.4333209814565677\n",
      "output: tensor([-1.2850e+01, -1.1076e+01, -1.0239e+01, -9.6710e+00, -8.8185e+00,\n",
      "        -3.3046e-04])\n",
      "counts: tensor([  1,   1,   1,   2,   2, 788])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 1.220415\n",
      "         Iterations: 2\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 9\n",
      "mu 1.4691823899371215\n",
      "theta 0.1185578703734309\n",
      "p 1.3134612243590351\n",
      "logit nan\n",
      "output: tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan])\n",
      "counts: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.551026\n",
      "         Iterations: 3\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 12\n",
      "mu 1.025157232712623\n",
      "theta 0.023812475894732545\n",
      "p 1.0013134796161918\n",
      "logit nan\n",
      "output: tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan])\n",
      "counts: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.049775\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.01886792452830174\n",
      "theta 0.0037331502804563766\n",
      "p 0.01879774970372334\n",
      "logit 3.955041439732903\n",
      "output: tensor([-1.3383e+01, -1.0622e+01, -9.7518e+00, -1.2595e-04])\n",
      "counts: tensor([  1,   2,   2, 790])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.771422\n",
      "         Iterations: 4\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 9\n",
      "mu 0.7169811320755713\n",
      "theta 0.0649255762301082\n",
      "p 0.6732687692727991\n",
      "logit -0.7230067013604926\n",
      "output: tensor([-10.0648,  -8.5592,  -7.5223,  -7.2390,  -6.9411,  -6.7853,  -6.6239,\n",
      "         -6.4561,  -6.2806,  -6.0957,  -5.8994,  -5.6884,  -5.4583,  -5.2022,\n",
      "         -4.9085,  -4.5555,  -4.0953,  -3.3784,  -0.1059])\n",
      "counts: tensor([  1,   1,   1,   3,   3,   1,   1,   1,   7,   5,   3,   7,   5,   7,\n",
      "          4,   9,   5,  55, 676])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.120299\n",
      "         Iterations: 3\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 12\n",
      "mu 0.051572327044496796\n",
      "theta 0.00987253527410145\n",
      "p 0.05106815488402103\n",
      "logit 2.922175867599058\n",
      "output: tensor([-1.1358e+01, -1.1066e+01, -1.0075e+01, -9.6795e+00, -8.6443e+00,\n",
      "        -7.7858e+00, -9.1397e-04])\n",
      "counts: tensor([  1,   1,   2,   1,   1,   8, 781])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.234096\n",
      "         Iterations: 2\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.09685534591192682\n",
      "theta 0.02715865603014294\n",
      "p 0.09429443576541745\n",
      "logit 2.2622920867060485\n",
      "output: tensor([-1.1046e+01, -9.9956e+00, -8.7979e+00, -8.3343e+00, -7.8085e+00,\n",
      "        -7.1693e+00, -6.2558e+00, -3.8174e-03])\n",
      "counts: tensor([  1,   1,   2,   2,   2,  10,  14, 763])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.558276\n",
      "         Iterations: 3\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 9\n",
      "mu 0.40880503144651326\n",
      "theta 0.05036111430527325\n",
      "p 0.3892042706825679\n",
      "logit 0.45065825814239874\n",
      "output: tensor([-10.8797,  -8.8537,  -8.3323,  -7.9644,  -7.7723,  -7.1513,  -6.9236,\n",
      "         -6.6811,  -6.4191,  -6.1306,  -5.8038,  -5.4166,  -4.9199,  -4.1597,\n",
      "         -0.0419])\n",
      "counts: tensor([  1,   1,   2,   2,   2,   1,   3,   2,   4,   5,   4,   9,   9,  39,\n",
      "        711])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.556113\n",
      "         Iterations: 5\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.31194968553459423\n",
      "theta 0.07191953486575509\n",
      "p 0.2910196851423761\n",
      "logit 0.8904368500679446\n",
      "output: tensor([-8.1646, -7.8597, -7.5434, -7.2127, -6.8630, -6.4875, -6.0747, -5.6032,\n",
      "        -5.0256, -4.1945, -0.0336])\n",
      "counts: tensor([  1,   2,   2,   4,   3,   5,  12,   5,  14,  42, 705])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.676583\n",
      "         Iterations: 4\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.6943396226411291\n",
      "theta 0.04651817769752216\n",
      "p 0.6634759313677359\n",
      "logit -0.6788229049355157\n",
      "output: tensor([-9.7819, -8.9622, -8.1693, -8.0639, -7.8476, -7.7363, -7.6226, -7.3870,\n",
      "        -7.1381, -6.8722, -6.7312, -6.5835, -6.4280, -6.2630, -6.0861, -5.8944,\n",
      "        -5.6831, -5.4452, -5.1688, -4.8316, -4.3843, -3.6718, -0.0836])\n",
      "counts: tensor([  1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   3,   5,   4,\n",
      "          5,   2,   4,   2,   7,   7,  13,  33, 699])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.485281\n",
      "         Iterations: 4\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.2465408805031024\n",
      "theta 0.07104957485103831\n",
      "p 0.2301862456155602\n",
      "logit 1.2072598633803688\n",
      "output: tensor([-10.1819,  -9.1988,  -8.8573,  -8.5066,  -8.1444,  -7.7677,  -7.3721,\n",
      "         -6.9507,  -6.4919,  -5.9744,  -5.3506,  -4.4729,  -0.0235])\n",
      "counts: tensor([  1,   1,   2,   2,   1,   1,   1,   1,   6,   4,  10,  51, 714])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.009658\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.0012578616352201257\n",
      "theta 20.0\n",
      "p 5.989817310572027e-05\n",
      "logit 9.722804652410682\n",
      "output: tensor([-1.6403e+01, -7.5294e-08])\n",
      "counts: tensor([  1, 794])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.503392\n",
      "         Iterations: 4\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 9\n",
      "mu 0.21886792452831078\n",
      "theta 0.11132238472824528\n",
      "p 0.19694368397144377\n",
      "logit 1.4055070238235325\n",
      "output: tensor([-11.0335,  -7.9545,  -7.3830,  -6.7761,  -6.1137,  -5.3512,  -4.3524,\n",
      "         -0.0227])\n",
      "counts: tensor([  1,   4,   2,   3,  11,  13,  57, 704])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.009658\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.0012578616352201257\n",
      "theta 20.0\n",
      "p 5.989817310572027e-05\n",
      "logit 9.722804652410682\n",
      "output: tensor([-1.6403e+01, -7.5294e-08])\n",
      "counts: tensor([  1, 794])\n",
      "(795,)\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.017573\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.0025157232704402514\n",
      "theta 20.0\n",
      "p 0.00011979634621144054\n",
      "logit 9.029597568295443\n",
      "output: tensor([-1.5018e+01, -3.0098e-07])\n",
      "counts: tensor([  2, 793])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.606462\n",
      "         Iterations: 5\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.30314465408806723\n",
      "theta 0.11981226951469917\n",
      "p 0.2707102452265888\n",
      "logit 0.9910220787983686\n",
      "output: tensor([-9.7100, -9.3008, -8.8843, -8.0231, -7.5735, -7.1061, -6.6144, -6.0877,\n",
      "        -5.5061, -4.8258, -3.9127, -0.0387])\n",
      "counts: tensor([  1,   1,   1,   2,   3,   3,   2,   6,   7,  11,  76, 682])\n",
      "(795,)\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: nan\n",
      "         Iterations: 1\n",
      "         Function evaluations: 112\n",
      "         Gradient evaluations: 112\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.120161\n",
      "         Iterations: 4\n",
      "         Function evaluations: 13\n",
      "         Gradient evaluations: 13\n",
      "mu 0.03018867924516885\n",
      "theta 0.03384985789612229\n",
      "p 0.02920025477065172\n",
      "logit 3.503942777160203\n",
      "output: tensor([-1.3168e+01, -1.0246e+01, -7.6930e+00, -6.2361e-04])\n",
      "counts: tensor([  1,   1,  15, 778])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.009658\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.0012578616352201257\n",
      "theta 20.0\n",
      "p 5.989817310572027e-05\n",
      "logit 9.722804652410682\n",
      "output: tensor([-1.6403e+01, -7.5294e-08])\n",
      "counts: tensor([  1, 794])\n",
      "(795,)\n",
      " data for (contrl, control): None\n",
      "(795,)\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.038170\n",
      "         Iterations: 1\n",
      "         Function evaluations: 124\n",
      "         Gradient evaluations: 112\n",
      "mu 0.006289308176100685\n",
      "theta 33.61131987639512\n",
      "p 0.00018171246281740284\n",
      "logit 8.612903265860547\n",
      "output: tensor([-1.3688e+01, -1.1392e-06])\n",
      "counts: tensor([  5, 790])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.215580\n",
      "         Iterations: 3\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.07044025157241413\n",
      "theta 0.04111245130913337\n",
      "p 0.06765863906808524\n",
      "logit 2.6232239658462433\n",
      "output: tensor([-1.0990e+01, -9.7494e+00, -8.3429e+00, -7.4981e+00, -6.3855e+00,\n",
      "        -2.7241e-03])\n",
      "counts: tensor([  1,   2,   2,   6,  21, 763])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.608128\n",
      "         Iterations: 4\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.6729559748427939\n",
      "theta 0.03908113820384626\n",
      "p 0.647645260894702\n",
      "logit -0.608704659886767\n",
      "output: tensor([-10.3506,  -9.9672,  -8.6420,  -8.2737,  -7.9816,  -7.7772,  -7.4519,\n",
      "         -7.0962,  -6.8354,  -6.6955,  -6.5476,  -6.3902,  -5.8323,  -5.6013,\n",
      "         -5.3314,  -5.0002,  -4.5577,  -3.8464,  -0.0720])\n",
      "counts: tensor([  1,   1,   1,   1,   4,   2,   3,   1,   1,   1,   3,   2,   4,   1,\n",
      "          2,   6,  15,  37, 709])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.657534\n",
      "         Iterations: 3\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 9\n",
      "mu 0.6515723270439794\n",
      "theta 0.047004231107443305\n",
      "p 0.6223206245831449\n",
      "logit -0.4994098110343273\n",
      "output: tensor([-9.7475, -9.0660, -8.8631, -8.5502, -8.3347, -7.8825, -7.6429, -7.3915,\n",
      "        -7.2604, -6.8392, -6.5265, -6.3567, -6.1751, -5.9786, -5.7626, -5.5200,\n",
      "        -5.2389, -4.8971, -4.4452, -3.7283, -0.0770])\n",
      "counts: tensor([  1,   1,   1,   1,   1,   1,   2,   1,   1,   4,   2,   2,   5,   1,\n",
      "          2,   5,   6,   9,  13,  36, 700])\n",
      "(795,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3123: RuntimeWarning: invalid value encountered in log\n",
      "  (y + a1) * np.log(a2))\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: invalid value encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: divide by zero encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in multiply\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3156: RuntimeWarning: invalid value encountered in divide\n",
      "  a4 = p * a1 / mu\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3164: RuntimeWarning: divide by zero encountered in divide\n",
      "  y / mu)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3164: RuntimeWarning: invalid value encountered in divide\n",
      "  y / mu)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/var/folders/12/d54y_pk95zdbftjg_qkcj_c80000gq/T/ipykernel_42688/534514316.py:123: RuntimeWarning: invalid value encountered in log\n",
      "  logit=np.log((1-p)/p)\n",
      "/var/folders/12/d54y_pk95zdbftjg_qkcj_c80000gq/T/ipykernel_42688/534514316.py:123: RuntimeWarning: invalid value encountered in log\n",
      "  logit=np.log((1-p)/p)\n",
      "/var/folders/12/d54y_pk95zdbftjg_qkcj_c80000gq/T/ipykernel_42688/534514316.py:123: RuntimeWarning: invalid value encountered in log\n",
      "  logit=np.log((1-p)/p)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3123: RuntimeWarning: invalid value encountered in log\n",
      "  (y + a1) * np.log(a2))\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: invalid value encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3439: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(linpred)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in multiply\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: divide by zero encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3162: RuntimeWarning: invalid value encountered in multiply\n",
      "  dparams = (a4 * dgterm -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3123: RuntimeWarning: invalid value encountered in log\n",
      "  (y + a1) * np.log(a2))\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3123: RuntimeWarning: invalid value encountered in log\n",
      "  (y + a1) * np.log(a2))\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: invalid value encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3439: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(linpred)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in multiply\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: divide by zero encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3162: RuntimeWarning: invalid value encountered in multiply\n",
      "  dparams = (a4 * dgterm -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3123: RuntimeWarning: invalid value encountered in log\n",
      "  (y + a1) * np.log(a2))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.462693\n",
      "         Iterations: 4\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 12\n",
      "mu 0.24402515723288185\n",
      "theta 0.05971120553204695\n",
      "p 0.23027515039851318\n",
      "logit 1.2067582128075873\n",
      "output: tensor([-13.3623,  -8.3914,  -8.0622,  -7.3551,  -6.9658,  -6.5386,  -6.0517,\n",
      "         -5.4568,  -4.6027,  -0.0215])\n",
      "counts: tensor([  1,   3,   1,   3,   3,   4,   5,  14,  40, 721])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.027824\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.005031446540880504\n",
      "theta 0.010152397848688998\n",
      "p 0.004980878678896296\n",
      "logit 5.297155637364061\n",
      "output: tensor([-1.2788e+01, -1.1001e+01, -2.0313e-05])\n",
      "counts: tensor([  1,   2, 792])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.436862\n",
      "         Iterations: 3\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 9\n",
      "mu 0.19496855345916406\n",
      "theta 0.07479803945879589\n",
      "p 0.18140017594127597\n",
      "logit 1.5068898415404235\n",
      "output: tensor([-9.7383, -8.8833, -8.4357, -7.9692, -7.4770, -6.9477, -6.3599, -5.6664,\n",
      "        -4.7207, -0.0167])\n",
      "counts: tensor([  1,   1,   2,   3,   3,   1,   6,  12,  44, 722])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.244188\n",
      "         Iterations: 4\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 12\n",
      "mu 0.10817610062923917\n",
      "theta 0.026841755146616866\n",
      "p 0.10534836559484603\n",
      "logit 2.1391617811216466\n",
      "output: tensor([-1.2142e+01, -1.0355e+01, -9.6952e+00, -9.3438e+00, -8.1358e+00,\n",
      "        -7.6354e+00, -7.0216e+00, -6.1333e+00, -4.4805e-03])\n",
      "counts: tensor([  1,   1,   1,   1,   1,   2,   8,  19, 761])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.379791\n",
      "         Iterations: 4\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 9\n",
      "mu 0.12327044025155538\n",
      "theta 0.24500012790248343\n",
      "p 0.09901239163664627\n",
      "logit 2.208246493961281\n",
      "output: tensor([-1.0428e+01, -9.1697e+00, -7.8661e+00, -6.4817e+00, -4.9133e+00,\n",
      "        -9.4536e-03])\n",
      "counts: tensor([  1,   2,   2,   8,  63, 719])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.520598\n",
      "         Iterations: 5\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.28930817610065424\n",
      "theta 0.06398796320740435\n",
      "p 0.2719092565939668\n",
      "logit 0.984957292793966\n",
      "output: tensor([-8.2834, -7.6757, -7.3515, -7.0081, -6.6387, -6.2316, -5.7652, -5.1914,\n",
      "        -4.3605, -0.0286])\n",
      "counts: tensor([  2,   3,   2,   4,   6,   7,  10,  12,  36, 713])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.544110\n",
      "         Iterations: 3\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 9\n",
      "mu 0.38238993710700897\n",
      "theta 0.04974155357636486\n",
      "p 0.36427055383703216\n",
      "logit 0.5568762024042198\n",
      "output: tensor([-10.6830,  -8.3131,  -7.9224,  -7.7176,  -7.5049,  -7.2828,  -7.0489,\n",
      "         -6.5320,  -6.2373,  -5.9042,  -5.5107,  -5.0075,  -4.2406,  -0.0378])\n",
      "counts: tensor([  1,   1,   2,   2,   3,   4,   1,   2,   3,   6,  11,  13,  32, 714])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.775695\n",
      "         Iterations: 4\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 9\n",
      "mu 0.7308176100628745\n",
      "theta 0.06365654021073215\n",
      "p 0.6870804460226274\n",
      "logit -0.7865052414662392\n",
      "output: tensor([-10.3704,  -8.7163,  -7.9897,  -7.8627,  -7.6021,  -6.8992,  -6.7464,\n",
      "         -6.5882,  -6.4234,  -6.2509,  -6.0691,  -5.8757,  -5.6677,  -5.4406,\n",
      "         -5.1874,  -4.8966,  -4.5464,  -4.0887,  -3.3738,  -0.1076])\n",
      "counts: tensor([  1,   1,   1,   1,   1,   2,   3,   3,   2,   2,   2,   4,   7,   6,\n",
      "          7,  10,   8,  12,  45, 677])\n",
      "(795,)\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.316258\n",
      "         Iterations: 4\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "mu 0.17735849056646122\n",
      "theta 0.027000926609737574\n",
      "p 0.1726955506768085\n",
      "logit 1.5666425427938004\n",
      "output: tensor([-9.3009e+00, -9.0666e+00, -8.8225e+00, -8.5664e+00, -8.2950e+00,\n",
      "        -8.0036e+00, -7.6850e+00, -7.3269e+00, -6.9064e+00, -6.3727e+00,\n",
      "        -5.5645e+00, -9.2270e-03])\n",
      "counts: tensor([  1,   1,   3,   1,   1,   1,   1,   3,   9,   7,  14, 753])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.560389\n",
      "         Iterations: 5\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.3132075471698196\n",
      "theta 0.07104462515843359\n",
      "p 0.29243183693068675\n",
      "logit 0.8836023650571488\n",
      "output: tensor([-7.8416, -7.5283, -7.2004, -6.8536, -6.4809, -6.0710, -5.6023, -5.0273,\n",
      "        -4.1983, -0.0336])\n",
      "counts: tensor([  1,   1,   2,   7,   4,  13,  16,  11,  34, 706])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.017573\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.0025157232704402514\n",
      "theta 20.0\n",
      "p 0.00011979634621144054\n",
      "logit 9.029597568295443\n",
      "output: tensor([-1.5018e+01, -3.0098e-07])\n",
      "counts: tensor([  2, 793])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.327295\n",
      "         Iterations: 7\n",
      "         Function evaluations: 16\n",
      "         Gradient evaluations: 16\n",
      "mu 0.3144654088066522\n",
      "theta 0.0192583588167498\n",
      "p 0.3085237477686354\n",
      "logit 0.8070299895533781\n",
      "output: tensor([-12.5233, -11.9787, -10.5609, -10.3807,  -7.6860,  -7.0475,  -6.7697,\n",
      "         -6.4290,  -5.9736,  -5.2401,  -0.0166])\n",
      "counts: tensor([  1,   1,   1,   1,   2,   2,   1,   5,   5,  24, 752])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.034919\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.006289308176100629\n",
      "theta 0.015954347258223322\n",
      "p 0.006190542117442298\n",
      "logit 5.0785228335940635\n",
      "output: tensor([-1.2432e+01, -1.0491e+01, -3.2736e-05])\n",
      "counts: tensor([  1,   3, 791])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.506703\n",
      "         Iterations: 3\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "mu 0.24654088050306855\n",
      "theta 0.08575782319711077\n",
      "p 0.2270680212803873\n",
      "logit 1.2249414224218522\n",
      "output: tensor([-13.7452, -10.2398,  -8.3053,  -7.8855,  -7.4470,  -6.9832,  -6.4828,\n",
      "         -5.9248,  -5.2628,  -4.3534,  -0.0252])\n",
      "counts: tensor([  1,   1,   1,   1,   3,   3,   3,   4,  17,  54, 707])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.097332\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.13836477987421394\n",
      "theta 0.0029397849731835762\n",
      "p 0.13795920946332138\n",
      "logit 1.8323445324876688\n",
      "output: tensor([-1.3528e+01, -1.0921e+01, -1.0326e+01, -9.2866e+00, -8.9789e+00,\n",
      "        -8.5539e+00, -7.8426e+00, -1.5629e-03])\n",
      "counts: tensor([  1,   1,   1,   1,   2,   1,   2, 786])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.029737\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.006289308176100584\n",
      "theta 0.007913963061527389\n",
      "p 0.006239925635117585\n",
      "logit 5.070527538781605\n",
      "output: tensor([-1.2235e+01, -1.0735e+01, -2.8815e-05])\n",
      "counts: tensor([  2,   1, 792])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.747682\n",
      "         Iterations: 5\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "mu 0.6452830188677721\n",
      "theta 0.0701396099213638\n",
      "p 0.6029895659269998\n",
      "logit -0.4179373303322622\n",
      "output: tensor([-11.0995, -10.0777,  -9.1413,  -8.7224,  -8.2889,  -8.1404,  -7.9896,\n",
      "         -7.5206,  -7.3575,  -6.8410,  -6.6572,  -6.4657,  -6.2649,  -6.0527,\n",
      "         -5.8259,  -5.5802,  -5.3086,  -4.9997,  -4.6319,  -4.1577,  -3.4292,\n",
      "         -0.0950])\n",
      "counts: tensor([  1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   2,   2,   3,\n",
      "          3,   3,   5,   6,   6,  24,  56, 674])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.487078\n",
      "         Iterations: 3\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 12\n",
      "mu 0.25534591194914863\n",
      "theta 0.06954360663293727\n",
      "p 0.23874287160016863\n",
      "logit 1.1595840613092554\n",
      "output: tensor([-12.6279, -11.4869, -10.3040,  -9.3743,  -9.0527,  -7.2869,  -6.8776,\n",
      "         -6.4308,  -5.9252,  -5.3130,  -4.4462,  -0.0246])\n",
      "counts: tensor([  1,   1,   1,   1,   1,   1,   2,   2,   8,  11,  53, 713])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.009658\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.0012578616352201257\n",
      "theta 20.0\n",
      "p 5.989817310572027e-05\n",
      "logit 9.722804652410682\n",
      "output: tensor([-1.6403e+01, -7.5294e-08])\n",
      "counts: tensor([  1, 794])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.042223\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.016352201257861642\n",
      "theta 0.004264354649545183\n",
      "p 0.01628276576994313\n",
      "logit 4.101231258434309\n",
      "output: tensor([-1.2754e+01, -1.2341e+01, -9.8136e+00, -1.0906e-04])\n",
      "counts: tensor([  1,   1,   2, 791])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.202400\n",
      "         Iterations: 3\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 12\n",
      "mu 0.060377358490659644\n",
      "theta 0.05096202494818898\n",
      "p 0.05744961003099628\n",
      "logit 2.797681165751591\n",
      "output: tensor([-1.2718e+01, -8.7330e+00, -7.7407e+00, -6.4853e+00, -2.2456e-03])\n",
      "counts: tensor([  1,   3,   4,  23, 764])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.493150\n",
      "         Iterations: 4\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.21886792452824036\n",
      "theta 0.10287952042745513\n",
      "p 0.19845134529600417\n",
      "logit 1.3960017165426046\n",
      "output: tensor([-12.6127,  -9.8810,  -8.9114,  -8.4071,  -7.3374,  -6.7544,  -6.1151,\n",
      "         -5.3745,  -4.3940,  -0.0222])\n",
      "counts: tensor([  1,   1,   1,   1,   4,   4,   4,  12,  61, 706])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.283806\n",
      "         Iterations: 4\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 9\n",
      "mu 0.08050314465408737\n",
      "theta 0.3018892777313197\n",
      "p 0.0618356307491622\n",
      "logit 2.7194454197353584\n",
      "output: tensor([-1.1171e+01, -9.4210e+00, -7.5980e+00, -5.6105e+00, -4.2681e-03])\n",
      "counts: tensor([  1,   1,   4,  49, 740])\n",
      "(795,)\n",
      "(795,)\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.031659\n",
      "         Iterations: 1\n",
      "         Function evaluations: 124\n",
      "         Gradient evaluations: 112\n",
      "mu 0.005031446540880552\n",
      "theta 26.997358511388374\n",
      "p 0.0001797114730960755\n",
      "logit 8.623978192757319\n",
      "output: tensor([-1.3921e+01, -9.0185e-07])\n",
      "counts: tensor([  4, 791])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.009658\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.0012578616352201257\n",
      "theta 20.0\n",
      "p 5.989817310572027e-05\n",
      "logit 9.722804652410682\n",
      "output: tensor([-1.6403e+01, -7.5294e-08])\n",
      "counts: tensor([  1, 794])\n",
      "(795,)\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.336383\n",
      "         Iterations: 1\n",
      "         Function evaluations: 124\n",
      "         Gradient evaluations: 112\n",
      "mu 0.24402515723270635\n",
      "theta 0.006028197681645216\n",
      "p 0.2425629398808635\n",
      "logit 1.1386792223780156\n",
      "output: tensor([-1.3037e+01, -9.7780e+00, -9.6933e+00, -9.3098e+00, -8.9508e+00,\n",
      "        -8.0231e+00, -7.7131e+00, -7.2862e+00, -6.5747e+00, -5.4009e-03])\n",
      "counts: tensor([  1,   1,   1,   1,   1,   1,   2,  10,  25, 752])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.348764\n",
      "         Iterations: 4\n",
      "         Function evaluations: 13\n",
      "         Gradient evaluations: 13\n",
      "mu 0.19119496855393464\n",
      "theta 0.03656648917698182\n",
      "p 0.18445026976102666\n",
      "logit 1.4864825149307337\n",
      "output: tensor([-11.4330, -10.7516, -10.5183,  -8.4515,  -7.8251,  -7.4751,  -6.6355,\n",
      "         -6.0731,  -5.2409,  -0.0120])\n",
      "counts: tensor([  1,   1,   2,   2,   1,   1,   2,   8,  34, 743])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.056675\n",
      "         Iterations: 2\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "mu 0.011320754717071892\n",
      "theta 0.023725956799973705\n",
      "p 0.011058383976566357\n",
      "logit 4.493446425352875\n",
      "output: tensor([-1.1185e+01, -9.3851e+00, -1.0189e-04])\n",
      "counts: tensor([  2,   5, 788])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.041648\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.0075471698113207565\n",
      "theta 0.02313688351424205\n",
      "p 0.00737650057673412\n",
      "logit 4.902052087927705\n",
      "output: tensor([-1.2158e+01, -1.0085e+01, -4.8027e-05])\n",
      "counts: tensor([  1,   4, 790])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.438052\n",
      "         Iterations: 3\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "mu 0.18867924528301444\n",
      "theta 0.08541267690044227\n",
      "p 0.17383180544916443\n",
      "logit 1.5587101817846933\n",
      "output: tensor([-12.7789, -10.5702,  -8.1518,  -7.6130,  -7.0376,  -6.4046,  -5.6675,\n",
      "         -4.6829,  -0.0166])\n",
      "counts: tensor([  1,   1,   3,   1,   4,   3,  12,  51, 719])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.495714\n",
      "         Iterations: 4\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 9\n",
      "mu 0.23522012578615076\n",
      "theta 0.08293644575873228\n",
      "p 0.21720584500353543\n",
      "logit 1.2820242703082605\n",
      "output: tensor([-8.4137, -7.9899, -7.5474, -7.0795, -6.5749, -6.0125, -5.3456, -4.4301,\n",
      "        -0.0232])\n",
      "counts: tensor([  3,   2,   5,   1,   4,   6,  17,  46, 711])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.347610\n",
      "         Iterations: 3\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.15597484276718135\n",
      "theta 0.047081888880060124\n",
      "p 0.14896145604620212\n",
      "logit 1.7427698318631117\n",
      "output: tensor([-1.1542e+01, -9.0981e+00, -8.7075e+00, -8.2973e+00, -7.8606e+00,\n",
      "        -7.3854e+00, -6.8495e+00, -6.2035e+00, -5.2925e+00, -9.9554e-03])\n",
      "counts: tensor([  1,   1,   1,   1,   2,   4,   6,   7,  30, 742])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.570493\n",
      "         Iterations: 5\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.40125786163516874\n",
      "theta 0.051946604780208794\n",
      "p 0.3814431833438985\n",
      "logit 0.48342713963072415\n",
      "output: tensor([-9.2822, -8.2141, -7.8248, -7.6208, -7.4089, -7.1876, -6.9545, -6.7066,\n",
      "        -6.4394, -6.1456, -5.8137, -5.4214, -4.9198, -4.1556, -0.0414])\n",
      "counts: tensor([  1,   2,   1,   1,   1,   1,   3,   5,   5,   7,   2,  12,  18,  25,\n",
      "        711])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.107944\n",
      "         Iterations: 3\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "mu 0.025157232704437153\n",
      "theta 0.036515745526698846\n",
      "p 0.024270960487583974\n",
      "logit 3.6939043303665713\n",
      "output: tensor([-1.0782e+01, -9.4983e+00, -7.9443e+00, -4.6019e-04])\n",
      "counts: tensor([  1,   3,  11, 780])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.274690\n",
      "         Iterations: 3\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "mu 0.0842767295594339\n",
      "theta 0.10342538117128094\n",
      "p 0.07637737086487402\n",
      "logit 2.4926171187621122\n",
      "output: tensor([-1.4033e+01, -1.1270e+01, -8.2546e+00, -7.0988e+00, -5.7034e+00,\n",
      "        -4.5763e-03])\n",
      "counts: tensor([  1,   1,   1,   4,  41, 747])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.009658\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.0012578616352201257\n",
      "theta 20.0\n",
      "p 5.989817310572027e-05\n",
      "logit 9.722804652410682\n",
      "output: tensor([-1.6403e+01, -7.5294e-08])\n",
      "counts: tensor([  1, 794])\n",
      "(795,)\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: nan\n",
      "         Iterations: 1\n",
      "         Function evaluations: 112\n",
      "         Gradient evaluations: 112\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.414844\n",
      "         Iterations: 4\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 12\n",
      "mu 0.343396226414437\n",
      "theta 0.028102017701123347\n",
      "p 0.33400987499497814\n",
      "logit 0.6901042847178093\n",
      "output: tensor([-13.5193,  -9.5826,  -9.0824,  -8.0856,  -7.9225,  -7.5704,  -7.3775,\n",
      "         -7.1693,  -6.9412,  -6.6858,  -6.3910,  -6.0340,  -5.5638,  -4.8197,\n",
      "         -0.0237])\n",
      "counts: tensor([  1,   1,   1,   2,   1,   2,   2,   1,   2,   1,   6,   3,  11,  22,\n",
      "        739])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.024830\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.0037735849056603783\n",
      "theta 20.0\n",
      "p 0.00017969451931716085\n",
      "logit 8.624072553043334\n",
      "output: tensor([-1.4208e+01, -6.7675e-07])\n",
      "counts: tensor([  3, 792])\n",
      "(795,)\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.038170\n",
      "         Iterations: 1\n",
      "         Function evaluations: 124\n",
      "         Gradient evaluations: 112\n",
      "mu 0.006289308176100808\n",
      "theta 33.61131987639512\n",
      "p 0.0001817124628174064\n",
      "logit 8.612903265860526\n",
      "output: tensor([-1.3688e+01, -1.1392e-06])\n",
      "counts: tensor([  5, 790])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.677615\n",
      "         Iterations: 2\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "mu 0.6477987421388878\n",
      "theta 0.05451464242346712\n",
      "p 0.6143098597949556\n",
      "logit -0.46546515698753016\n",
      "output: tensor([-14.6674, -13.1800, -10.6405,  -7.1350,  -6.8280,  -6.6651,  -6.4945,\n",
      "         -6.3143,  -6.1225,  -5.9160,  -5.6901,  -5.4378,  -5.1474,  -4.7969,\n",
      "         -4.3375,  -3.6167,  -0.0833])\n",
      "counts: tensor([  1,   1,   1,   1,   3,   3,   2,   2,   1,   1,   4,   5,   6,   6,\n",
      "         20,  48, 690])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.141634\n",
      "         Iterations: 1\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.04402515723284357\n",
      "theta 0.021099075617702846\n",
      "p 0.04311546086378644\n",
      "logit 3.0998010818915143\n",
      "output: tensor([-1.1119e+01, -1.0550e+01, -9.9402e+00, -8.4815e+00, -7.4177e+00,\n",
      "        -1.0137e-03])\n",
      "counts: tensor([  1,   1,   2,   1,  14, 776])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.027824\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.005031446540880504\n",
      "theta 0.010152397848689\n",
      "p 0.004980878678896296\n",
      "logit 5.297155637364061\n",
      "output: tensor([-1.2788e+01, -1.1001e+01, -2.0313e-05])\n",
      "counts: tensor([  1,   2, 792])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.263434\n",
      "         Iterations: 4\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "mu 0.1358490566039966\n",
      "theta 0.023642772176182335\n",
      "p 0.1327113914116664\n",
      "logit 1.877195022268574\n",
      "output: tensor([-1.0387e+01, -1.0149e+01, -9.1116e+00, -8.8209e+00, -8.5103e+00,\n",
      "        -7.7945e+00, -7.3542e+00, -6.8001e+00, -5.9699e+00, -5.8737e-03])\n",
      "counts: tensor([  1,   1,   1,   1,   3,   5,   1,   5,  17, 760])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.038280\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.06289308176100658\n",
      "theta 0.0015791488077665315\n",
      "p 0.0627939208158153\n",
      "logit 2.7030449266588525\n",
      "output: tensor([-1.4140e+01, -1.0707e+01, -9.2494e+00, -3.6681e-04])\n",
      "counts: tensor([  1,   1,   1, 792])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.713721\n",
      "         Iterations: 4\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "mu 1.329559748425537\n",
      "theta 0.03413182558923479\n",
      "p 1.2856772372013319\n",
      "logit nan\n",
      "output: tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan])\n",
      "counts: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.655332\n",
      "         Iterations: 5\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 9\n",
      "mu 0.27421383647799047\n",
      "theta 0.43203373428674846\n",
      "p 0.19148559835747714\n",
      "logit 1.440385890257641\n",
      "output: tensor([-15.3577,  -8.2525,  -7.1858,  -6.0866,  -4.9307,  -3.6506,  -0.0373])\n",
      "counts: tensor([  1,   1,   3,   7,  26, 116, 641])\n",
      "(795,)\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.355106\n",
      "         Iterations: 3\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 12\n",
      "mu 0.2088050314469857\n",
      "theta 0.034277253776794796\n",
      "p 0.20188496912651574\n",
      "logit 1.374554660597927\n",
      "output: tensor([-10.9555,  -9.6994,  -9.4760,  -9.2468,  -8.7670,  -7.6668,  -7.3393,\n",
      "         -6.9727,  -6.0039,  -5.1925,  -0.0132])\n",
      "counts: tensor([  1,   2,   1,   2,   1,   2,   1,   3,   3,  36, 743])\n",
      "(795,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: invalid value encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3439: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(linpred)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in multiply\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: divide by zero encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3162: RuntimeWarning: invalid value encountered in multiply\n",
      "  dparams = (a4 * dgterm -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: invalid value encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3439: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(linpred)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in multiply\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: divide by zero encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3162: RuntimeWarning: invalid value encountered in multiply\n",
      "  dparams = (a4 * dgterm -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3123: RuntimeWarning: invalid value encountered in log\n",
      "  (y + a1) * np.log(a2))\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: invalid value encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: divide by zero encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in multiply\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3156: RuntimeWarning: invalid value encountered in divide\n",
      "  a4 = p * a1 / mu\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3164: RuntimeWarning: divide by zero encountered in divide\n",
      "  y / mu)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3164: RuntimeWarning: invalid value encountered in divide\n",
      "  y / mu)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3123: RuntimeWarning: invalid value encountered in log\n",
      "  (y + a1) * np.log(a2))\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3123: RuntimeWarning: invalid value encountered in log\n",
      "  (y + a1) * np.log(a2))\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: invalid value encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3439: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(linpred)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in multiply\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: divide by zero encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3162: RuntimeWarning: invalid value encountered in multiply\n",
      "  dparams = (a4 * dgterm -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/var/folders/12/d54y_pk95zdbftjg_qkcj_c80000gq/T/ipykernel_42688/534514316.py:123: RuntimeWarning: invalid value encountered in log\n",
      "  logit=np.log((1-p)/p)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3123: RuntimeWarning: invalid value encountered in log\n",
      "  (y + a1) * np.log(a2))\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: invalid value encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3439: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(linpred)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in multiply\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: divide by zero encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3162: RuntimeWarning: invalid value encountered in multiply\n",
      "  dparams = (a4 * dgterm -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3123: RuntimeWarning: invalid value encountered in log\n",
      "  (y + a1) * np.log(a2))\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: invalid value encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3439: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(linpred)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in multiply\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: divide by zero encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3162: RuntimeWarning: invalid value encountered in multiply\n",
      "  dparams = (a4 * dgterm -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/var/folders/12/d54y_pk95zdbftjg_qkcj_c80000gq/T/ipykernel_42688/534514316.py:123: RuntimeWarning: invalid value encountered in log\n",
      "  logit=np.log((1-p)/p)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.094690\n",
      "         Iterations: 2\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 12\n",
      "mu 0.04654088050107775\n",
      "theta 0.006687482100578605\n",
      "p 0.04623170678944414\n",
      "logit 3.026754905874758\n",
      "output: tensor([-1.1722e+01, -1.1483e+01, -1.0362e+01, -1.0007e+01, -9.5869e+00,\n",
      "        -9.0505e+00, -8.2297e+00, -6.3711e-04])\n",
      "counts: tensor([  1,   1,   1,   1,   1,   1,   4, 785])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.009658\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.0012578616352201257\n",
      "theta 20.0\n",
      "p 5.989817310572027e-05\n",
      "logit 9.722804652410682\n",
      "output: tensor([-1.6403e+01, -7.5294e-08])\n",
      "counts: tensor([  1, 794])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.575498\n",
      "         Iterations: 3\n",
      "         Function evaluations: 8\n",
      "         Gradient evaluations: 8\n",
      "mu 0.2679245283019247\n",
      "theta 0.1243857070381726\n",
      "p 0.23828524911410023\n",
      "logit 1.162103661231094\n",
      "output: tensor([-10.3583,  -7.9830,  -7.4680,  -6.9289,  -6.3550,  -5.7266,  -5.0001,\n",
      "         -4.0429,  -0.0322])\n",
      "counts: tensor([  1,   1,   4,   3,   5,  12,  19,  61, 689])\n",
      "(795,)\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.557020\n",
      "         Iterations: 5\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.3547169811320106\n",
      "theta 0.05783783149899493\n",
      "p 0.335322646411089\n",
      "logit 0.6842085484093449\n",
      "output: tensor([-10.4552,  -8.4081,  -8.1818,  -7.4584,  -6.9204,  -6.6248,  -6.3029,\n",
      "         -5.9431,  -5.5234,  -4.9954,  -4.2075,  -0.0367])\n",
      "counts: tensor([  1,   1,   1,   3,   3,   6,   3,   9,  14,  13,  31, 710])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.540306\n",
      "         Iterations: 4\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 9\n",
      "mu 0.2301886792452281\n",
      "theta 0.14839989076975182\n",
      "p 0.20044296511639054\n",
      "logit 1.3835281249010636\n",
      "output: tensor([-11.1062,  -8.7345,  -8.1072,  -7.4566,  -6.7723,  -6.0354,  -5.2039,\n",
      "         -4.1516,  -0.0264])\n",
      "counts: tensor([  1,   1,   2,   4,   3,   5,  18,  70, 691])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.060836\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.02767295597484261\n",
      "theta 0.004699231961664179\n",
      "p 0.027543522573229673\n",
      "logit 3.564057928556757\n",
      "output: tensor([-1.3071e+01, -1.0966e+01, -1.0524e+01, -9.9636e+00, -9.1183e+00,\n",
      "        -2.4869e-04])\n",
      "counts: tensor([  1,   1,   1,   1,   2, 789])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.034919\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.006289308176100629\n",
      "theta 0.015954347258223322\n",
      "p 0.006190542117442298\n",
      "logit 5.0785228335940635\n",
      "output: tensor([-1.2432e+01, -1.0491e+01, -3.2736e-05])\n",
      "counts: tensor([  1,   3, 791])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.711981\n",
      "         Iterations: 3\n",
      "         Function evaluations: 115\n",
      "         Gradient evaluations: 115\n",
      "mu 0.39496855345918325\n",
      "theta 0.13767716793484833\n",
      "p 0.34717102934934\n",
      "logit 0.6314976432349193\n",
      "output: tensor([-17.6645,  -9.6289,  -8.5250,  -7.3553,  -6.5117,  -6.0575,  -5.5691,\n",
      "         -5.0273,  -4.3893,  -3.5261,  -0.0608])\n",
      "counts: tensor([  1,   2,   1,   3,   3,   1,   5,  13,  22,  87, 657])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.142872\n",
      "         Iterations: 3\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.033962264151028154\n",
      "theta 0.07624499743746713\n",
      "p 0.03155625738739051\n",
      "logit 3.4239184923005226\n",
      "output: tensor([-1.0577e+01, -9.0318e+00, -7.2350e+00, -8.7445e-04])\n",
      "counts: tensor([  1,   3,  18, 773])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.081093\n",
      "         Iterations: 1\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "mu 0.021383647798648656\n",
      "theta 0.010612932749558414\n",
      "p 0.021159087822545976\n",
      "logit 3.8342996315108713\n",
      "output: tensor([-1.2599e+01, -1.0705e+01, -8.8161e+00, -2.4640e-04])\n",
      "counts: tensor([  1,   1,   8, 785])\n",
      "(795,)\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.031659\n",
      "         Iterations: 1\n",
      "         Function evaluations: 124\n",
      "         Gradient evaluations: 112\n",
      "mu 0.005031446540880557\n",
      "theta 26.997358511388374\n",
      "p 0.00017971147309607568\n",
      "logit 8.623978192757319\n",
      "output: tensor([-1.3921e+01, -9.0185e-07])\n",
      "counts: tensor([  4, 791])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.113881\n",
      "         Iterations: 2\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "mu 0.036477987421118005\n",
      "theta 0.014648915820432512\n",
      "p 0.03595133927839696\n",
      "logit 3.2889754338711588\n",
      "output: tensor([-1.1841e+01, -1.0277e+01, -9.6569e+00, -8.9211e+00, -7.9049e+00,\n",
      "        -6.5250e-04])\n",
      "counts: tensor([  1,   1,   1,   4,   7, 781])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.899108\n",
      "         Iterations: 3\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "mu 1.135849056603594\n",
      "theta 0.0642772907142482\n",
      "p 1.0672491713520573\n",
      "logit nan\n",
      "output: tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan])\n",
      "counts: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.677750\n",
      "         Iterations: 5\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.5610062893081347\n",
      "theta 0.05892430368085701\n",
      "p 0.5297888502115379\n",
      "logit -0.11929668264976077\n",
      "output: tensor([-8.9632, -8.0936, -7.9400, -7.2886, -7.1135, -6.7427, -6.5440, -6.3337,\n",
      "        -6.1086, -5.8644, -5.5939, -5.2855, -4.9174, -4.4411, -3.7053, -0.0711])\n",
      "counts: tensor([  1,   2,   3,   3,   2,   4,   2,   4,   2,   3,   5,   5,  10,  18,\n",
      "         39, 692])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 1.883337\n",
      "         Iterations: 5\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 2.578616352201276\n",
      "theta 0.2787259919896823\n",
      "p 2.0165511363298245\n",
      "logit nan\n",
      "output: tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan])\n",
      "counts: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.020233\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.0037735849056603783\n",
      "theta 0.005685439117826078\n",
      "p 0.003752251707025327\n",
      "logit 5.581639854931183\n",
      "output: tensor([-1.3284e+01, -1.1677e+01, -1.0844e-05])\n",
      "counts: tensor([  1,   1, 793])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.474628\n",
      "         Iterations: 5\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "mu 0.349685534591246\n",
      "theta 0.03702952505490207\n",
      "p 0.3371992080676132\n",
      "logit 0.6758006028092722\n",
      "output: tensor([-9.3485, -9.0458, -8.5685, -8.2296, -8.0520, -7.6754, -7.4735, -7.2597,\n",
      "        -7.0308, -6.7821, -6.5065, -6.1919, -5.8159, -5.3281, -4.5706, -0.0285])\n",
      "counts: tensor([  1,   1,   1,   1,   1,   2,   2,   2,   3,   3,   5,   7,   6,   5,\n",
      "         26, 729])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.677008\n",
      "         Iterations: 4\n",
      "         Function evaluations: 8\n",
      "         Gradient evaluations: 8\n",
      "mu 0.29056603773584366\n",
      "theta 0.3976431385088636\n",
      "p 0.20789715895993785\n",
      "logit 1.3376477038284986\n",
      "output: tensor([-11.6888,  -9.8167,  -8.8645,  -7.8964,  -6.9058,  -5.8803,  -4.7939,\n",
      "         -3.5733,  -0.0416])\n",
      "counts: tensor([  1,   1,   1,   1,   2,  11,  22, 119, 637])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.621070\n",
      "         Iterations: 3\n",
      "         Function evaluations: 8\n",
      "         Gradient evaluations: 8\n",
      "mu 0.284276729559759\n",
      "theta 0.16643528699632296\n",
      "p 0.2437141028987536\n",
      "logit 1.1324236465861526\n",
      "output: tensor([-9.8309, -8.1540, -7.5664, -6.9559, -6.3126, -5.6181, -4.8317, -3.8316,\n",
      "        -0.0379])\n",
      "counts: tensor([  1,   3,   3,   2,   3,   9,  27,  74, 673])\n",
      "(795,)\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.044428\n",
      "         Iterations: 1\n",
      "         Function evaluations: 124\n",
      "         Gradient evaluations: 112\n",
      "mu 0.007547169811320677\n",
      "theta 47.97559473333018\n",
      "p 0.0001541006260855976\n",
      "logit 8.777750640292973\n",
      "output: tensor([-1.3672e+01, -1.1586e-06])\n",
      "counts: tensor([  6, 789])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.522881\n",
      "         Iterations: 4\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 12\n",
      "mu 0.4113207547175752\n",
      "theta 0.042899606464252775\n",
      "p 0.3944011026258585\n",
      "logit 0.4288494629760889\n",
      "output: tensor([-13.6821,  -9.4409,  -8.3995,  -8.0735,  -7.9035,  -7.7278,  -7.5455,\n",
      "         -7.1554,  -6.7171,  -6.4709,  -6.1979,  -5.8862,  -5.5135,  -5.0301,\n",
      "         -4.2797,  -0.0387])\n",
      "counts: tensor([  1,   1,   1,   1,   2,   1,   2,   2,   3,   1,   2,   6,   5,  14,\n",
      "         35, 718])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 1.173266\n",
      "         Iterations: 5\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 12\n",
      "mu 2.932075471697383\n",
      "theta 0.061614151624503066\n",
      "p 2.7619031521110218\n",
      "logit nan\n",
      "output: tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan])\n",
      "counts: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.390720\n",
      "         Iterations: 4\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "mu 0.21509433962289404\n",
      "theta 0.041100389301163595\n",
      "p 0.20660288079161668\n",
      "logit 1.3455253767282198\n",
      "output: tensor([-11.8508, -10.5121,  -9.5558,  -8.2169,  -7.5922,  -7.2432,  -6.8554,\n",
      "         -6.4065,  -5.8465,  -5.0188,  -0.0151])\n",
      "counts: tensor([  1,   1,   1,   1,   1,   3,   6,   8,   5,  31, 737])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.069507\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.03144654088050314\n",
      "theta 0.005640061770752087\n",
      "p 0.031270175161012784\n",
      "logit 3.4333209814565677\n",
      "output: tensor([-1.2850e+01, -1.1076e+01, -1.0239e+01, -9.6710e+00, -8.8185e+00,\n",
      "        -3.3046e-04])\n",
      "counts: tensor([  1,   1,   1,   2,   2, 788])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 1.220415\n",
      "         Iterations: 2\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 9\n",
      "mu 1.4691823899371215\n",
      "theta 0.1185578703734309\n",
      "p 1.3134612243590351\n",
      "logit nan\n",
      "output: tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan])\n",
      "counts: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.551026\n",
      "         Iterations: 3\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 12\n",
      "mu 1.025157232712623\n",
      "theta 0.023812475894732545\n",
      "p 1.0013134796161918\n",
      "logit nan\n",
      "output: tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan])\n",
      "counts: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.049775\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.01886792452830174\n",
      "theta 0.0037331502804563766\n",
      "p 0.01879774970372334\n",
      "logit 3.955041439732903\n",
      "output: tensor([-1.3383e+01, -1.0622e+01, -9.7518e+00, -1.2595e-04])\n",
      "counts: tensor([  1,   2,   2, 790])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.771422\n",
      "         Iterations: 4\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 9\n",
      "mu 0.7169811320755713\n",
      "theta 0.0649255762301082\n",
      "p 0.6732687692727991\n",
      "logit -0.7230067013604926\n",
      "output: tensor([-10.0648,  -8.5592,  -7.5223,  -7.2390,  -6.9411,  -6.7853,  -6.6239,\n",
      "         -6.4561,  -6.2806,  -6.0957,  -5.8994,  -5.6884,  -5.4583,  -5.2022,\n",
      "         -4.9085,  -4.5555,  -4.0953,  -3.3784,  -0.1059])\n",
      "counts: tensor([  1,   1,   1,   3,   3,   1,   1,   1,   7,   5,   3,   7,   5,   7,\n",
      "          4,   9,   5,  55, 676])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.120299\n",
      "         Iterations: 3\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 12\n",
      "mu 0.051572327044496796\n",
      "theta 0.00987253527410145\n",
      "p 0.05106815488402103\n",
      "logit 2.922175867599058\n",
      "output: tensor([-1.1358e+01, -1.1066e+01, -1.0075e+01, -9.6795e+00, -8.6443e+00,\n",
      "        -7.7858e+00, -9.1397e-04])\n",
      "counts: tensor([  1,   1,   2,   1,   1,   8, 781])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.234096\n",
      "         Iterations: 2\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.09685534591192682\n",
      "theta 0.02715865603014294\n",
      "p 0.09429443576541745\n",
      "logit 2.2622920867060485\n",
      "output: tensor([-1.1046e+01, -9.9956e+00, -8.7979e+00, -8.3343e+00, -7.8085e+00,\n",
      "        -7.1693e+00, -6.2558e+00, -3.8174e-03])\n",
      "counts: tensor([  1,   1,   2,   2,   2,  10,  14, 763])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.558276\n",
      "         Iterations: 3\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 9\n",
      "mu 0.40880503144651326\n",
      "theta 0.05036111430527325\n",
      "p 0.3892042706825679\n",
      "logit 0.45065825814239874\n",
      "output: tensor([-10.8797,  -8.8537,  -8.3323,  -7.9644,  -7.7723,  -7.1513,  -6.9236,\n",
      "         -6.6811,  -6.4191,  -6.1306,  -5.8038,  -5.4166,  -4.9199,  -4.1597,\n",
      "         -0.0419])\n",
      "counts: tensor([  1,   1,   2,   2,   2,   1,   3,   2,   4,   5,   4,   9,   9,  39,\n",
      "        711])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.556113\n",
      "         Iterations: 5\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.31194968553459423\n",
      "theta 0.07191953486575509\n",
      "p 0.2910196851423761\n",
      "logit 0.8904368500679446\n",
      "output: tensor([-8.1646, -7.8597, -7.5434, -7.2127, -6.8630, -6.4875, -6.0747, -5.6032,\n",
      "        -5.0256, -4.1945, -0.0336])\n",
      "counts: tensor([  1,   2,   2,   4,   3,   5,  12,   5,  14,  42, 705])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.676583\n",
      "         Iterations: 4\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.6943396226411291\n",
      "theta 0.04651817769752216\n",
      "p 0.6634759313677359\n",
      "logit -0.6788229049355157\n",
      "output: tensor([-9.7819, -8.9622, -8.1693, -8.0639, -7.8476, -7.7363, -7.6226, -7.3870,\n",
      "        -7.1381, -6.8722, -6.7312, -6.5835, -6.4280, -6.2630, -6.0861, -5.8944,\n",
      "        -5.6831, -5.4452, -5.1688, -4.8316, -4.3843, -3.6718, -0.0836])\n",
      "counts: tensor([  1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   3,   5,   4,\n",
      "          5,   2,   4,   2,   7,   7,  13,  33, 699])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.485281\n",
      "         Iterations: 4\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.2465408805031024\n",
      "theta 0.07104957485103831\n",
      "p 0.2301862456155602\n",
      "logit 1.2072598633803688\n",
      "output: tensor([-10.1819,  -9.1988,  -8.8573,  -8.5066,  -8.1444,  -7.7677,  -7.3721,\n",
      "         -6.9507,  -6.4919,  -5.9744,  -5.3506,  -4.4729,  -0.0235])\n",
      "counts: tensor([  1,   1,   2,   2,   1,   1,   1,   1,   6,   4,  10,  51, 714])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.009658\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.0012578616352201257\n",
      "theta 20.0\n",
      "p 5.989817310572027e-05\n",
      "logit 9.722804652410682\n",
      "output: tensor([-1.6403e+01, -7.5294e-08])\n",
      "counts: tensor([  1, 794])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.503392\n",
      "         Iterations: 4\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 9\n",
      "mu 0.21886792452831078\n",
      "theta 0.11132238472824528\n",
      "p 0.19694368397144377\n",
      "logit 1.4055070238235325\n",
      "output: tensor([-11.0335,  -7.9545,  -7.3830,  -6.7761,  -6.1137,  -5.3512,  -4.3524,\n",
      "         -0.0227])\n",
      "counts: tensor([  1,   4,   2,   3,  11,  13,  57, 704])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.009658\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.0012578616352201257\n",
      "theta 20.0\n",
      "p 5.989817310572027e-05\n",
      "logit 9.722804652410682\n",
      "output: tensor([-1.6403e+01, -7.5294e-08])\n",
      "counts: tensor([  1, 794])\n",
      "(795,)\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.017573\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.0025157232704402514\n",
      "theta 20.0\n",
      "p 0.00011979634621144054\n",
      "logit 9.029597568295443\n",
      "output: tensor([-1.5018e+01, -3.0098e-07])\n",
      "counts: tensor([  2, 793])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.606462\n",
      "         Iterations: 5\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.30314465408806723\n",
      "theta 0.11981226951469917\n",
      "p 0.2707102452265888\n",
      "logit 0.9910220787983686\n",
      "output: tensor([-9.7100, -9.3008, -8.8843, -8.0231, -7.5735, -7.1061, -6.6144, -6.0877,\n",
      "        -5.5061, -4.8258, -3.9127, -0.0387])\n",
      "counts: tensor([  1,   1,   1,   2,   3,   3,   2,   6,   7,  11,  76, 682])\n",
      "(795,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/12/d54y_pk95zdbftjg_qkcj_c80000gq/T/ipykernel_42688/534514316.py:123: RuntimeWarning: invalid value encountered in log\n",
      "  logit=np.log((1-p)/p)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3123: RuntimeWarning: invalid value encountered in log\n",
      "  (y + a1) * np.log(a2))\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: invalid value encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: divide by zero encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in multiply\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3156: RuntimeWarning: invalid value encountered in divide\n",
      "  a4 = p * a1 / mu\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3164: RuntimeWarning: divide by zero encountered in divide\n",
      "  y / mu)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3164: RuntimeWarning: invalid value encountered in divide\n",
      "  y / mu)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/var/folders/12/d54y_pk95zdbftjg_qkcj_c80000gq/T/ipykernel_42688/534514316.py:123: RuntimeWarning: invalid value encountered in log\n",
      "  logit=np.log((1-p)/p)\n",
      "/var/folders/12/d54y_pk95zdbftjg_qkcj_c80000gq/T/ipykernel_42688/534514316.py:123: RuntimeWarning: invalid value encountered in log\n",
      "  logit=np.log((1-p)/p)\n",
      "/var/folders/12/d54y_pk95zdbftjg_qkcj_c80000gq/T/ipykernel_42688/534514316.py:123: RuntimeWarning: invalid value encountered in log\n",
      "  logit=np.log((1-p)/p)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3123: RuntimeWarning: invalid value encountered in log\n",
      "  (y + a1) * np.log(a2))\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: invalid value encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3439: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(linpred)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in multiply\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: divide by zero encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3162: RuntimeWarning: invalid value encountered in multiply\n",
      "  dparams = (a4 * dgterm -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3123: RuntimeWarning: invalid value encountered in log\n",
      "  (y + a1) * np.log(a2))\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3123: RuntimeWarning: invalid value encountered in log\n",
      "  (y + a1) * np.log(a2))\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: invalid value encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: divide by zero encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in multiply\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3156: RuntimeWarning: invalid value encountered in divide\n",
      "  a4 = p * a1 / mu\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3164: RuntimeWarning: divide by zero encountered in divide\n",
      "  y / mu)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3164: RuntimeWarning: invalid value encountered in divide\n",
      "  y / mu)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: invalid value encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3123: RuntimeWarning: invalid value encountered in log\n",
      "  (y + a1) * np.log(a2))\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: divide by zero encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in multiply\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3156: RuntimeWarning: invalid value encountered in divide\n",
      "  a4 = p * a1 / mu\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3164: RuntimeWarning: divide by zero encountered in divide\n",
      "  y / mu)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3164: RuntimeWarning: invalid value encountered in divide\n",
      "  y / mu)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: nan\n",
      "         Iterations: 1\n",
      "         Function evaluations: 112\n",
      "         Gradient evaluations: 112\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.120161\n",
      "         Iterations: 4\n",
      "         Function evaluations: 13\n",
      "         Gradient evaluations: 13\n",
      "mu 0.03018867924516885\n",
      "theta 0.03384985789612229\n",
      "p 0.02920025477065172\n",
      "logit 3.503942777160203\n",
      "output: tensor([-1.3168e+01, -1.0246e+01, -7.6930e+00, -6.2361e-04])\n",
      "counts: tensor([  1,   1,  15, 778])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.009658\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.0012578616352201257\n",
      "theta 20.0\n",
      "p 5.989817310572027e-05\n",
      "logit 9.722804652410682\n",
      "output: tensor([-1.6403e+01, -7.5294e-08])\n",
      "counts: tensor([  1, 794])\n",
      "(795,)\n",
      " data for (contrl, control): None\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.083668\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.04654088050314485\n",
      "theta 0.002441999545314968\n",
      "p 0.046427504558123806\n",
      "logit 3.0223233987656557\n",
      "output: tensor([-1.3759e+01, -9.8851e+00, -9.1433e+00, -3.3879e-04])\n",
      "counts: tensor([  1,   2,   6, 786])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.225853\n",
      "         Iterations: 2\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.0930817610065355\n",
      "theta 0.026401438876016134\n",
      "p 0.09068748101957726\n",
      "logit 2.305269518999406\n",
      "output: tensor([-1.0448e+01, -1.0084e+01, -9.3047e+00, -8.8779e+00, -8.4117e+00,\n",
      "        -7.8831e+00, -7.2410e+00, -6.3242e+00, -3.5500e-03])\n",
      "counts: tensor([  1,   1,   1,   2,   3,   1,   4,  18, 764])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.610131\n",
      "         Iterations: 4\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "mu 0.92704402515587\n",
      "theta 0.029662444306515268\n",
      "p 0.9003378051534555\n",
      "logit -2.2009836160060563\n",
      "output: tensor([-12.1346,  -9.8075,  -8.8737,  -8.2025,  -7.8252,  -7.7585,  -7.5498,\n",
      "         -7.4770,  -7.3258,  -7.2470,  -6.9949,  -6.9047,  -6.8106,  -6.7122,\n",
      "         -6.6089,  -6.4998,  -6.3840,  -6.1266,  -5.9810,  -5.8202,  -5.6395,\n",
      "         -5.4316,  -5.1844,  -4.8750,  -4.4528,  -3.7574,  -0.0923])\n",
      "counts: tensor([  1,   1,   1,   1,   1,   1,   1,   1,   1,   2,   1,   1,   1,   2,\n",
      "          1,   2,   3,   1,   5,   1,   3,   2,   4,   6,  11,  23, 717])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.712604\n",
      "         Iterations: 4\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "mu 0.8729559748427203\n",
      "theta 0.045169999347308164\n",
      "p 0.8352286952245732\n",
      "logit -1.6231470930411618\n",
      "output: tensor([-11.2322, -10.1254,  -9.5775,  -9.0038,  -8.0680,  -7.8124,  -7.1601,\n",
      "         -7.0581,  -6.8449,  -6.6167,  -6.4956,  -6.3689,  -6.2355,  -6.0943,\n",
      "         -5.9434,  -5.6033,  -5.1824,  -4.9201,  -4.5969,  -4.1633,  -3.4639,\n",
      "         -0.1123])\n",
      "counts: tensor([  1,   1,   1,   1,   1,   2,   1,   1,   1,   1,   3,   6,   1,   2,\n",
      "          1,   4,   7,   3,   9,  17,  38, 693])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.384211\n",
      "         Iterations: 3\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.1937106918237516\n",
      "theta 0.045728114572486704\n",
      "p 0.18524001518592068\n",
      "logit 1.4812412091151657\n",
      "output: tensor([-11.6598, -10.0266,  -9.7384,  -7.8073,  -7.4221,  -6.9984,  -6.5139,\n",
      "         -5.9191,  -5.0588,  -0.0136])\n",
      "counts: tensor([  1,   1,   1,   2,   2,   5,   4,  14,  28, 737])\n",
      "(795,)\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.044428\n",
      "         Iterations: 1\n",
      "         Function evaluations: 124\n",
      "         Gradient evaluations: 112\n",
      "mu 0.00754716981132067\n",
      "theta 47.97559473333018\n",
      "p 0.00015410062608559746\n",
      "logit 8.777750640292973\n",
      "output: tensor([-1.3672e+01, -1.1586e-06])\n",
      "counts: tensor([  6, 789])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.481575\n",
      "         Iterations: 4\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.21383647798737265\n",
      "theta 0.09361298655308137\n",
      "p 0.19553213121705515\n",
      "logit 1.414456306300229\n",
      "output: tensor([-11.1132,  -8.8598,  -8.3764,  -7.3478,  -6.7847,  -6.1646,  -5.4418,\n",
      "         -4.4750,  -0.0208])\n",
      "counts: tensor([  1,   2,   1,   4,   2,   6,  20,  48, 711])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.230749\n",
      "         Iterations: 3\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "mu 0.09056603773653132\n",
      "theta 0.03113239997231543\n",
      "p 0.0878316283524433\n",
      "logit 2.340402923491355\n",
      "output: tensor([-1.1502e+01, -1.0307e+01, -8.9669e+00, -8.4560e+00, -7.8832e+00,\n",
      "        -7.1977e+00, -6.2397e+00, -3.6565e-03])\n",
      "counts: tensor([  1,   2,   1,   1,   2,   4,  22, 762])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.407450\n",
      "         Iterations: 3\n",
      "         Function evaluations: 7\n",
      "         Gradient evaluations: 7\n",
      "mu 0.13207547169811432\n",
      "theta 0.6754855100807526\n",
      "p 0.07882817899854516\n",
      "logit 2.4583760435876623\n",
      "output: tensor([-8.7769, -6.8518, -4.8641, -0.0090])\n",
      "counts: tensor([  1,  13,  76, 705])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.511422\n",
      "         Iterations: 5\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.2855345911949054\n",
      "theta 0.06155130589276153\n",
      "p 0.2689786066955771\n",
      "logit 0.9998108777013524\n",
      "output: tensor([-8.2908, -7.9971, -7.6918, -7.3718, -7.0326, -6.6673, -6.2643, -5.8017,\n",
      "        -5.2313, -4.4027, -0.0275])\n",
      "counts: tensor([  3,   2,   2,   1,   2,   5,   7,   8,  17,  33, 715])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.534567\n",
      "         Iterations: 4\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.3320754716982341\n",
      "theta 0.05660835081005549\n",
      "p 0.3142843528007765\n",
      "logit 0.780164875053384\n",
      "output: tensor([-9.2438, -8.5761, -8.1041, -7.8570, -7.6005, -7.3324, -7.0495, -6.7473,\n",
      "        -6.4189, -6.0524, -5.6260, -5.0910, -4.2955, -0.0330])\n",
      "counts: tensor([  1,   2,   1,   1,   3,   1,   1,   2,   6,   7,   8,  16,  33, 713])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.738226\n",
      "         Iterations: 4\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 9\n",
      "mu 0.7333333333335373\n",
      "theta 0.0555839773185413\n",
      "p 0.6947181362077844\n",
      "logit -0.822270710459401\n",
      "output: tensor([-10.6280,  -8.7768,  -7.8997,  -7.6637,  -7.4182,  -7.1611,  -7.0272,\n",
      "         -6.7462,  -6.5977,  -6.4427,  -6.2798,  -6.1076,  -5.9236,  -5.7250,\n",
      "         -5.5070,  -5.2626,  -4.9802,  -4.6378,  -4.1867,  -3.4746,  -0.1001])\n",
      "counts: tensor([  1,   1,   1,   1,   3,   1,   1,   4,   2,   1,   2,   3,   4,   5,\n",
      "          4,   7,   8,   3,  19,  38, 686])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.011885\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.0025157232704402514\n",
      "theta 0.0025188956831394198\n",
      "p 0.0025094023476993715\n",
      "logit 5.985198106502054\n",
      "output: tensor([-1.4052e+01, -4.3735e-06])\n",
      "counts: tensor([  1, 794])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.300658\n",
      "         Iterations: 3\n",
      "         Function evaluations: 13\n",
      "         Gradient evaluations: 13\n",
      "mu 0.17232704402479265\n",
      "theta 0.027194850650280804\n",
      "p 0.1677647078504127\n",
      "logit 1.6015527546953254\n",
      "output: tensor([-1.3312e+01, -1.1660e+01, -8.6258e+00, -8.3497e+00, -8.0535e+00,\n",
      "        -7.3672e+00, -6.9420e+00, -6.4035e+00, -5.5906e+00, -8.8897e-03])\n",
      "counts: tensor([  1,   1,   1,   1,   2,   4,   2,   5,  25, 753])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.617161\n",
      "         Iterations: 4\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 9\n",
      "mu 0.41006289308178195\n",
      "theta 0.06699820503233735\n",
      "p 0.3843145106971892\n",
      "logit 0.4712750082021657\n",
      "output: tensor([-9.1783, -8.3390, -8.1187, -7.8929, -7.6606, -7.4206, -6.9106, -6.6352,\n",
      "        -6.3409, -6.0205, -5.6626, -5.2457, -4.7219, -3.9422, -0.0485])\n",
      "counts: tensor([  2,   1,   1,   1,   1,   1,   3,   2,   6,   4,   9,   8,  17,  42,\n",
      "        697])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.027824\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.005031446540880504\n",
      "theta 0.010152397848689\n",
      "p 0.004980878678896296\n",
      "logit 5.297155637364061\n",
      "output: tensor([-1.2788e+01, -1.1001e+01, -2.0313e-05])\n",
      "counts: tensor([  1,   2, 792])\n",
      "(795,)\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.342242\n",
      "         Iterations: 1\n",
      "         Function evaluations: 124\n",
      "         Gradient evaluations: 112\n",
      "mu 0.18993710691823204\n",
      "theta 0.012159039845257366\n",
      "p 0.1876553974632982\n",
      "logit 1.465317348867777\n",
      "output: tensor([-1.2579e+01, -9.4510e+00, -9.3099e+00, -8.8398e+00, -8.6614e+00,\n",
      "        -8.2534e+00, -8.0115e+00, -7.7293e+00, -7.3836e+00, -6.9222e+00,\n",
      "        -6.1791e+00, -6.3247e-03])\n",
      "counts: tensor([  1,   1,   1,   1,   1,   1,   1,   1,   2,   6,  32, 747])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.048089\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.008805031446540873\n",
      "theta 0.03175753717058664\n",
      "p 0.008534012235749807\n",
      "logit 4.755125025023559\n",
      "output: tensor([-1.1938e+01, -9.7486e+00, -6.6068e-05])\n",
      "counts: tensor([  1,   5, 789])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.476660\n",
      "         Iterations: 5\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "mu 0.1962264150943367\n",
      "theta 0.12027074219343716\n",
      "p 0.1751598142339544\n",
      "logit 1.5494908719646614\n",
      "output: tensor([-13.4748, -10.7297,  -9.5787,  -8.3718,  -7.0636,  -6.3372,  -5.5121,\n",
      "         -4.4545,  -0.0194])\n",
      "counts: tensor([  1,   1,   1,   1,   1,   3,  24,  56, 707])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.131746\n",
      "         Iterations: 1\n",
      "         Function evaluations: 13\n",
      "         Gradient evaluations: 13\n",
      "mu 0.1446540880626428\n",
      "theta 0.0049432078102713915\n",
      "p 0.14394255012463633\n",
      "logit 1.7829232257265601\n",
      "output: tensor([-1.3697e+01, -1.0831e+01, -1.0138e+01, -8.4570e+00, -8.0204e+00,\n",
      "        -7.2985e+00, -2.4089e-03])\n",
      "counts: tensor([  1,   1,   1,   2,   2,   7, 781])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.031927\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.012578616352201125\n",
      "theta 0.0022484064348598756\n",
      "p 0.012550397956675284\n",
      "logit 4.365373084797114\n",
      "output: tensor([-1.3869e+01, -1.0644e+01, -5.3115e-05])\n",
      "counts: tensor([  1,   2, 792])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.724774\n",
      "         Iterations: 3\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "mu 0.5320754716978338\n",
      "theta 0.08078465044067927\n",
      "p 0.49230480048072967\n",
      "logit 0.03078322871410672\n",
      "output: tensor([-14.4888, -11.5634,  -9.6622,  -7.5451,  -7.3304,  -6.6429,  -6.3938,\n",
      "         -5.8483,  -5.5407,  -5.1962,  -4.7937,  -4.2865,  -3.5297,  -0.0772])\n",
      "counts: tensor([  1,   1,   1,   1,   1,   2,   5,   3,   6,  10,  10,  30,  50, 674])\n",
      "(795,)\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: nan\n",
      "         Iterations: 2\n",
      "         Function evaluations: 119\n",
      "         Gradient evaluations: 119\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.009658\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.0012578616352201257\n",
      "theta 20.0\n",
      "p 5.989817310572027e-05\n",
      "logit 9.722804652410682\n",
      "output: tensor([-1.6403e+01, -7.5294e-08])\n",
      "counts: tensor([  1, 794])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.076016\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.021383647798742127\n",
      "theta 0.00795558220521543\n",
      "p 0.021214871147356276\n",
      "logit 3.831609733230616\n",
      "output: tensor([-1.2838e+01, -1.0015e+01, -9.0136e+00, -2.1915e-04])\n",
      "counts: tensor([  1,   2,   6, 786])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.257407\n",
      "         Iterations: 4\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 12\n",
      "mu 0.10440251572330686\n",
      "theta 0.03532477966311992\n",
      "p 0.10084035249043104\n",
      "logit 2.187922004286852\n",
      "output: tensor([-1.1187e+01, -1.0411e+01, -1.0006e+01, -9.5863e+00, -8.6799e+00,\n",
      "        -8.1741e+00, -7.6067e+00, -6.9273e+00, -5.9774e+00, -4.7928e-03])\n",
      "counts: tensor([  1,   1,   1,   1,   1,   2,   1,   2,  28, 757])\n",
      "(795,)\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: nan\n",
      "         Iterations: 2\n",
      "         Function evaluations: 121\n",
      "         Gradient evaluations: 121\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.304530\n",
      "         Iterations: 4\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.09056603773581771\n",
      "theta 0.2071493385645791\n",
      "p 0.07502471719324286\n",
      "logit 2.5119493940463484\n",
      "output: tensor([-1.1395e+01, -1.0032e+01, -8.6214e+00, -7.1244e+00, -5.4294e+00,\n",
      "        -5.4451e-03])\n",
      "counts: tensor([  1,   1,   2,   3,  51, 737])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.009658\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.0012578616352201257\n",
      "theta 20.0\n",
      "p 5.989817310572027e-05\n",
      "logit 9.722804652410682\n",
      "output: tensor([-1.6403e+01, -7.5294e-08])\n",
      "counts: tensor([  1, 794])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.048089\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.00880503144654088\n",
      "theta 0.031757537170586825\n",
      "p 0.008534012235749813\n",
      "logit 4.755125025023558\n",
      "output: tensor([-1.1938e+01, -9.7486e+00, -6.6068e-05])\n",
      "counts: tensor([  1,   5, 789])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.017573\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.0025157232704402514\n",
      "theta 20.0\n",
      "p 0.00011979634621144054\n",
      "logit 9.029597568295443\n",
      "output: tensor([-1.5018e+01, -3.0098e-07])\n",
      "counts: tensor([  2, 793])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.380463\n",
      "         Iterations: 5\n",
      "         Function evaluations: 13\n",
      "         Gradient evaluations: 13\n",
      "mu 0.26037735849076743\n",
      "theta 0.03131307336159818\n",
      "p 0.252471693820441\n",
      "logit 1.0854730335967102\n",
      "output: tensor([-12.0278, -11.4484, -10.8505,  -8.6639,  -8.0450,  -7.5749,  -7.3124,\n",
      "         -7.0227,  -6.6938,  -6.3030,  -5.7995,  -5.0236,  -0.0172])\n",
      "counts: tensor([  1,   1,   1,   1,   1,   2,   1,   3,   2,   2,  11,  28, 741])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.368447\n",
      "         Iterations: 5\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 12\n",
      "mu 0.16352201257863233\n",
      "theta 0.05600928794159221\n",
      "p 0.1548490287404335\n",
      "logit 1.6970646414722692\n",
      "output: tensor([-1.1771e+01, -8.6968e+00, -8.2574e+00, -7.7917e+00, -7.2879e+00,\n",
      "        -6.7242e+00, -6.0518e+00, -5.1186e+00, -1.1471e-02])\n",
      "counts: tensor([  2,   1,   1,   3,   2,   3,   8,  39, 736])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.017573\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.0025157232704402514\n",
      "theta 20.0\n",
      "p 0.00011979634621144054\n",
      "logit 9.029597568295443\n",
      "output: tensor([-1.5018e+01, -3.0098e-07])\n",
      "counts: tensor([  2, 793])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.058217\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.012578616352201226\n",
      "theta 0.015954347258223193\n",
      "p 0.012381084234884565\n",
      "logit 4.379127067577139\n",
      "output: tensor([-1.2071e+01, -1.0854e+01, -9.3579e+00, -1.1431e-04])\n",
      "counts: tensor([  1,   1,   5, 788])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.471483\n",
      "         Iterations: 4\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 9\n",
      "mu 0.19119496855345364\n",
      "theta 0.11725995972643906\n",
      "p 0.1711284530417323\n",
      "logit 1.577650731861321\n",
      "output: tensor([-9.0384, -7.7879, -7.1153, -6.3877, -5.5609, -4.5004, -0.0185])\n",
      "counts: tensor([  2,   2,   4,   6,  23,  48, 710])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.528660\n",
      "         Iterations: 4\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.2830188679245794\n",
      "theta 0.0754644307415049\n",
      "p 0.2631596730069866\n",
      "logit 1.0296102464079036\n",
      "output: tensor([-10.9265, -10.3451,  -8.8178,  -8.4937,  -8.1603,  -7.4564,  -7.0783,\n",
      "         -6.6746,  -6.2338,  -5.7346,  -5.1298,  -4.2730,  -0.0296])\n",
      "counts: tensor([  1,   1,   1,   1,   1,   2,   3,   2,   6,   6,  12,  53, 706])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.321880\n",
      "         Iterations: 3\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.17484276729560377\n",
      "theta 0.030581767073212154\n",
      "p 0.16965443488501286\n",
      "logit 1.5880783253878201\n",
      "output: tensor([-1.0347e+01, -9.8859e+00, -9.6472e+00, -8.8851e+00, -8.3196e+00,\n",
      "        -7.6719e+00, -7.2952e+00, -6.8565e+00, -6.3050e+00, -5.4808e+00,\n",
      "        -9.6462e-03])\n",
      "counts: tensor([  1,   2,   1,   1,   1,   2,   1,   5,  10,  21, 750])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.525197\n",
      "         Iterations: 4\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.3660377358488039\n",
      "theta 0.047738640792767344\n",
      "p 0.34935977504069415\n",
      "logit 0.6218545743504945\n",
      "output: tensor([-9.8125, -9.1313, -8.7741, -8.0103, -7.5919, -7.3692, -7.1348, -6.8855,\n",
      "        -6.6167, -6.3213, -5.9874, -5.5929, -5.0885, -4.3194, -0.0348])\n",
      "counts: tensor([  1,   1,   1,   1,   3,   1,   3,   4,   3,   4,   6,   6,   8,  36,\n",
      "        717])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.108249\n",
      "         Iterations: 2\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 9\n",
      "mu 0.02264150943397143\n",
      "theta 0.23984533150892023\n",
      "p 0.01826155961438851\n",
      "logit 3.9845266358485136\n",
      "output: tensor([-1.0831e+01, -7.9028e+00, -3.9093e-04])\n",
      "counts: tensor([  1,  16, 778])\n",
      "(795,)\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.419881\n",
      "         Iterations: 1\n",
      "         Function evaluations: 124\n",
      "         Gradient evaluations: 112\n",
      "mu 0.21635220125786322\n",
      "theta 0.005562130534533887\n",
      "p 0.2151554784017725\n",
      "logit 1.2941247143733996\n",
      "output: tensor([-1.3172e+01, -9.2894e+00, -9.1593e+00, -9.0168e+00, -8.2263e+00,\n",
      "        -7.9151e+00, -7.4870e+00, -6.7741e+00, -4.3761e-03])\n",
      "counts: tensor([  1,   1,   1,   1,   1,   2,   4,  46, 738])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.009658\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.0012578616352201257\n",
      "theta 20.0\n",
      "p 5.989817310572027e-05\n",
      "logit 9.722804652410682\n",
      "output: tensor([-1.6403e+01, -7.5294e-08])\n",
      "counts: tensor([  1, 794])\n",
      "(795,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3123: RuntimeWarning: invalid value encountered in log\n",
      "  (y + a1) * np.log(a2))\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: invalid value encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: divide by zero encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in multiply\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3156: RuntimeWarning: invalid value encountered in divide\n",
      "  a4 = p * a1 / mu\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3164: RuntimeWarning: divide by zero encountered in divide\n",
      "  y / mu)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3164: RuntimeWarning: invalid value encountered in divide\n",
      "  y / mu)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3123: RuntimeWarning: invalid value encountered in log\n",
      "  (y + a1) * np.log(a2))\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: invalid value encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: invalid value encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3225: RuntimeWarning: invalid value encountered in log\n",
      "  lprob = np.log(prob)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: invalid value encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3439: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(linpred)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in multiply\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: divide by zero encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3162: RuntimeWarning: invalid value encountered in multiply\n",
      "  dparams = (a4 * dgterm -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: invalid value encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: invalid value encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3225: RuntimeWarning: invalid value encountered in log\n",
      "  lprob = np.log(prob)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: invalid value encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3439: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(linpred)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in multiply\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: divide by zero encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3162: RuntimeWarning: invalid value encountered in multiply\n",
      "  dparams = (a4 * dgterm -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: invalid value encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3123: RuntimeWarning: invalid value encountered in log\n",
      "  (y + a1) * np.log(a2))\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: divide by zero encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in multiply\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3156: RuntimeWarning: invalid value encountered in divide\n",
      "  a4 = p * a1 / mu\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3164: RuntimeWarning: divide by zero encountered in divide\n",
      "  y / mu)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3164: RuntimeWarning: invalid value encountered in divide\n",
      "  y / mu)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: invalid value encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3439: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(linpred)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in multiply\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: divide by zero encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3162: RuntimeWarning: invalid value encountered in multiply\n",
      "  dparams = (a4 * dgterm -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: invalid value encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: invalid value encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3225: RuntimeWarning: invalid value encountered in log\n",
      "  lprob = np.log(prob)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3123: RuntimeWarning: invalid value encountered in log\n",
      "  (y + a1) * np.log(a2))\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: invalid value encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: divide by zero encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in multiply\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3156: RuntimeWarning: invalid value encountered in divide\n",
      "  a4 = p * a1 / mu\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3164: RuntimeWarning: divide by zero encountered in divide\n",
      "  y / mu)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3164: RuntimeWarning: invalid value encountered in divide\n",
      "  y / mu)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.097456\n",
      "         Iterations: 1\n",
      "         Function evaluations: 124\n",
      "         Gradient evaluations: 112\n",
      "mu 0.045283018867924\n",
      "theta 0.002937063290264211\n",
      "p 0.04515040925835089\n",
      "logit 3.0515544872340197\n",
      "output: tensor([-1.3612e+01, -9.7522e+00, -8.9992e+00, -3.6964e-04])\n",
      "counts: tensor([  1,   2,   8, 784])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.369304\n",
      "         Iterations: 4\n",
      "         Function evaluations: 14\n",
      "         Gradient evaluations: 14\n",
      "mu 0.22641509433891216\n",
      "theta 0.03258444326362047\n",
      "p 0.2192702938883111\n",
      "logit 1.2699238155500894\n",
      "output: tensor([-13.6198,  -9.8255,  -8.3285,  -8.0803,  -7.8170,  -7.5338,  -7.2235,\n",
      "         -6.8740,  -6.4627,  -5.9389,  -5.1434,  -0.0144])\n",
      "counts: tensor([  1,   1,   1,   1,   1,   2,   3,   5,   4,  10,  23, 743])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.017573\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.0025157232704402514\n",
      "theta 20.0\n",
      "p 0.00011979634621144054\n",
      "logit 9.029597568295443\n",
      "output: tensor([-1.5018e+01, -3.0098e-07])\n",
      "counts: tensor([  2, 793])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.062133\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.017610062893081622\n",
      "theta 0.0077544548851011365\n",
      "p 0.01747455722742444\n",
      "logit 4.0293802890474195\n",
      "output: tensor([-1.2333e+01, -1.1747e+01, -9.2806e+00, -1.5986e-04])\n",
      "counts: tensor([  1,   1,   5, 788])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.737216\n",
      "         Iterations: 4\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 12\n",
      "mu 0.7358490566041689\n",
      "theta 0.05918788498667564\n",
      "p 0.6947294875955136\n",
      "logit -0.8223242338778499\n",
      "output: tensor([-13.6258, -10.2785,  -7.8040,  -7.6808,  -7.2960,  -7.1617,  -7.0238,\n",
      "         -6.8816,  -6.5822,  -6.4232,  -6.0802,  -5.8925,  -5.6900,  -5.4683,\n",
      "         -5.2204,  -4.9346,  -4.5891,  -4.1354,  -3.4224,  -0.1043])\n",
      "counts: tensor([  1,   1,   2,   4,   1,   2,   1,   1,   2,   2,   1,   3,   2,   1,\n",
      "          6,   5,   6,  20,  54, 680])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.154265\n",
      "         Iterations: 4\n",
      "         Function evaluations: 13\n",
      "         Gradient evaluations: 13\n",
      "mu 0.060377358491336644\n",
      "theta 0.016297687928010996\n",
      "p 0.05940912707814155\n",
      "logit 2.762060396920851\n",
      "output: tensor([-1.2269e+01, -1.1269e+01, -1.0154e+01, -8.7565e+00, -8.1202e+00,\n",
      "        -7.2042e+00, -1.4817e-03])\n",
      "counts: tensor([  1,   1,   1,   1,   2,  14, 775])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.009658\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.0012578616352201257\n",
      "theta 20.0\n",
      "p 5.989817310572027e-05\n",
      "logit 9.722804652410682\n",
      "output: tensor([-1.6403e+01, -7.5294e-08])\n",
      "counts: tensor([  1, 794])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.218072\n",
      "         Iterations: 2\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.09433962264114083\n",
      "theta 0.02302137010304511\n",
      "p 0.09221666858399875\n",
      "logit 2.2868648268704304\n",
      "output: tensor([-1.0614e+01, -1.0293e+01, -9.2421e+00, -8.8461e+00, -7.9119e+00,\n",
      "        -7.2995e+00, -6.4108e+00, -3.3997e-03])\n",
      "counts: tensor([  1,   1,   2,   1,   6,   3,  15, 766])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.034462\n",
      "         Iterations: 2\n",
      "         Function evaluations: 13\n",
      "         Gradient evaluations: 13\n",
      "mu 0.012578616353722614\n",
      "theta 0.00396226970213702\n",
      "p 0.012528973182881197\n",
      "logit 4.367103339555464\n",
      "output: tensor([-1.2391e+01, -1.1831e+01, -7.0743e-05])\n",
      "counts: tensor([  1,   2, 792])\n",
      "(795,)\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: nan\n",
      "         Iterations: 2\n",
      "         Function evaluations: 120\n",
      "         Gradient evaluations: 120\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.738799\n",
      "         Iterations: 5\n",
      "         Function evaluations: 13\n",
      "         Gradient evaluations: 13\n",
      "mu 0.38867924528300546\n",
      "theta 0.18680105431935418\n",
      "p 0.32750160093674513\n",
      "logit 0.7195067874782427\n",
      "output: tensor([-26.1365, -12.3415,  -9.6592,  -8.7273,  -7.7629,  -6.7473,  -6.2092,\n",
      "         -5.6393,  -5.0196,  -4.3109,  -3.3966,  -0.0641])\n",
      "counts: tensor([  1,   1,   1,   1,   1,   1,   2,   3,   5,  26, 115, 638])\n",
      "(795,)\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.288124\n",
      "         Iterations: 3\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "mu 0.2025157232715327\n",
      "theta 0.019702068639728284\n",
      "p 0.19860283655370684\n",
      "logit 1.395049624440925\n",
      "output: tensor([-1.1574e+01, -1.0374e+01, -9.9364e+00, -8.9529e+00, -8.5708e+00,\n",
      "        -8.1390e+00, -7.3130e+00, -6.9390e+00, -6.4505e+00, -5.6841e+00,\n",
      "        -9.3011e-03])\n",
      "counts: tensor([  1,   1,   1,   1,   2,   3,   2,   2,   7,  17, 758])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.063800\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.02012578616352202\n",
      "theta 0.00647384380972765\n",
      "p 0.01999633302673961\n",
      "logit 3.892007405386516\n",
      "output: tensor([-1.2844e+01, -1.0887e+01, -1.0206e+01, -9.2402e+00, -1.8212e-04])\n",
      "counts: tensor([  1,   1,   1,   4, 788])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.024830\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.0037735849056603783\n",
      "theta 20.0\n",
      "p 0.00017969451931716085\n",
      "logit 8.624072553043334\n",
      "output: tensor([-1.4208e+01, -6.7675e-07])\n",
      "counts: tensor([  3, 792])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.565094\n",
      "         Iterations: 4\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 9\n",
      "mu 0.2754716981132159\n",
      "theta 0.10343125345856999\n",
      "p 0.249650077655299\n",
      "logit 1.1004794128466455\n",
      "output: tensor([-8.1601, -7.7225, -7.2666, -6.7860, -6.2696, -5.6970, -5.0231, -4.1096,\n",
      "        -0.0319])\n",
      "counts: tensor([  3,   2,   4,   3,   6,   9,  18,  55, 695])\n",
      "(795,)\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.522291\n",
      "         Iterations: 5\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.3132075471697179\n",
      "theta 0.0568655969938018\n",
      "p 0.2963551354690891\n",
      "logit 0.8647152579361197\n",
      "output: tensor([-8.9861, -8.7496, -8.5074, -8.0023, -7.7364, -7.1665, -6.8550, -6.5171,\n",
      "        -6.1413, -5.7055, -5.1613, -4.3566, -0.0304])\n",
      "counts: tensor([  1,   1,   1,   1,   1,   5,   3,   5,   5,  10,  16,  31, 715])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.574899\n",
      "         Iterations: 5\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.26289308176100973\n",
      "theta 0.13553143512791344\n",
      "p 0.23151545930685374\n",
      "logit 1.1997737964070294\n",
      "output: tensor([-9.6979, -9.1917, -8.1449, -7.5973, -7.0259, -6.4203, -5.7611, -5.0054,\n",
      "        -4.0236, -0.0320])\n",
      "counts: tensor([  2,   1,   2,   3,   1,   3,   6,  22,  69, 686])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.017573\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.0025157232704402514\n",
      "theta 20.0\n",
      "p 0.00011979634621144054\n",
      "logit 9.029597568295443\n",
      "output: tensor([-1.5018e+01, -3.0098e-07])\n",
      "counts: tensor([  2, 793])\n",
      "(795,)\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.665944\n",
      "         Iterations: 5\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.33962264150944554\n",
      "theta 0.14197300791110806\n",
      "p 0.2973998852483228\n",
      "logit 0.8597102549796879\n",
      "output: tensor([-10.4303, -10.0178,  -6.9151,  -6.4115,  -5.8740,  -5.2833,  -4.5971,\n",
      "         -3.6875,  -0.0485])\n",
      "counts: tensor([  2,   2,   2,   6,   5,   9,  25,  77, 667])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.161538\n",
      "         Iterations: 4\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.03899371069181288\n",
      "theta 0.10986057541853236\n",
      "p 0.035133882178946856\n",
      "logit 3.312823384595917\n",
      "output: tensor([-8.8586e+00, -6.9301e+00, -1.1538e-03])\n",
      "counts: tensor([  5,  21, 769])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.114946\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.05408805031446526\n",
      "theta 0.008677939548387775\n",
      "p 0.05362271562980939\n",
      "logit 2.8706685328684465\n",
      "output: tensor([-1.2162e+01, -1.1203e+01, -1.0355e+01, -9.2220e+00, -8.6720e+00,\n",
      "        -7.8387e+00, -9.1328e-04])\n",
      "counts: tensor([  1,   1,   1,   1,   3,   6, 782])\n",
      "(795,)\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.038170\n",
      "         Iterations: 1\n",
      "         Function evaluations: 124\n",
      "         Gradient evaluations: 112\n",
      "mu 0.006289308176100623\n",
      "theta 33.61131987639512\n",
      "p 0.00018171246281740107\n",
      "logit 8.612903265860556\n",
      "output: tensor([-1.3688e+01, -1.1392e-06])\n",
      "counts: tensor([  5, 790])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.110065\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.03270440251572315\n",
      "theta 0.01486000980619327\n",
      "p 0.032225530811849284\n",
      "logit 3.402240053014389\n",
      "output: tensor([-1.1667e+01, -1.1113e+01, -9.8616e+00, -9.0889e+00, -8.0359e+00,\n",
      "        -5.5249e-04])\n",
      "counts: tensor([  1,   1,   1,   1,  10, 781])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 1.034196\n",
      "         Iterations: 4\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 9\n",
      "mu 1.4025157232700844\n",
      "theta 0.07444515764618367\n",
      "p 1.3053395171351638\n",
      "logit nan\n",
      "output: tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan])\n",
      "counts: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.600877\n",
      "         Iterations: 5\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "mu 0.47672955974869985\n",
      "theta 0.05210307276495657\n",
      "p 0.4531205849402579\n",
      "logit 0.1880700483500402\n",
      "output: tensor([-12.1154,  -9.3418,  -8.7543,  -8.2893,  -8.1282,  -7.6206,  -7.0613,\n",
      "         -6.8580,  -6.6430,  -6.4132,  -6.1639,  -5.8883,  -5.5743,  -5.2001,\n",
      "         -4.7167,  -3.9706,  -0.0529])\n",
      "counts: tensor([  1,   1,   1,   1,   1,   1,   2,   3,   2,   5,   3,   4,   5,   7,\n",
      "         13,  41, 704])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 1.799612\n",
      "         Iterations: 5\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 2.386163522012565\n",
      "theta 0.25746270794740417\n",
      "p 1.8976018190691115\n",
      "logit nan\n",
      "output: tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan])\n",
      "counts: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.027824\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.005031446540880504\n",
      "theta 0.010152397848689\n",
      "p 0.004980878678896296\n",
      "logit 5.297155637364061\n",
      "output: tensor([-1.2788e+01, -1.1001e+01, -2.0313e-05])\n",
      "counts: tensor([  1,   2, 792])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.468263\n",
      "         Iterations: 3\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 9\n",
      "mu 0.2943396226412443\n",
      "theta 0.04583013928538137\n",
      "p 0.28144113616994\n",
      "logit 0.9373243101927881\n",
      "output: tensor([-9.9779, -8.9634, -8.7481, -8.5271, -8.0641, -7.5624, -7.2906, -6.9994,\n",
      "        -6.6815, -6.3250, -5.9078, -5.3803, -4.5872, -0.0250])\n",
      "counts: tensor([  1,   1,   1,   1,   3,   4,   1,   2,   1,   5,   4,  12,  34, 725])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.695102\n",
      "         Iterations: 2\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 9\n",
      "mu 0.33081761006289706\n",
      "theta 0.216572667471198\n",
      "p 0.27192589387245053\n",
      "logit 0.9848732569705816\n",
      "output: tensor([-18.0579,  -9.7913,  -9.2061,  -7.3825,  -6.7390,  -6.0650,  -5.3434,\n",
      "         -4.5372,  -3.5365,  -0.0507])\n",
      "counts: tensor([  1,   2,   1,   1,   2,   3,   8,  27, 103, 647])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.598085\n",
      "         Iterations: 4\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "mu 0.27421383647794845\n",
      "theta 0.15261566242487556\n",
      "p 0.23790570041453082\n",
      "logit 1.164195922355533\n",
      "output: tensor([-15.7592,  -9.2539,  -8.1581,  -7.5866,  -6.9919,  -6.3637,  -5.6831,\n",
      "         -4.9087,  -3.9151,  -0.0352])\n",
      "counts: tensor([  1,   1,   1,   1,   4,   3,   7,  22,  77, 678])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.089306\n",
      "         Iterations: 3\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 12\n",
      "mu 0.02389937106930237\n",
      "theta 0.014452367051567865\n",
      "p 0.023558889353044888\n",
      "logit 3.724411225480895\n",
      "output: tensor([-1.1251e+01, -1.0495e+01, -9.6239e+00, -8.4722e+00, -3.3001e-04])\n",
      "counts: tensor([  1,   2,   1,   7, 784])\n",
      "(795,)\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.511618\n",
      "         Iterations: 1\n",
      "         Function evaluations: 124\n",
      "         Gradient evaluations: 112\n",
      "mu 0.8075471698112944\n",
      "theta 0.008209800460577654\n",
      "p 0.8009713548136358\n",
      "logit -1.3923764246227306\n",
      "output: tensor([-12.2139, -11.4938,  -9.3200,  -9.0855,  -8.5995,  -8.3458,  -7.9694,\n",
      "         -7.7331,  -7.6437,  -7.4427,  -7.2012,  -7.0588,  -6.8959,  -6.7051,\n",
      "         -6.4739,  -6.1788,  -5.7673,  -5.0722,  -0.0301])\n",
      "counts: tensor([  1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   2,   1,   1,   1,\n",
      "          4,   6,  11,  28, 731])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 1.074913\n",
      "         Iterations: 4\n",
      "         Function evaluations: 13\n",
      "         Gradient evaluations: 13\n",
      "mu 3.528301886800607\n",
      "theta 0.047590858418760346\n",
      "p 3.368015154433713\n",
      "logit nan\n",
      "output: tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan])\n",
      "counts: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.400139\n",
      "         Iterations: 3\n",
      "         Function evaluations: 117\n",
      "         Gradient evaluations: 117\n",
      "mu 0.22012578616373038\n",
      "theta 0.045314667278126164\n",
      "p 0.21058327511744526\n",
      "logit 1.3214131686834232\n",
      "output: tensor([-13.5711, -13.1309, -10.1023,  -7.5450,  -6.7854,  -6.3256,  -5.7553,\n",
      "         -4.9193,  -0.0163])\n",
      "counts: tensor([  1,   1,   1,   3,   2,   4,  14,  36, 733])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.080847\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.07295597484276733\n",
      "theta 0.00258357085889588\n",
      "p 0.07276797362664462\n",
      "logit 2.544927896686557\n",
      "output: tensor([-1.3658e+01, -1.1091e+01, -9.7869e+00, -9.3479e+00, -8.6226e+00,\n",
      "        -6.3204e-04])\n",
      "counts: tensor([  1,   1,   1,   1,   4, 787])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 1.277351\n",
      "         Iterations: 4\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "mu 1.5220125786161953\n",
      "theta 0.13498205407103286\n",
      "p 1.3410014485752744\n",
      "logit nan\n",
      "output: tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan])\n",
      "counts: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.543020\n",
      "         Iterations: 5\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 12\n",
      "mu 0.8515723270464695\n",
      "theta 0.02506029825327263\n",
      "p 0.8307533990903454\n",
      "logit -1.5909761704796421\n",
      "output: tensor([-11.6823,  -9.8581,  -9.7691,  -9.6792,  -8.4288,  -8.0715,  -7.8108,\n",
      "         -7.3694,  -7.2877,  -7.2030,  -7.0231,  -6.8257,  -6.7187,  -6.6050,\n",
      "         -6.3516,  -6.2079,  -6.0490,  -5.8700,  -5.6637,  -5.4178,  -5.1094,\n",
      "         -4.6874,  -3.9900,  -0.0734])\n",
      "counts: tensor([  1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   2,   2,   1,   1,\n",
      "          2,   3,   2,   4,   3,   2,   6,   9,  21, 727])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.094884\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.032704402515723166\n",
      "theta 0.010465165476246801\n",
      "p 0.032365690211902665\n",
      "logit 3.39775531911174\n",
      "output: tensor([-1.2279e+01, -1.0483e+01, -9.9211e+00, -9.2432e+00, -8.2828e+00,\n",
      "        -4.7656e-04])\n",
      "counts: tensor([  1,   1,   2,   1,   6, 784])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.754162\n",
      "         Iterations: 3\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 9\n",
      "mu 0.7572327044024474\n",
      "theta 0.05889719160081493\n",
      "p 0.715114470421516\n",
      "logit -0.9203551793876106\n",
      "output: tensor([-10.8117,  -9.5613,  -7.7304,  -7.6096,  -7.1004,  -6.9648,  -6.8251,\n",
      "         -6.6807,  -6.5306,  -6.3740,  -6.2097,  -6.0360,  -5.8506,  -5.6505,\n",
      "         -5.4313,  -5.1857,  -4.9023,  -4.5592,  -4.1078,  -3.3970,  -0.1082])\n",
      "counts: tensor([  1,   1,   1,   2,   3,   1,   2,   3,   2,   2,   3,   4,   2,   6,\n",
      "          6,   3,   3,   3,  13,  54, 680])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.142651\n",
      "         Iterations: 3\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "mu 0.0427672955978409\n",
      "theta 0.0235587148111125\n",
      "p 0.04178294315605839\n",
      "logit 3.1325861274283935\n",
      "output: tensor([-1.1319e+01, -9.3280e+00, -8.4954e+00, -7.3867e+00, -1.0071e-03])\n",
      "counts: tensor([  1,   2,   6,  10, 776])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.273013\n",
      "         Iterations: 3\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.12327044025118275\n",
      "theta 0.030261357551462986\n",
      "p 0.11964967854773222\n",
      "logit 1.9957517930678834\n",
      "output: tensor([-1.0395e+01, -1.0083e+01, -9.4281e+00, -8.7107e+00, -8.3149e+00,\n",
      "        -7.8798e+00, -7.3826e+00, -6.7726e+00, -5.8897e+00, -5.7546e-03])\n",
      "counts: tensor([  1,   1,   1,   1,   1,   1,   8,   6,  18, 757])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.554116\n",
      "         Iterations: 5\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 12\n",
      "mu 0.42264150943386564\n",
      "theta 0.04975460406501891\n",
      "p 0.40260981737755575\n",
      "logit 0.3946025742890334\n",
      "output: tensor([-12.2223, -10.7097, -10.4238,  -8.4125,  -8.0633,  -7.8817,  -7.6945,\n",
      "         -7.2990,  -6.8650,  -6.6273,  -6.3701,  -6.0864,  -5.7643,  -5.3818,\n",
      "         -4.8896,  -4.1337,  -0.0436])\n",
      "counts: tensor([  1,   1,   1,   1,   1,   1,   1,   1,   4,   2,   1,   3,   2,   7,\n",
      "         15,  43, 710])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.542001\n",
      "         Iterations: 6\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "mu 0.3685534591194867\n",
      "theta 0.04934781616585038\n",
      "p 0.35122144768559416\n",
      "logit 0.6136745151701614\n",
      "output: tensor([-8.7819, -7.7981, -7.5820, -7.3565, -7.1192, -6.8670, -6.5954, -6.2972,\n",
      "        -5.9607, -5.5637, -5.0569, -4.2863, -0.0358])\n",
      "counts: tensor([  1,   3,   1,   3,   1,   3,   5,   9,   7,   9,  11,  26, 716])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.621624\n",
      "         Iterations: 4\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n",
      "mu 0.6578616352197808\n",
      "theta 0.039486534939261055\n",
      "p 0.6328717237864181\n",
      "logit -0.5445564401096034\n",
      "output: tensor([-9.1507, -9.0637, -8.2335, -8.1343, -8.0334, -7.3781, -7.2579, -7.1334,\n",
      "        -7.0040, -6.8690, -6.7273, -6.5776, -6.4183, -6.2472, -6.0610, -5.8551,\n",
      "        -5.6224, -5.3508, -5.0179, -4.5737, -3.8610, -0.0702])\n",
      "counts: tensor([  1,   1,   1,   1,   2,   2,   1,   2,   2,   2,   1,   2,   3,   4,\n",
      "          5,   3,   3,   4,   4,  13,  28, 710])\n",
      "(795,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.486786\n",
      "         Iterations: 2\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "mu 0.27295597484270834\n",
      "theta 0.06175200539379608\n",
      "p 0.2570807245534431\n",
      "logit 1.0611972531448952\n",
      "output: tensor([-15.6068,  -9.8168,  -8.4176,  -8.1151,  -7.8011,  -7.1245,  -6.7505,\n",
      "         -6.3387,  -5.8675,  -5.2885,  -4.4513,  -0.0258])\n",
      "counts: tensor([  1,   1,   1,   3,   1,   1,   3,   3,   4,  11,  51, 715])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.009658\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.0012578616352201257\n",
      "theta 20.0\n",
      "p 5.989817310572027e-05\n",
      "logit 9.722804652410682\n",
      "output: tensor([-1.6403e+01, -7.5294e-08])\n",
      "counts: tensor([  1, 794])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.590289\n",
      "         Iterations: 4\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 9\n",
      "mu 0.30062893081754627\n",
      "theta 0.10499597617253473\n",
      "p 0.27206337154173116\n",
      "logit 0.9841789728178215\n",
      "output: tensor([-8.7066, -8.3133, -7.9090, -7.4908, -7.0544, -6.5933, -6.0966, -5.5437,\n",
      "        -4.8899, -3.9970, -0.0367])\n",
      "counts: tensor([  1,   4,   2,   1,   3,   4,   3,   8,  16,  64, 689])\n",
      "(795,)\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.009658\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.0012578616352201257\n",
      "theta 20.0\n",
      "p 5.989817310572027e-05\n",
      "logit 9.722804652410682\n",
      "output: tensor([-1.6403e+01, -7.5294e-08])\n",
      "counts: tensor([  1, 794])\n",
      "(795,)\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.038170\n",
      "         Iterations: 1\n",
      "         Function evaluations: 124\n",
      "         Gradient evaluations: 112\n",
      "mu 0.006289308176100819\n",
      "theta 33.61131987639512\n",
      "p 0.00018171246281740674\n",
      "logit 8.612903265860524\n",
      "output: tensor([-1.3688e+01, -1.1392e-06])\n",
      "counts: tensor([  5, 790])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.614421\n",
      "         Iterations: 4\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "mu 0.34088050314456986\n",
      "theta 0.09604463687255495\n",
      "p 0.31100969036921305\n",
      "logit 0.7954031360809575\n",
      "output: tensor([-11.2830,  -9.4721,  -9.1572,  -8.5103,  -7.8333,  -7.4793,  -7.1111,\n",
      "         -6.7246,  -6.3131,  -5.8655,  -5.3611,  -4.7543,  -3.9046,  -0.0430])\n",
      "counts: tensor([  1,   1,   1,   1,   1,   3,   3,   1,   3,   5,   5,  19,  65, 686])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.082741\n",
      "         Iterations: 1\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 9\n",
      "mu 0.016352201257869337\n",
      "theta 0.11549080766781006\n",
      "p 0.01465919857471293\n",
      "logit 4.207919545143492\n",
      "output: tensor([-1.1155e+01, -8.4838e+00, -2.2251e-04])\n",
      "counts: tensor([  1,  11, 783])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.159931\n",
      "         Iterations: 3\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 12\n",
      "mu 0.04654088050329511\n",
      "theta 0.03382363251660352\n",
      "p 0.04501820140249855\n",
      "logit 3.0546253976357067\n",
      "output: tensor([-1.2204e+01, -1.0788e+01, -9.2039e+00, -8.2689e+00, -7.0628e+00,\n",
      "        -1.2995e-03])\n",
      "counts: tensor([  1,   1,   1,   2,  18, 772])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.015177\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.010062893081761008\n",
      "theta 0.0014378148473849633\n",
      "p 0.01004844527794724\n",
      "logit 4.590238083480822\n",
      "output: tensor([-1.4292e+01, -2.9997e-05])\n",
      "counts: tensor([  1, 794])\n",
      "(795,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.015177\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "mu 0.010062893081761008\n",
      "theta 0.0014378148473849633\n",
      "p 0.01004844527794724\n",
      "logit 4.590238083480822\n",
      "output: tensor([-1.4292e+01, -2.9997e-05])\n",
      "counts: tensor([  1, 794])\n",
      " data for (contrl, control): None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/var/folders/12/d54y_pk95zdbftjg_qkcj_c80000gq/T/ipykernel_42688/534514316.py:123: RuntimeWarning: invalid value encountered in log\n",
      "  logit=np.log((1-p)/p)\n",
      "/var/folders/12/d54y_pk95zdbftjg_qkcj_c80000gq/T/ipykernel_42688/534514316.py:123: RuntimeWarning: invalid value encountered in log\n",
      "  logit=np.log((1-p)/p)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: invalid value encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3123: RuntimeWarning: invalid value encountered in log\n",
      "  (y + a1) * np.log(a2))\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: divide by zero encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in multiply\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3156: RuntimeWarning: invalid value encountered in divide\n",
      "  a4 = p * a1 / mu\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3164: RuntimeWarning: divide by zero encountered in divide\n",
      "  y / mu)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3164: RuntimeWarning: invalid value encountered in divide\n",
      "  y / mu)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/var/folders/12/d54y_pk95zdbftjg_qkcj_c80000gq/T/ipykernel_42688/534514316.py:123: RuntimeWarning: invalid value encountered in log\n",
      "  logit=np.log((1-p)/p)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3123: RuntimeWarning: invalid value encountered in log\n",
      "  (y + a1) * np.log(a2))\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: invalid value encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3439: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(linpred)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in multiply\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: divide by zero encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3162: RuntimeWarning: invalid value encountered in multiply\n",
      "  dparams = (a4 * dgterm -\n",
      "/var/folders/12/d54y_pk95zdbftjg_qkcj_c80000gq/T/ipykernel_42688/534514316.py:123: RuntimeWarning: invalid value encountered in log\n",
      "  logit=np.log((1-p)/p)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in log\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3123: RuntimeWarning: invalid value encountered in log\n",
      "  (y + a1) * np.log(a2))\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: invalid value encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3439: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(linpred)\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3122: RuntimeWarning: invalid value encountered in multiply\n",
      "  a1 * np.log(a1) + y * np.log(mu) -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3159: RuntimeWarning: divide by zero encountered in log\n",
      "  dgterm = dgpart + np.log(a1 / a2) + 1 - a3 / a2\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:3162: RuntimeWarning: invalid value encountered in multiply\n",
      "  dparams = (a4 * dgterm -\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/Users/meiqiliu/miniforge3/envs/fa_base2/lib/python3.8/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.discrete.count_model import (ZeroInflatedNegativeBinomialP, ZeroInflatedPoisson,\n",
    "                                              ZeroInflatedGeneralizedPoisson)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm \n",
    "from joblib import Parallel,delayed\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def process_item_alpha(i,x):\n",
    "    plt.hist(x[i],bins=30)\n",
    "    plt.yscale('log')\n",
    "    plt.show()\n",
    "    \n",
    "    print(all(elem == 0 for elem in x[i]))\n",
    "    print(\"gene\",i)\n",
    "\n",
    "    if all(elem == 0 for elem in x[i]):\n",
    "        return None \n",
    "\n",
    "    else:\n",
    "        one=np.ones_like(x[i])\n",
    "        model=sm.NegativeBinomialP(x[i],one).fit()\n",
    "        if model.params[1]>0:\n",
    "            # print(model.summary())\n",
    "            return model.params[1]\n",
    "        else:\n",
    "            return None \n",
    "    \n",
    "def process_item_const(i,x):\n",
    "    plt.hist(x[i],bins=30)\n",
    "    plt.yscale('log')\n",
    "    plt.show()\n",
    "    \n",
    "    print(all(elem == 0 for elem in x[i]))\n",
    "    print(\"gene\",i)\n",
    "\n",
    "    if all(elem == 0 for elem in x[i]):\n",
    "        return None \n",
    "\n",
    "    else:\n",
    "        one=np.ones_like(x[i])\n",
    "        model=sm.NegativeBinomialP(x[i],one).fit()\n",
    "        if model.params[1]>0:\n",
    "        # print(model.summary())\n",
    "            return model.params[0]\n",
    "        else:\n",
    "            return None \n",
    "\n",
    "def process_per_batch(x,num_from,num_to):\n",
    "    x=x[num_from:num_to,:]\n",
    "\n",
    "    alpha=Parallel(n_jobs=-1,backend=\"multiprocessing\")(\n",
    "        delayed(process_item_alpha)(i,x)for i in range(x.shape[0])\n",
    "    )\n",
    "\n",
    "    const=Parallel(n_jobs=-1,backend=\"multiprocessing\")(\n",
    "        delayed(process_item_const)(i,x)for i in range(x.shape[0])\n",
    "    ) \n",
    "    return alpha,const    \n",
    "\n",
    "\n",
    "\n",
    "def zinb(adata, conditions,stim_str,sample_ctrl=False,ctrl1=None,ctrl2=None):\n",
    "    if sample_ctrl:\n",
    "        x,y=ctrl1,ctrl2\n",
    "    else:\n",
    "        x,y=data_prep(adata,conditions)\n",
    "        \n",
    "    x,y=dist_based(x,y)\n",
    "\n",
    "    np.savetxt(stim_str+'datax.csv', x, delimiter=',')\n",
    "    np.savetxt(stim_str+'datay.csv', y, delimiter=',')\n",
    "\n",
    "    alpha_x, const_x =process_per_batch(x,0,99)\n",
    "    alpha_y, const_y = process_per_batch(y,0,99)\n",
    "\n",
    "    df=pd.DataFrame({\"alpha_x\":alpha_x,\"const_x\":const_x,\"alpha_y\":alpha_y,\"const_y\":const_y})\n",
    "    df.to_csv(stim_str+'df.csv', index=False)\n",
    "\n",
    "#     p=1/(1+np.exp(const_x))*alpha_x # probability of single success \n",
    "#     #1-p probability of failure \n",
    "#     logit=np.log((1-p)/p)\n",
    "    \n",
    "\n",
    "#     x_val=log_zinb_positive(torch.Tensor(x),torch.Tensor(np.exp(const_x)),torch.Tensor(1/alpha_x),torch.Tensor(logit))\n",
    "#     print(x_val)\n",
    "  \n",
    "    return None\n",
    "\n",
    "def log_zinb(adata, conditions,stim_str,sample_ctrl=False,ctrl1=None,ctrl2=None):\n",
    "    \n",
    "    if sample_ctrl:\n",
    "        x,y=ctrl1,ctrl2\n",
    "    else:\n",
    "        x,y=data_prep(adata,conditions)\n",
    "        \n",
    "    x,y=dist_based(x,y)\n",
    "\n",
    "    output=[]\n",
    "    \n",
    "    for i in range(x.shape[0]):\n",
    "        print(x[i].shape)\n",
    "#         plt.hist(x[i],bins=30)\n",
    "#         plt.yscale('log')\n",
    "#         plt.show()\n",
    "\n",
    "\n",
    "\n",
    "        if all(elem == 0 for elem in x[i]):\n",
    "            continue \n",
    "\n",
    "        else:\n",
    "            one=np.ones_like(x[i])\n",
    "            model=sm.NegativeBinomialP(x[i],one).fit()\n",
    "            if model.params[1]>0:\n",
    "                const_x=model.params[0]\n",
    "                alpha_x=model.params[1]\n",
    "                mu=np.exp(const_x)\n",
    "                print(\"mu\",mu)\n",
    "                theta=1/alpha_x\n",
    "                p=1/(1+theta)*mu\n",
    "                logit=np.log((1-p)/p)\n",
    "                print(\"theta\",theta)\n",
    "                print(\"p\",p)\n",
    "                print(\"logit\",logit)\n",
    "                x_val=log_zinb_positive(torch.tensor(x[i]),torch.tensor(mu),torch.tensor(theta),torch.tensor(logit))\n",
    "                unique, counts = torch.unique(x_val, return_counts=True)\n",
    "                print(\"output:\",unique)\n",
    "                print(\"counts:\",counts)\n",
    "                output.append(x_val)\n",
    "            else:\n",
    "                continue \n",
    "\n",
    "  \n",
    "    return None \n",
    "\n",
    "test_get_data(log_zinb,train,'zinb')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_zinb(x,const_x,alpha_x):\n",
    "    mu=np.exp(const_x)\n",
    "    print(\"mu\",mu)\n",
    "    theta=1/alpha_x\n",
    "    p=1/(1+theta)*mu\n",
    "    logit=np.log((1-p)/p)\n",
    "    print(\"theta\",theta)\n",
    "    print(\"p\",p)\n",
    "    print(\"logit\",logit)\n",
    "    x_val=log_zinb_positive(torch.tensor(x[i]),torch.tensor(mu),torch.tensor(theta),torch.tensor(logit))\n",
    "    most_frequent=torch.mode(x_val).values.item()\n",
    "    return most_frequent \n",
    "    \n",
    "    \n",
    "\n",
    "def load_saved_data():\n",
    "    list_stim=list(train.obs['perturbation'].unique())\n",
    "    list_stim.remove('control')\n",
    "    \n",
    "    for stim in list_stim:\n",
    "        x=np.loadtxt(str(stim)+'datax.csv', delimiter=',')\n",
    "        y=np.loadtxt(str(stim)+'datay.csv', delimiter=',')\n",
    "        df=pd.read_csv(str(stim)+'df.csv') \n",
    "        resx=[]\n",
    "        resy=[]\n",
    "        for i in range(x.shape[0]):\n",
    "            if all(elem == 0 for elem in x[i]):\n",
    "                continue \n",
    "            else:\n",
    "                outputx=compute_log_zinb(x[i],df['const_x'],df['alpha_x'])\n",
    "                outputy=compute_log_zinb(y[i],df['const_y'],df['alpha_y'])\n",
    "                resx.append(outputx)\n",
    "                resy.append(outputy)\n",
    "        avgx=np.mean(resx)\n",
    "        avgy=np.mean(resy)\n",
    "        \n",
    "        \n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
